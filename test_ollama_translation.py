#!/usr/bin/env python3
"""
æµ‹è¯• Ollama ç¿»è¯‘åŠŸèƒ½
ä½¿ç”¨æ–¹æ³•ï¼š
1. ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œ (ollama serve)
2. ä¸‹è½½ä¸€ä¸ªä¸­æ–‡æ¨¡å‹ï¼Œä¾‹å¦‚ï¼šollama pull qwen2.5:7b
3. è¿è¡Œæ­¤è„šæœ¬ï¼špython test_ollama_translation.py
"""

import asyncio
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from mdcx.config.models import Config, TranslateConfig
from mdcx.config.enums import Translator
from mdcx.config.computed import Computed
from mdcx.base.translate import ollama_translate


async def test_ollama_translation():
    """æµ‹è¯• Ollama ç¿»è¯‘åŠŸèƒ½"""
    print("ğŸš€ å¼€å§‹æµ‹è¯• Ollama ç¿»è¯‘åŠŸèƒ½...")
    
    # åˆ›å»ºæµ‹è¯•é…ç½®
    config = Config()
    config.translate_config = TranslateConfig(
        ollama_url="http://localhost:11434",
        ollama_model="qwen2.5:7b",
        ollama_prompt="è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘ä¸º{lang}ã€‚åªè¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸è¦ä»»ä½•è§£é‡Šã€‚\n{content}",
        ollama_read_timeout=120,
        ollama_max_req_sec=0.5,
        ollama_max_try=3,
        ollama_temperature=0.3,
    )
    
    # åˆå§‹åŒ–è®¡ç®—é…ç½®
    computed = Computed(config)
    
    # æµ‹è¯•æ–‡æœ¬
    test_title = "ç¾å°‘å¥³æˆ¦å£«ã‚»ãƒ¼ãƒ©ãƒ¼ãƒ ãƒ¼ãƒ³"
    test_outline = "æœˆé‡ã†ã•ãã¯æ™®é€šã®ä¸­å­¦2å¹´ç”Ÿã€‚ã‚ã‚‹æ—¥ã€é»’çŒ«ã®ãƒ«ãƒŠã¨å‡ºä¼šã„ã€ã‚»ãƒ¼ãƒ©ãƒ¼ãƒ ãƒ¼ãƒ³ã¨ã—ã¦æˆ¦ã†ã“ã¨ã«ãªã‚‹ã€‚"
    
    print(f"ğŸ“ æµ‹è¯•æ ‡é¢˜: {test_title}")
    print(f"ğŸ“ æµ‹è¯•ç®€ä»‹: {test_outline}")
    print()
    
    try:
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
        print("ğŸ” æ£€æŸ¥ Ollama æ¨¡å‹æ˜¯å¦å¯ç”¨...")
        is_available = await computed.ollama_client.check_model_available(config.translate_config.ollama_model)
        
        if not is_available:
            print(f"âŒ æ¨¡å‹ {config.translate_config.ollama_model} ä¸å¯ç”¨")
            print("ğŸ’¡ è¯·ç¡®ä¿ï¼š")
            print("   1. Ollama æœåŠ¡æ­£åœ¨è¿è¡Œ (ollama serve)")
            print(f"   2. å·²ä¸‹è½½æ¨¡å‹ (ollama pull {config.translate_config.ollama_model})")
            return False
        
        print(f"âœ… æ¨¡å‹ {config.translate_config.ollama_model} å¯ç”¨")
        print()
        
        # æ‰§è¡Œç¿»è¯‘
        print("ğŸ”„ å¼€å§‹ç¿»è¯‘...")
        translated_title, translated_outline, error = await ollama_translate(
            test_title, test_outline, "ç®€ä½“ä¸­æ–‡"
        )
        
        if error:
            print(f"âŒ ç¿»è¯‘å¤±è´¥: {error}")
            return False
        
        print("âœ… ç¿»è¯‘æˆåŠŸï¼")
        print(f"ğŸ“– ç¿»è¯‘æ ‡é¢˜: {translated_title}")
        print(f"ğŸ“– ç¿»è¯‘ç®€ä»‹: {translated_outline}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return False
    
    finally:
        # æ¸…ç†èµ„æº
        await computed.ollama_client.close()


async def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ§ª Ollama ç¿»è¯‘åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    success = await test_ollama_translation()
    
    print()
    print("=" * 60)
    if success:
        print("ğŸ‰ æµ‹è¯•é€šè¿‡ï¼Ollama ç¿»è¯‘åŠŸèƒ½å·¥ä½œæ­£å¸¸")
    else:
        print("ğŸ’¥ æµ‹è¯•å¤±è´¥ï¼è¯·æ£€æŸ¥ Ollama é…ç½®")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
