import asyncio
import re

from mdcx.config.manager import config
from mdcx.config.models import Language, Website, WebsiteSet
from mdcx.crawlers import get_crawler_compat
from mdcx.gen.field_enums import CrawlerResultFields
from mdcx.manual import ManualConfig
from mdcx.models.enums import FileMode
from mdcx.models.flags import Flags
from mdcx.models.log_buffer import LogBuffer
from mdcx.models.types import CrawlerInput, CrawlerResponse, CrawlerResult, CrawlersResult, CrawlTask
from mdcx.number import is_uncensored
from mdcx.utils.dataclass import update
from mdcx.utils.language import is_japanese

MULTI_LANGUAGE_WEBSITES = [  # æ”¯æŒå¤šè¯­è¨€, language å‚æ•°æœ‰æ„ä¹‰
    Website.AIRAV_CC,
    Website.AIRAV,
    Website.IQQTV,
    Website.JAVLIBRARY,
]


def clean_list(raw: list[str]) -> list[str]:
    """æ¸…ç†åˆ—è¡¨ï¼Œå»é™¤ç©ºå€¼å’Œé‡å¤å€¼, ä¿æŒåŸæœ‰é¡ºåº"""
    cleaned = []
    for item in raw:
        if item.strip() and item not in cleaned:
            cleaned.append(item.strip())
    return cleaned


def _deal_some_list(field: str, website: Website, same_list: list[Website]) -> list[Website]:
    if website not in same_list:
        same_list.append(website)
    if field in ["title", "outline", "thumb", "poster", "trailer", "extrafanart"]:
        same_list.remove(website)
        same_list.insert(0, website)
    elif field in ["tag", "score", "director", "series"]:
        same_list.remove(website)
    return same_list


async def _call_crawler(task_input: CrawlerInput, website: Website, timeout: int = 30) -> CrawlerResponse:
    """
    è°ƒç”¨æŒ‡å®šç½‘ç«™çš„çˆ¬è™«å‡½æ•°

    Args:
        task_input (CallCrawlerInput): åŒ…å«çˆ¬è™«æ‰€éœ€çš„è¾“å…¥æ•°æ®
        website (str): ç½‘ç«™åç§°
        timeout (int): è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼Œé»˜è®¤ä¸º30ç§’

    Raises:
        asyncio.TimeoutError: å¦‚æœè¯·æ±‚è¶…æ—¶
        Exception: çˆ¬è™«å‡½æ•°æŠ›å‡ºçš„å¼‚å¸¸
    """
    short_number = task_input.short_number

    # 259LUXU-1111ï¼Œ mgstage å’Œ avsex ä¹‹å¤–ä½¿ç”¨ LUXU-1111ï¼ˆç´ äººç•ªå·æ—¶ï¼Œshort_numberæœ‰å€¼ï¼Œä¸å¸¦å‰ç¼€æ•°å­—ï¼›åä¹‹ï¼Œshort_numberä¸ºç©º)
    if short_number and website != "mgstage" and website != "avsex":
        task_input.number = short_number

    # è·å–çˆ¬è™«å‡½æ•°
    crawler = get_crawler_compat(website)
    c = crawler(config.async_client, config.get_website_base_url(website))

    # å¯¹çˆ¬è™«å‡½æ•°è°ƒç”¨æ·»åŠ è¶…æ—¶é™åˆ¶, è¶…æ—¶å¼‚å¸¸ç”±è°ƒç”¨è€…å¤„ç†
    r = await asyncio.wait_for(c.run(task_input), timeout=timeout)
    return r


async def _call_crawlers(task_input: CrawlerInput, number_website_list: list[Website]) -> CrawlersResult:
    """
    è·å–ä¸€ç»„ç½‘ç«™çš„æ•°æ®ï¼šæŒ‰ç…§è®¾ç½®çš„ç½‘ç«™ç»„ï¼Œè¯·æ±‚å„å­—æ®µæ•°æ®ï¼Œå¹¶è¿”å›æœ€ç»ˆçš„æ•°æ®
    é‡‡ç”¨æŒ‰éœ€è¯·æ±‚ç­–ç•¥ï¼šä»…è¯·æ±‚å¿…è¦çš„ç½‘ç«™ï¼Œå¤±è´¥æ—¶æ‰è¯·æ±‚ä¸‹ä¸€ä¼˜å…ˆçº§ç½‘ç«™
    """
    number = task_input.number
    short_number = task_input.short_number
    scrape_like = config.scrape_like
    none_fields = config.none_fields  # ä¸å•ç‹¬åˆ®å‰Šçš„å­—æ®µ

    def get_field_websites(field: str) -> list[Website]:
        """
        è·å–æŒ‡å®šå­—æ®µçš„æ¥æºä¼˜å…ˆçº§åˆ—è¡¨

        field_websites = (config.{field}_website - config.{field}_website_exclude) âˆ© (number_website_list)
        """
        # æŒ‡å®šå­—æ®µç½‘ç«™åˆ—è¡¨
        field_no_zh = field.replace("_zh", "")  # å»é™¤ _zh åç¼€çš„å­—æ®µå
        field_list: list[Website] = getattr(config.pydantic, f"{field}_website", [])
        # todo ç§»é™¤è¿è¡Œæ—¶æ£€æŸ¥
        assert all(isinstance(i, Website) for i in field_list), f"{field}_website must be a list of Website"
        # ä¸æŒ‡å®šç±»å‹ç½‘ç«™åˆ—è¡¨å–äº¤é›†
        field_list = [i for i in field_list if i in number_website_list]
        if WebsiteSet.OFFICIAL in config.pydantic.website_set:  # ä¼˜å…ˆä½¿ç”¨å®˜æ–¹ç½‘ç«™
            field_list.insert(0, Website.OFFICIAL)
        # æŒ‡å®šå­—æ®µæ’é™¤ç½‘ç«™åˆ—è¡¨
        field_ex_list: list[Website] = getattr(config.pydantic, f"{field_no_zh}_website_exclude", [])
        # æ‰€æœ‰è®¾å®šçš„æœ¬å­—æ®µæ¥æºå¤±è´¥æ—¶, æ˜¯å¦ç»§ç»­ä½¿ç”¨ç±»å‹ç½‘ç«™è¡¥å…¨
        include_others = field == "title" or field in config.whole_fields
        if include_others and field != "trailer":  # å–å‰©ä½™æœªç›¸äº¤ç½‘ç«™ï¼Œ trailer ä¸å–æœªç›¸äº¤ç½‘ç«™
            field_list.extend([i for i in number_website_list if i not in field_list])
        # æ’é™¤æŒ‡å®šç½‘ç«™
        field_list = [i for i in field_list if i not in field_ex_list]
        # ç‰¹æ®Šå¤„ç†
        # mgstage ç´ äººç•ªå·æ£€æŸ¥
        if short_number:
            not_frist_field_list = ["title", "actor"]  # è¿™äº›å­—æ®µä»¥å¤–ï¼Œç´ äººæŠŠ mgstage æ”¾åœ¨ç¬¬ä¸€ä½
            if field not in not_frist_field_list and Website.MGSTAGE in field_list:
                field_list.remove(Website.MGSTAGE)
                field_list.insert(0, Website.MGSTAGE)
        # faleno.jp ç•ªå·æ£€æŸ¥ dldss177 dhla009
        elif re.findall(r"F[A-Z]{2}SS", number):
            field_list = _deal_some_list(field, Website.FALENO, field_list)
        # dahlia-av.jp ç•ªå·æ£€æŸ¥
        elif number.startswith("DLDSS") or number.startswith("DHLA"):
            field_list = _deal_some_list(field, Website.DAHLIA, field_list)
        # fantastica ç•ªå·æ£€æŸ¥ FAVIã€FAAPã€FAPLã€FAKGã€FAHOã€FAVAã€FAKYã€FAMIã€FAITã€FAKAã€FAMOã€FASOã€FAIHã€FASHã€FAKSã€FAAN
        elif (
            re.search(r"FA[A-Z]{2}-?\d+", number.upper())
            or number.upper().startswith("CLASS")
            or number.upper().startswith("FADRV")
            or number.upper().startswith("FAPRO")
            or number.upper().startswith("FAKWM")
            or number.upper().startswith("PDS")
        ):
            field_list = _deal_some_list(field, Website.FANTASTICA, field_list)
        return field_list

    # è·å–ä½¿ç”¨çš„ç½‘ç«™
    all_fields = [f for f in ManualConfig.CONFIG_DATA_FIELDS if f not in none_fields]  # å»é™¤ä¸ä¸“é—¨åˆ®å‰Šçš„å­—æ®µ
    if scrape_like == "speed":  # å¿«é€Ÿæ¨¡å¼
        all_field_websites = dict.fromkeys(all_fields, number_website_list)
    else:  # å…¨éƒ¨æ¨¡å¼
        # å„å­—æ®µç½‘ç«™åˆ—è¡¨
        all_field_websites = {field: get_field_websites(field) for field in all_fields}
        if config.outline_language == "jp" and "outline_zh" in all_field_websites:
            del all_field_websites["outline_zh"]
        if config.title_language == "jp" and "title_zh" in all_field_websites:
            del all_field_websites["title_zh"]

    # å„å­—æ®µè¯­è¨€, æœªæŒ‡å®šåˆ™é»˜è®¤ä¸º "any"
    all_field_languages: dict[str, Language] = {
        field: getattr(config.pydantic, f"{field}_language", Language.UNDEFINED) for field in all_fields
    }
    all_field_languages["title_zh"] = config.pydantic.title_language
    all_field_languages["outline_zh"] = config.pydantic.outline_language

    # å¤„ç†é…ç½®é¡¹ä¸­æ²¡æœ‰çš„å­—æ®µ
    # originaltitle çš„ç½‘ç«™ä¼˜å…ˆçº§åŒ title, è¯­è¨€ä¸º jp
    all_field_websites["originaltitle"] = all_field_websites.get("title", number_website_list)
    all_field_languages["originaltitle"] = Language.JP
    all_field_websites["originalplot"] = all_field_websites.get("outline", number_website_list)
    all_field_languages["originalplot"] = Language.JP

    # å„å­—æ®µçš„å–å€¼ä¼˜å…ˆçº§ (ç½‘ç«™, è¯­è¨€) å¯¹
    all_field_website_lang_pairs: dict[str, list[tuple[Website, Language]]] = {}
    for field, websites in all_field_websites.items():
        language = all_field_languages[field]
        all_field_website_lang_pairs[field] = []
        for website in websites:
            pair = (website, language)
            if website not in MULTI_LANGUAGE_WEBSITES:
                pair = (website, Language.UNDEFINED)  # å•è¯­è¨€ç½‘ç«™, è¯­è¨€å‚æ•°æ— æ„ä¹‰
            all_field_website_lang_pairs[field].append(pair)

    # ç¼“å­˜å·²è¯·æ±‚çš„ç½‘ç«™ç»“æœ
    all_res: dict[tuple[Website, Language], CrawlerResult] = {}
    reduced = CrawlersResult.empty()

    # æ— ä¼˜å…ˆçº§è®¾ç½®çš„å­—æ®µçš„é»˜è®¤é…ç½®
    default_website_lang_pairs: list[tuple[Website, Language]] = [(w, Language.UNDEFINED) for w in number_website_list]

    # å¤„ç† CrawlerResult å­—æ®µé‡å‘½å
    for old, new in ManualConfig.RENAME_MAP.items():
        if old in all_field_languages:
            all_field_languages[new] = all_field_languages[old]
            del all_field_languages[old]
        if old in all_field_website_lang_pairs:
            all_field_website_lang_pairs[new] = all_field_website_lang_pairs[old]
            del all_field_website_lang_pairs[old]

    all_field_website_lang_pairs["all_actors"] = all_field_website_lang_pairs["actors"]

    # æŒ‰å­—æ®µåˆ†åˆ«å¤„ç†ï¼Œæ¯ä¸ªå­—æ®µæŒ‰ä¼˜å…ˆçº§å°è¯•è·å–
    for field in ManualConfig.REDUCED_FIELDS:  # ä¸ CONFIG_DATA_FIELDS ä¸å®Œå…¨ä¸€è‡´
        # è·å–è¯¥å­—æ®µçš„ä¼˜å…ˆçº§åˆ—è¡¨
        sources = all_field_website_lang_pairs.get(field, default_website_lang_pairs)

        # å¦‚æœtitle_languageä¸æ˜¯jpï¼Œåˆ™å…è®¸ä»title_zhæ¥æºè·å–title
        if field == CrawlerResultFields.TITLE and config.pydantic.title_language != Language.JP:
            sources = sources + all_field_website_lang_pairs.get("title_zh", [])
        # å¦‚æœoutline_languageä¸æ˜¯jpï¼Œåˆ™å…è®¸ä»outline_zhæ¥æºè·å–outline
        elif field == CrawlerResultFields.OUTLINE and config.pydantic.outline_language != Language.JP:
            sources = sources + all_field_website_lang_pairs.get("outline_zh", [])

        LogBuffer.info().write(
            f"\n\n    ğŸ™‹ğŸ»â€ {field} \n    ====================================\n"
            f"    ğŸŒ æ¥æºä¼˜å…ˆçº§ï¼š{' -> '.join(i[0] + f'({i[1]})' * bool(i[1]) for i in sources)}"
        )

        # æŒ‰ä¼˜å…ˆçº§ä¾æ¬¡å°è¯•è·å–å­—æ®µå€¼
        for website, language in sources:
            # æ£€æŸ¥æ˜¯å¦å·²ç»è¯·æ±‚è¿‡è¯¥ç½‘ç«™
            key = (website, language)

            # å¦‚æœç½‘ç«™ä¸æ”¯æŒå¤šè¯­è¨€ï¼Œæ ‡å‡†åŒ–key
            if website not in MULTI_LANGUAGE_WEBSITES:
                key = (website, Language.UNDEFINED)

            # å¦‚æœå·²æœ‰è¯¥ç½‘ç«™æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨
            if key in all_res:
                site_data = all_res[key]
            else:
                # å¦‚æœç½‘ç«™æ•°æ®å°šæœªè¯·æ±‚ï¼Œåˆ™è¿›è¡Œè¯·æ±‚
                try:
                    task_input.language = language
                    task_input.org_language = config.title_language
                    web_data = await _call_crawler(task_input, website)
                    if web_data.data is None:
                        if e := web_data.debug_info.error:
                            raise e
                        raise ValueError(f"çˆ¬è™« {website} è¿”å›äº†ç©ºæ•°æ®")
                    site_data = web_data.data
                    # å¤„ç†å¹¶ä¿å­˜ç»“æœ
                    all_res[key] = web_data.data
                    # å¤šè¯­è¨€ç½‘ç«™, å¦‚æœ undefined å°šä¸å­˜åœ¨, ä¹Ÿä½¿ç”¨å½“å‰è¯­è¨€æ•°æ®
                    if website in MULTI_LANGUAGE_WEBSITES and (website, Language.UNDEFINED) not in all_res:
                        all_res[(website, Language.UNDEFINED)] = web_data.data
                except Exception as e:
                    LogBuffer.info().write(f"\n    ğŸ”´ {website} (å¼‚å¸¸: {str(e)})")
                    continue

            # è·å–ç½‘ç«™æ•°æ®
            if not site_data or not site_data.title or not getattr(site_data, field, None):
                LogBuffer.info().write(f"\n    ğŸ”´ {website} (å¤±è´¥)")
                continue

            # è¯­è¨€æ£€æµ‹é€»è¾‘
            if config.scrape_like != "speed" and field in [
                CrawlerResultFields.TITLE,
                CrawlerResultFields.OUTLINE,
                CrawlerResultFields.ORIGINALTITLE,
                CrawlerResultFields.ORIGINALPLOT,
            ]:
                lang = all_field_languages.get(field, Language.JP)
                if website in ["airav_cc", "iqqtv", "airav", "avsex", "javlibrary", "lulubar"]:  # why?
                    if not is_japanese(getattr(site_data, field, "")):
                        if lang == Language.JP:
                            LogBuffer.info().write(f"\n    ğŸ”´ {website} (å¤±è´¥ï¼Œæ£€æµ‹ä¸ºéæ—¥æ–‡ï¼Œè·³è¿‡ï¼)")
                            continue
                    elif lang != Language.JP:
                        LogBuffer.info().write(f"\n    ğŸ”´ {website} (å¤±è´¥ï¼Œæ£€æµ‹ä¸ºæ—¥æ–‡ï¼Œè·³è¿‡ï¼)")
                        continue

            # æ·»åŠ æ¥æºä¿¡æ¯
            if field in [
                CrawlerResultFields.POSTER,
                CrawlerResultFields.THUMB,
                CrawlerResultFields.EXTRAFANART,
                CrawlerResultFields.TRAILER,
                CrawlerResultFields.OUTLINE,
            ]:
                setattr(reduced, field + "_from", website)

            if field == CrawlerResultFields.POSTER:
                reduced.image_download = site_data.image_download
            elif field == CrawlerResultFields.ORIGINALTITLE and site_data.actor:
                reduced.amazon_orginaltitle_actor = site_data.actor.split(",")[0]

            # ä¿å­˜æ•°æ®
            setattr(reduced, field, getattr(site_data, field))
            reduced.fields_info += f"\n     {field:<13}: {website}" + f" ({language})" * bool(language)
            LogBuffer.info().write(f"\n    ğŸŸ¢ {website} (æˆåŠŸ)\n     â†³ {getattr(reduced, field)}")

            # æ‰¾åˆ°æœ‰æ•ˆæ•°æ®ï¼Œè·³å‡ºå¾ªç¯ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªå­—æ®µ
            break
        else:  # æ‰€æœ‰æ¥æºéƒ½æ— æ­¤å­—æ®µ
            reduced.fields_info += f"\n     {field:<13}: {'-----'} ({'not found'})"

    # éœ€å°½åŠ›æ”¶é›†çš„å­—æ®µ
    for data in all_res.values():
        # è®°å½•æ‰€æœ‰æ¥æºçš„ thumb url ä»¥ä¾¿åç»­ä¸‹è½½
        if data.thumb:
            reduced.thumb_list.append((data.source, data.thumb))
        # è®°å½•æ‰€æœ‰æ¥æºçš„ actor ç”¨äº Amazon æœå›¾
        if data.actor:
            reduced.actor_amazon.extend(data.actor.split(","))
    # å»é‡
    reduced.thumb_list = list(dict.fromkeys(reduced.thumb_list))  # ä¿åº
    reduced.actor_amazon = list(set(reduced.actor_amazon))

    # å¤„ç† year
    if reduced.year and (r := re.search(r"\d{4}", reduced.release)):
        reduced.year = r.group()

    # å¤„ç† numberï¼šç´ äººå½±ç‰‡æ—¶ä½¿ç”¨æœ‰æ•°å­—å‰ç¼€çš„number
    if short_number:
        reduced.number = number

    # å¤„ç† javdbid
    if r := all_res.get((Website.JAVDB, Language.UNDEFINED)):
        reduced.javdbid = r.javdbid

    # å¤„ç† all_actor
    if not reduced.all_actor:
        # å¦‚æœæ²¡æœ‰ all_actor å­—æ®µï¼Œåˆ™ä» actor ä¸­è·å–
        reduced.all_actor = reduced.actor

    # todo ç”±äºå¼‚æ­¥, æ­¤å¤„æ—¥å¿—æ··ä¹±. éœ€ç§»é™¤ LogBuffer.req(), æ”¹ä¸ºè¿”å›æ—¥å¿—ä¿¡æ¯
    reduced.fields_info = f"\n ğŸŒ [website] {LogBuffer.req().get().strip('-> ')}{reduced.fields_info}"

    return reduced


async def _call_specific_crawler(task_input: CrawlerInput, website: Website) -> CrawlersResult:
    file_number = task_input.number
    short_number = task_input.short_number

    title_language = config.title_language
    org_language = title_language

    if website not in ["airav_cc", "iqqtv", "airav", "avsex", "javlibrary", "mdtv", "madouqu", "lulubar"]:
        title_language = "jp"

    elif website == "mdtv":
        title_language = "zh_cn"

    task_input.language = title_language
    task_input.org_language = org_language
    web_data = await _call_crawler(task_input, website)
    web_data_json = web_data.data
    if web_data_json is None:
        return CrawlersResult.empty()

    res = update(CrawlersResult.empty(), web_data_json)
    if not res.title:
        return res
    if res.thumb:
        res.thumb_list = [(website, res.thumb)]

    # åŠ å…¥æ¥æºä¿¡æ¯
    res.outline_from = website
    res.poster_from = website
    res.thumb_from = website
    res.extrafanart_from = website
    res.trailer_from = website
    # todo
    res.fields_info = f"\n ğŸŒ [website] {LogBuffer.req().get().strip('-> ')}"

    if short_number:
        res.number = file_number

    res.actor_amazon = web_data_json.actors
    res.all_actors = res.all_actors or web_data_json.actors

    return res


async def _crawl(task_input: CrawlTask, website: Website | None) -> CrawlersResult | None:  # ä»JSONè¿”å›å…ƒæ•°æ®
    appoint_number = task_input.appoint_number
    cd_part = task_input.cd_part
    destroyed = task_input.destroyed
    file_number = task_input.number
    file_path = task_input.file_path
    leak = task_input.leak
    mosaic = task_input.mosaic
    short_number = task_input.short_number
    wuma = task_input.wuma
    youma = task_input.youma

    # ================================================ç½‘ç«™è§„åˆ™æ·»åŠ å¼€å§‹================================================

    if website is None:  # ä»å…¨éƒ¨ç½‘ç«™åˆ®å‰Š
        # =======================================================================å…ˆåˆ¤æ–­æ˜¯ä¸æ˜¯å›½äº§ï¼Œé¿å…æµªè´¹æ—¶é—´
        if (
            mosaic == "å›½äº§"
            or mosaic == "åœ‹ç”¢"
            or (re.search(r"([^A-Z]|^)MD[A-Z-]*\d{4,}", file_number) and "MDVR" not in file_number)
            or re.search(r"MKY-[A-Z]+-\d{3,}", file_number)
        ):
            task_input.mosaic = "å›½äº§"
            res = await _call_crawlers(task_input, config.pydantic.website_guochan)

        # =======================================================================kin8
        elif file_number.startswith("KIN8"):
            website = Website.KIN8
            res = await _call_specific_crawler(task_input, website)

        # =======================================================================åŒäºº
        elif file_number.startswith("DLID"):
            website = Website.GETCHU
            res = await _call_specific_crawler(task_input, website)

        # =======================================================================é‡Œç•ª
        elif "getchu" in file_path.lower() or "é‡Œç•ª" in file_path or "è£ç•ª" in file_path:
            website = Website.GETCHU_DMM
            res = await _call_specific_crawler(task_input, website)

        # =======================================================================Mywife No.1111
        elif "mywife" in file_path.lower():
            website = Website.MYWIFE
            res = await _call_specific_crawler(task_input, website)

        # =======================================================================FC2-111111
        elif "FC2" in file_number.upper():
            file_number_1 = re.search(r"\d{5,}", file_number)
            if file_number_1:
                file_number_1.group()
                res = await _call_crawlers(task_input, config.pydantic.website_fc2)
            else:
                LogBuffer.error().write(f"æœªè¯†åˆ«åˆ°FC2ç•ªå·ï¼š{file_number}")
                res = None

        # =======================================================================sexart.15.06.14
        elif re.search(r"[^.]+\.\d{2}\.\d{2}\.\d{2}", file_number) or (
            "æ¬§ç¾" in file_path and "ä¸œæ¬§ç¾" not in file_path
        ):
            res = await _call_crawlers(task_input, config.pydantic.website_oumei)

        # =======================================================================æ— ç æŠ“å–:111111-111,n1111,HEYZO-1111,SMD-115
        elif mosaic == "æ— ç " or mosaic == "ç„¡ç¢¼":
            res = await _call_crawlers(task_input, config.pydantic.website_wuma)

        # =======================================================================259LUXU-1111
        elif short_number or "SIRO" in file_number.upper():
            res = await _call_crawlers(task_input, config.pydantic.website_suren)

        # =======================================================================ssni00321
        elif re.match(r"\D{2,}00\d{3,}", file_number) and "-" not in file_number and "_" not in file_number:
            res = await _call_crawlers(task_input, [Website.DMM])

        # =======================================================================å‰©ä¸‹çš„ï¼ˆå«åŒ¹é…ä¸äº†ï¼‰çš„æŒ‰æœ‰ç æ¥åˆ®å‰Š
        else:
            website_list = config.pydantic.website_youma
            if WebsiteSet.OFFICIAL in config.pydantic.website_set:  # ä¼˜å…ˆä½¿ç”¨å®˜æ–¹ç½‘ç«™
                website_list.insert(0, Website.OFFICIAL)
            res = await _call_crawlers(task_input, website_list)
    else:
        res = await _call_specific_crawler(task_input, website)

    # ================================================ç½‘ç«™è¯·æ±‚ç»“æŸ================================================
    # ======================================è¶…æ—¶æˆ–æœªæ‰¾åˆ°è¿”å›
    if res is None:
        return None

    number = file_number  # res.number å®é™…ä¸Šå¹¶æœªè®¾ç½®, æ­¤å¤„å– file_number
    if appoint_number:
        number = appoint_number
    res.number = number  # æ­¤å¤„è®¾ç½®

    # é©¬èµ›å…‹
    if leak:
        res.mosaic = "æ— ç æµå‡º"
    elif destroyed:
        res.mosaic = "æ— ç ç ´è§£"
    elif wuma:
        res.mosaic = "æ— ç "
    elif youma:
        res.mosaic = "æœ‰ç "
    elif mosaic:
        res.mosaic = mosaic
    if not res.mosaic:
        if is_uncensored(number):
            res.mosaic = "æ— ç "
        else:
            res.mosaic = "æœ‰ç "
    print(number, cd_part, res.mosaic, LogBuffer.req().get().strip("-> "))

    # åŸæ ‡é¢˜ï¼Œç”¨äºamazonæœç´¢
    res.originaltitle_amazon = res.originaltitle
    if res.actor_amazon:
        for each in res.actor_amazon:  # å»é™¤æ¼”å‘˜åï¼Œé¿å…æœç´¢ä¸åˆ°
            try:
                end_actor = re.compile(rf" {each}$")
                res.originaltitle_amazon = re.sub(end_actor, "", res.originaltitle_amazon)
            except Exception:
                pass

    # VR æ—¶ä¸‹è½½å°å°é¢
    if "VR" in number:
        res.image_download = True

    return res


def _get_website_name(task_input: CrawlTask, file_mode: FileMode) -> str:
    # è·å–åˆ®å‰Šç½‘ç«™
    website_name = "all"
    if file_mode == FileMode.Single:  # åˆ®å‰Šå•æ–‡ä»¶ï¼ˆå·¥å…·é¡µé¢ï¼‰
        website_name = Flags.website_name
    elif file_mode == FileMode.Again:  # é‡æ–°åˆ®å‰Š
        website_temp = task_input.website_name
        if website_temp:
            website_name = website_temp
    elif config.scrape_like == "single":
        website_name = config.website_single

    return website_name


async def crawl(task_input: CrawlTask, file_mode: FileMode) -> CrawlersResult | None:
    # ä»æŒ‡å®šç½‘ç«™è·å–json_data
    website_name = _get_website_name(task_input, file_mode)
    if website_name == "all":
        website = None
    else:
        website = Website(website_name)
    res = await _crawl(task_input, website)
    return _deal_res(res)


def _deal_res(res: CrawlersResult | None) -> CrawlersResult | None:
    # æ ‡é¢˜ä¸ºç©ºè¿”å›
    if res is None or not res.title:
        return None

    # æ¼”å‘˜
    res.actor = (
        str(res.actor).strip(" [ ]").replace("'", "").replace(", ", ",").replace("<", "(").replace(">", ")").strip(",")
    )  # åˆ—è¡¨è½¬å­—ç¬¦ä¸²ï¼ˆé¿å…ä¸ªåˆ«ç½‘ç«™åˆ®å‰Šè¿”å›çš„æ˜¯åˆ—è¡¨ï¼‰

    # æ ‡ç­¾
    tag = str(res.tag).strip(" [ ]").replace("'", "").replace("ï¼Œ", ",").replace(", ", ",")  # åˆ—è¡¨è½¬å­—ç¬¦ä¸²
    tag = re.sub(r",\d+[kKpP],", ",", tag)
    tag_rep_word = [",HDé«˜ç”»è´¨", ",HDé«˜ç•«è³ª", ",é«˜ç”»è´¨", ",é«˜ç•«è³ª"]
    for each in tag_rep_word:
        if tag.endswith(each):
            tag = tag.replace(each, "")
        tag = tag.replace(each + ",", ",")
    res.tag = tag

    # å‘è¡Œæ—¥æœŸ
    release = res.release
    if release:
        release = release.replace("/", "-").strip(". ")
        if len(release) < 10:
            release_list = re.findall(r"(\d{4})-(\d{1,2})-(\d{1,2})", release)
            if release_list:
                r_year, r_month, r_day = release_list[0]
                r_month = "0" + r_month if len(r_month) == 1 else r_month
                r_day = "0" + r_day if len(r_day) == 1 else r_day
                release = r_year + "-" + r_month + "-" + r_day
    res.release = release

    # è¯„åˆ†
    if res.score:
        res.score = f"{float(res.score):.1f}"

    # publisher
    if not res.publisher:
        res.publisher = res.studio

    # å­—ç¬¦è½¬ä¹‰ï¼Œé¿å…æ˜¾ç¤ºé—®é¢˜
    key_word = [
        "title",
        "originaltitle",
        "number",
        "outline",
        "originalplot",
        "actor",
        "tag",
        "series",
        "director",
        "studio",
        "publisher",
    ]
    rep_word = {
        "&amp;": "&",
        "&lt;": "<",
        "&gt;": ">",
        "&apos;": "'",
        "&quot;": '"',
        "&lsquo;": "ã€Œ",
        "&rsquo;": "ã€",
        "&hellip;": "â€¦",
        "<br/>": "",
        "ãƒ»": "Â·",
        "â€œ": "ã€Œ",
        "â€": "ã€",
        "...": "â€¦",
        "\xa0": "",
        "\u3000": "",
        "\u2800": "",
    }
    for each in key_word:
        for key, value in rep_word.items():
            # res[each] = res[each].replace(key, value)
            setattr(res, each, getattr(res, each).replace(key, value))

    # å‘½åè§„åˆ™
    # naming_media = config.naming_media
    # naming_file = config.naming_file
    # folder_name = config.folder_name
    # res.naming_media = naming_media
    # res.naming_file = naming_file
    # res.folder_name = folder_name
    return res
