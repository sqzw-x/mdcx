"""
çˆ¬è™«æ§åˆ¶, è°ƒç”¨ models.crawlers ä¸­å„ä¸ªç½‘ç«™çˆ¬è™«
"""

import asyncio
import re
from typing import Callable, TypedDict

import langid

from mdcx.config.manager import config
from mdcx.config.manual import ManualConfig
from mdcx.crawlers import (
    airav,
    airav_cc,
    avsex,
    avsox,
    cableav,
    cnmdb,
    dahlia,
    dmm,
    faleno,
    fantastica,
    fc2,
    fc2club,
    fc2hub,
    fc2ppvdb,
    freejavbt,
    getchu,
    getchu_dmm,
    giga,
    hdouban,
    hscangku,
    iqqtv_new,
    jav321,
    javbus,
    javday,
    javdb,
    javlibrary_new,
    kin8,
    love6,
    lulubar,
    madouqu,
    mdtv,
    mgstage,
    mmtv,
    mywife,
    official,
    prestige,
    theporndb,
    xcity,
)
from mdcx.models.enums import FileMode
from mdcx.models.flags import Flags
from mdcx.models.json_data import JsonData, new_json_data
from mdcx.models.log_buffer import LogBuffer
from mdcx.number import get_number_letters, is_uncensored


class CrawlersResult(TypedDict):
    """
    çˆ¬è™«ç»“æœç±»å‹ï¼ŒåŒ…å«æ‰€æœ‰å¯èƒ½çš„å…ƒæ•°æ®å­—æ®µ. (æ³¨é‡Šç”± AI ç”Ÿæˆ, ä»…ä¾›å‚è€ƒ)
    """

    # åŸºæœ¬ä¿¡æ¯
    number: str  # ç•ªå·
    short_number: str  # ç´ äººç•ªå·çš„çŸ­å½¢å¼ï¼ˆä¸å¸¦å‰ç¼€æ•°å­—ï¼‰
    title: str  # æ ‡é¢˜
    originaltitle: str  # åŸå§‹æ ‡é¢˜ï¼ˆæ—¥æ–‡ï¼‰
    originaltitle_amazon: str  # ç”¨äº Amazon æœç´¢çš„åŸå§‹æ ‡é¢˜
    outline: str  # ç®€ä»‹
    originalplot: str  # åŸå§‹ç®€ä»‹ï¼ˆæ—¥æ–‡ï¼‰
    # æ¼”å‘˜ä¿¡æ¯
    actor: str  # æ¼”å‘˜åç§°ï¼Œé€—å·åˆ†éš”
    all_actor: str  # æ‰€æœ‰æ¥æºçš„æ¼”å‘˜åç§°
    all_actor_photo: dict  # æ¼”å‘˜ç…§ç‰‡ä¿¡æ¯
    actor_amazon: list[str]  # ç”¨äº Amazon æœç´¢çš„æ¼”å‘˜åç§°
    amazon_orginaltitle_actor: str  # ç”¨äº Amazon æœç´¢çš„åŸå§‹æ ‡é¢˜ä¸­çš„æ¼”å‘˜
    # å…ƒæ•°æ®ä¿¡æ¯
    tag: str  # æ ‡ç­¾ï¼Œé€—å·åˆ†éš”
    release: str  # å‘è¡Œæ—¥æœŸ
    year: str  # å‘è¡Œå¹´ä»½
    runtime: str  # ç‰‡é•¿ï¼ˆåˆ†é’Ÿï¼‰
    score: str  # è¯„åˆ†
    series: str  # ç³»åˆ—
    director: str  # å¯¼æ¼”
    studio: str  # åˆ¶ä½œå•†
    publisher: str  # å‘è¡Œå•†
    # å›¾ç‰‡ä¸è§†é¢‘
    thumb: str  # ç¼©ç•¥å›¾URL
    thumb_list: list  # æ‰€æœ‰æ¥æºçš„ç¼©ç•¥å›¾URLåˆ—è¡¨
    poster: str  # æµ·æŠ¥URL
    extrafanart: list[str]  # é¢å¤–å‰§ç…§URLåˆ—è¡¨
    trailer: str  # é¢„å‘Šç‰‡URL
    image_download: bool  # æ˜¯å¦éœ€è¦ä¸‹è½½å›¾ç‰‡
    # é©¬èµ›å…‹ç±»å‹
    mosaic: str
    letters: str  # ç•ªå·å­—æ¯éƒ¨åˆ†
    # æ ‡å¿—ä¿¡æ¯
    has_sub: bool  # æ˜¯å¦æœ‰å­—å¹•
    c_word: str  # ä¸­æ–‡å­—å¹•æ ‡è¯†
    leak: str  # æ˜¯å¦æ˜¯æ— ç æµå‡º
    wuma: str  # æ˜¯å¦æ˜¯æ— ç 
    youma: str  # æ˜¯å¦æ˜¯æœ‰ç 
    cd_part: str  # CDåˆ†é›†ä¿¡æ¯
    destroyed: str  # æ˜¯å¦æ˜¯æ— ç ç ´è§£
    version: int  # ç‰ˆæœ¬ä¿¡æ¯
    # æ–‡ä»¶è·¯å¾„ä¸æŒ‡å®šä¿¡æ¯
    file_path: str  # æ–‡ä»¶è·¯å¾„
    appoint_number: str  # æŒ‡å®šç•ªå·
    appoint_url: str  # æŒ‡å®šURL
    # å…¶ä»–ä¿¡æ¯
    javdbid: str  # JavDB ID
    fields_info: str  # å­—æ®µæ¥æºä¿¡æ¯
    naming_media: str  # åª’ä½“å‘½åè§„åˆ™
    naming_file: str  # æ–‡ä»¶å‘½åè§„åˆ™
    folder_name: str  # æ–‡ä»¶å¤¹å‘½åè§„åˆ™
    # å­—æ®µæ¥æºä¿¡æ¯
    poster_from: str
    thumb_from: str
    extrafanart_from: str
    trailer_from: str
    outline_from: str
    website_name: str  # ä½¿ç”¨çš„ç½‘ç«™åç§°


CRAWLER_FUNCS: dict[str, Callable] = {
    "7mmtv": mmtv.main,
    "airav_cc": airav_cc.main,  # lang
    "airav": airav.main,  # lang
    "avsex": avsex.main,
    "avsox": avsox.main,
    "cableav": cableav.main,
    "cnmdb": cnmdb.main,
    "dahlia": dahlia.main,
    "dmm": dmm.main,
    "faleno": faleno.main,
    "fantastica": fantastica.main,
    "fc2": fc2.main,
    "fc2club": fc2club.main,
    "fc2hub": fc2hub.main,
    "fc2ppvdb": fc2ppvdb.main,
    "freejavbt": freejavbt.main,
    "getchu_dmm": getchu_dmm.main,
    "getchu": getchu.main,
    "giga": giga.main,
    "hdouban": hdouban.main,
    "hscangku": hscangku.main,
    "iqqtv": iqqtv_new.main,  # lang
    "jav321": jav321.main,
    "javbus": javbus.main,
    "javday": javday.main,
    "javdb": javdb.main,
    "javlibrary": javlibrary_new.main,  # lang
    "kin8": kin8.main,
    "love6": love6.main,
    "lulubar": lulubar.main,
    "madouqu": madouqu.main,
    "mdtv": mdtv.main,
    "mgstage": mgstage.main,
    "mywife": mywife.main,
    "official": official.main,
    "prestige": prestige.main,
    "theporndb": theporndb.main,
    "xcity": xcity.main,
}

MULTI_LANGUAGE_WEBSITES = [  # æ”¯æŒå¤šè¯­è¨€, language å‚æ•°æœ‰æ„ä¹‰
    "airav_cc",
    "airav",
    "iqqtv",
    "javlibrary",
]


def clean_list(raw: list[str]) -> list[str]:
    """æ¸…ç†åˆ—è¡¨ï¼Œå»é™¤ç©ºå€¼å’Œé‡å¤å€¼, ä¿æŒåŸæœ‰é¡ºåº"""
    cleaned = []
    for item in raw:
        if item.strip() and item not in cleaned:
            cleaned.append(item.strip())
    return cleaned


def _deal_some_list(field: str, website: str, same_list: list[str]) -> list[str]:
    if website not in same_list:
        same_list.append(website)
    if field in ["title", "outline", "thumb", "poster", "trailer", "extrafanart"]:
        same_list.remove(website)
        same_list.insert(0, website)
    elif field in ["tag", "score", "director", "series"]:
        same_list.remove(website)
    return same_list


async def _call_crawler(
    task_input: JsonData,
    website: str,
    language: str,
    org_language: str,
    timeout: int = 30,
) -> dict[str, dict[str, dict]]:
    """
    è·å–æŸä¸ªç½‘ç«™æ•°æ®
    """
    appoint_number = task_input["appoint_number"]
    appoint_url = task_input["appoint_url"]
    file_path = task_input["file_path"]
    number = task_input["number"]
    mosaic = task_input["mosaic"]
    short_number = task_input["short_number"]

    # 259LUXU-1111ï¼Œ mgstage å’Œ avsex ä¹‹å¤–ä½¿ç”¨ LUXU-1111ï¼ˆç´ äººç•ªå·æ—¶ï¼Œshort_numberæœ‰å€¼ï¼Œä¸å¸¦å‰ç¼€æ•°å­—ï¼›åä¹‹ï¼Œshort_numberä¸ºç©º)
    if short_number and website != "mgstage" and website != "avsex":
        number = short_number

    # è·å–çˆ¬è™«å‡½æ•°
    crawler_func = CRAWLER_FUNCS.get(website, javdb.main)

    # å‡†å¤‡å‚æ•°
    kwargs = {
        "number": number,
        "appoint_url": appoint_url,
        "language": language,
        "file_path": file_path,
        "appoint_number": appoint_number,
        "mosaic": mosaic,
        "short_number": short_number,
        "org_language": org_language,
    }

    try:
        # å¯¹çˆ¬è™«å‡½æ•°è°ƒç”¨æ·»åŠ è¶…æ—¶é™åˆ¶
        return await asyncio.wait_for(crawler_func(**kwargs), timeout=timeout)
    except asyncio.TimeoutError:
        # è¿”å›ç©ºç»“æœ
        return {website: {language or "jp": {"title": "", "thumb": "", "website": ""}}}


async def _call_crawlers(
    task_input: JsonData,
    number_website_list: list[str],
) -> CrawlersResult:
    """
    è·å–ä¸€ç»„ç½‘ç«™çš„æ•°æ®ï¼šæŒ‰ç…§è®¾ç½®çš„ç½‘ç«™ç»„ï¼Œè¯·æ±‚å„å­—æ®µæ•°æ®ï¼Œå¹¶è¿”å›æœ€ç»ˆçš„æ•°æ®
    é‡‡ç”¨æŒ‰éœ€è¯·æ±‚ç­–ç•¥ï¼šä»…è¯·æ±‚å¿…è¦çš„ç½‘ç«™ï¼Œå¤±è´¥æ—¶æ‰è¯·æ±‚ä¸‹ä¸€ä¼˜å…ˆçº§ç½‘ç«™
    """
    number = task_input["number"]
    short_number = task_input["short_number"]
    scrape_like = config.scrape_like
    none_fields = config.none_fields  # ä¸å•ç‹¬åˆ®å‰Šçš„å­—æ®µ

    def get_field_websites(field: str) -> list[str]:
        """
        è·å–æŒ‡å®šå­—æ®µçš„æ¥æºä¼˜å…ˆçº§åˆ—è¡¨

        field_websites = (config.{field}_website - config.{field}_website_exclude) âˆ© (number_website_list)
        """
        # æŒ‡å®šå­—æ®µç½‘ç«™åˆ—è¡¨
        field_no_zh = field.replace("_zh", "")  # å»é™¤ _zh åç¼€çš„å­—æ®µå
        field_list = clean_list(getattr(config, f"{field}_website", "").split(","))
        # ä¸æŒ‡å®šç±»å‹ç½‘ç«™åˆ—è¡¨å–äº¤é›†
        field_list = [i for i in field_list if i in number_website_list]
        if "official" in config.website_set:  # ä¼˜å…ˆä½¿ç”¨å®˜æ–¹ç½‘ç«™
            field_list.insert(0, "official")
        # æŒ‡å®šå­—æ®µæ’é™¤ç½‘ç«™åˆ—è¡¨
        field_ex_list = clean_list(getattr(config, f"{field_no_zh}_website_exclude", "").split(","))
        # æ‰€æœ‰è®¾å®šçš„æœ¬å­—æ®µæ¥æºå¤±è´¥æ—¶, æ˜¯å¦ç»§ç»­ä½¿ç”¨ç±»å‹ç½‘ç«™è¡¥å…¨
        include_others = field == "title" or field in config.whole_fields
        if include_others and field != "trailer":  # å–å‰©ä½™æœªç›¸äº¤ç½‘ç«™ï¼Œ trailer ä¸å–æœªç›¸äº¤ç½‘ç«™
            field_list.extend([i for i in number_website_list if i not in field_list])
        # æ’é™¤æŒ‡å®šç½‘ç«™
        field_list = [i for i in field_list if i not in field_ex_list]
        # ç‰¹æ®Šå¤„ç†
        # mgstage ç´ äººç•ªå·æ£€æŸ¥
        if short_number:
            not_frist_field_list = ["title", "actor"]  # è¿™äº›å­—æ®µä»¥å¤–ï¼Œç´ äººæŠŠ mgstage æ”¾åœ¨ç¬¬ä¸€ä½
            if field not in not_frist_field_list and "mgstage" in field_list:
                field_list.remove("mgstage")
                field_list.insert(0, "mgstage")
        # faleno.jp ç•ªå·æ£€æŸ¥ dldss177 dhla009
        elif re.findall(r"F[A-Z]{2}SS", number):
            field_list = _deal_some_list(field, "faleno", field_list)
        # dahlia-av.jp ç•ªå·æ£€æŸ¥
        elif number.startswith("DLDSS") or number.startswith("DHLA"):
            field_list = _deal_some_list(field, "dahlia", field_list)
        # fantastica ç•ªå·æ£€æŸ¥ FAVIã€FAAPã€FAPLã€FAKGã€FAHOã€FAVAã€FAKYã€FAMIã€FAITã€FAKAã€FAMOã€FASOã€FAIHã€FASHã€FAKSã€FAAN
        elif (
            re.search(r"FA[A-Z]{2}-?\d+", number.upper())
            or number.upper().startswith("CLASS")
            or number.upper().startswith("FADRV")
            or number.upper().startswith("FAPRO")
            or number.upper().startswith("FAKWM")
            or number.upper().startswith("PDS")
        ):
            field_list = _deal_some_list(field, "fantastica", field_list)
        return field_list

    # è·å–ä½¿ç”¨çš„ç½‘ç«™
    all_fields = [f for f in ManualConfig.CONFIG_DATA_FIELDS if f not in none_fields]  # å»é™¤ä¸ä¸“é—¨åˆ®å‰Šçš„å­—æ®µ
    if scrape_like == "speed":  # å¿«é€Ÿæ¨¡å¼
        all_field_websites = {field: number_website_list for field in all_fields}
    else:  # å…¨éƒ¨æ¨¡å¼
        # å„å­—æ®µç½‘ç«™åˆ—è¡¨
        all_field_websites = {field: get_field_websites(field) for field in all_fields}
        if config.outline_language == "jp" and "outline_zh" in all_field_websites:
            del all_field_websites["outline_zh"]
        if config.title_language == "jp" and "title_zh" in all_field_websites:
            del all_field_websites["title_zh"]

    # å„å­—æ®µè¯­è¨€, æœªæŒ‡å®šåˆ™é»˜è®¤ä¸º "any"
    all_field_languages = {field: getattr(config, f"{field}_language", "any") for field in all_fields}
    all_field_languages["title_zh"] = config.title_language
    all_field_languages["outline_zh"] = config.outline_language

    # å¤„ç†é…ç½®é¡¹ä¸­æ²¡æœ‰çš„å­—æ®µ
    # originaltitle çš„ç½‘ç«™ä¼˜å…ˆçº§åŒ title, è¯­è¨€ä¸º jp
    all_field_websites["originaltitle"] = all_field_websites.get("title", number_website_list)
    all_field_languages["originaltitle"] = "jp"
    all_field_websites["originalplot"] = all_field_websites.get("outline", number_website_list)
    all_field_languages["originalplot"] = "jp"

    # å„å­—æ®µçš„å–å€¼ä¼˜å…ˆçº§ (ç½‘ç«™, è¯­è¨€) å¯¹
    all_field_website_lang_pairs: dict[str, list[tuple[str, str]]] = {}
    for field, websites in all_field_websites.items():
        language = all_field_languages[field]
        all_field_website_lang_pairs[field] = []
        for website in websites:
            pair = (website, language)
            if website not in MULTI_LANGUAGE_WEBSITES:
                pair = (website, "")  # å•è¯­è¨€ç½‘ç«™, è¯­è¨€å‚æ•°æ— æ„ä¹‰
            all_field_website_lang_pairs[field].append(pair)

    # ç¼“å­˜å·²è¯·æ±‚çš„ç½‘ç«™ç»“æœ
    all_res: dict[tuple[str, str], dict] = {}

    reduced: CrawlersResult = new_json_data()  # éªŒè¯ JsonData å’Œ CrawlersResult ä¸€è‡´, åˆå§‹åŒ–æ‰€æœ‰å­—æ®µ
    reduced.update(task_input)  # å¤åˆ¶è¾“å…¥æ•°æ®

    # æ— ä¼˜å…ˆçº§è®¾ç½®çš„å­—æ®µçš„é»˜è®¤é…ç½®
    default_website_lang_pairs = [
        (w, "") if w not in MULTI_LANGUAGE_WEBSITES else (w, "any") for w in number_website_list
    ]

    # æŒ‰å­—æ®µåˆ†åˆ«å¤„ç†ï¼Œæ¯ä¸ªå­—æ®µæŒ‰ä¼˜å…ˆçº§å°è¯•è·å–
    for field in ManualConfig.CRAWLER_DATA_FIELDS:  # ä¸ CONFIG_DATA_FIELDS ä¸å®Œå…¨ä¸€è‡´
        # è·å–è¯¥å­—æ®µçš„ä¼˜å…ˆçº§åˆ—è¡¨
        sources = all_field_website_lang_pairs.get(field, default_website_lang_pairs)

        # å¦‚æœtitle_languageä¸æ˜¯jpï¼Œåˆ™å…è®¸ä»title_zhæ¥æºè·å–title
        if field == "title" and config.title_language != "jp":
            sources = sources + all_field_website_lang_pairs.get("title_zh", [])
        # å¦‚æœoutline_languageä¸æ˜¯jpï¼Œåˆ™å…è®¸ä»outline_zhæ¥æºè·å–outline
        elif field == "outline" and config.outline_language != "jp":
            sources = sources + all_field_website_lang_pairs.get("outline_zh", [])

        LogBuffer.info().write(
            f"\n\n    ğŸ™‹ğŸ»â€ {field} \n    ====================================\n"
            f"    ğŸŒ æ¥æºä¼˜å…ˆçº§ï¼š{' -> '.join(i[0] + f'({i[1]})' * bool(i[1]) for i in sources)}"
        )

        # æŒ‰ä¼˜å…ˆçº§ä¾æ¬¡å°è¯•è·å–å­—æ®µå€¼
        for website, language in sources:
            # æ£€æŸ¥æ˜¯å¦å·²ç»è¯·æ±‚è¿‡è¯¥ç½‘ç«™
            key = (website, language)

            # å¦‚æœç½‘ç«™ä¸æ”¯æŒå¤šè¯­è¨€ï¼Œæ ‡å‡†åŒ–key
            if website not in MULTI_LANGUAGE_WEBSITES:
                key = (website, "")

            # å¦‚æœå·²æœ‰è¯¥ç½‘ç«™æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨
            if key in all_res:
                site_data = all_res[key]
            else:
                # å¤„ç†å¤šè¯­è¨€ç½‘ç«™çš„ç‰¹æ®Šæƒ…å†µ
                if website in MULTI_LANGUAGE_WEBSITES:
                    # å¯¹äºå¤šè¯­è¨€ç½‘ç«™ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦è¯·æ±‚jpè¯­è¨€
                    if language == "any" and all((website, lang) not in all_res for lang in ["jp", "zh_cn", "zh_tw"]):
                        # æ·»åŠ ä¸€ä¸ªjpè¯­è¨€çš„è¯·æ±‚
                        language = "jp"
                        key = (website, language)

                    # å¯¹äºiqqtvï¼Œå¦‚æœè¯·æ±‚ä¸­æ–‡æ—¶å·²ç»æœ‰jpæ•°æ®ï¼Œå¯ä»¥è·³è¿‡jpè¯·æ±‚
                    if (
                        website == "iqqtv"
                        and language == "jp"
                        and any((website, lang) in all_res for lang in ["zh_cn", "zh_tw"])
                    ):
                        continue

                    # è·³è¿‡anyè¯­è¨€çš„è¯·æ±‚ï¼Œå› ä¸ºä¼šé€šè¿‡å…·ä½“è¯­è¨€è¯·æ±‚
                    if language == "any":
                        continue

                # å¦‚æœç½‘ç«™æ•°æ®å°šæœªè¯·æ±‚ï¼Œåˆ™è¿›è¡Œè¯·æ±‚
                try:
                    web_data = await _call_crawler(task_input, website, language, config.title_language)

                    # å¤„ç†å¹¶ä¿å­˜ç»“æœ
                    if website not in MULTI_LANGUAGE_WEBSITES:
                        # å•è¯­è¨€ç½‘ç«™, åªå–ç¬¬ä¸€ä¸ªè¯­è¨€çš„æ•°æ®
                        all_res[(website, "")] = next(iter(web_data[website].values()))
                    else:
                        # å¤šè¯­è¨€ç½‘ç«™ï¼Œä¿å­˜æ‰€æœ‰è¯­è¨€çš„æ•°æ®
                        for lang, data in web_data[website].items():
                            all_res[(website, lang)] = data
                            # åŒæ—¶ä¸ºanyè¯­è¨€æ·»åŠ ä¸€ä»½æ•°æ®
                            if (website, "any") not in all_res:
                                all_res[(website, "any")] = data

                    # æ›´æ–°keyä»¥ä¾¿åç»­ä½¿ç”¨
                    if website not in MULTI_LANGUAGE_WEBSITES:
                        key = (website, "")
                except Exception as e:
                    LogBuffer.info().write(f"\n    ğŸ”´ {website} (å¼‚å¸¸: {str(e)})")
                    continue

            # è·å–ç½‘ç«™æ•°æ®
            site_data = all_res.get(key, {})
            if not site_data or not site_data.get("title", "") or not site_data.get(field, ""):
                LogBuffer.info().write(f"\n    ğŸ”´ {website} (å¤±è´¥)")
                continue

            # è¯­è¨€æ£€æµ‹é€»è¾‘
            if config.scrape_like != "speed":
                if field in ["title", "outline", "originaltitle", "originalplot"]:
                    lang = all_field_languages.get(field, "jp")
                    if website in ["airav_cc", "iqqtv", "airav", "avsex", "javlibrary", "lulubar"]:  # why?
                        if langid.classify(site_data[field])[0] != "ja":
                            if lang == "jp":
                                LogBuffer.info().write(f"\n    ğŸ”´ {website} (å¤±è´¥ï¼Œæ£€æµ‹ä¸ºéæ—¥æ–‡ï¼Œè·³è¿‡ï¼)")
                                continue
                        elif lang != "jp":
                            LogBuffer.info().write(f"\n    ğŸ”´ {website} (å¤±è´¥ï¼Œæ£€æµ‹ä¸ºæ—¥æ–‡ï¼Œè·³è¿‡ï¼)")
                            continue

            # æ·»åŠ æ¥æºä¿¡æ¯
            if field in ["poster", "thumb", "extrafanart", "trailer", "outline"]:
                reduced[field + "_from"] = website

            if field == "poster":
                reduced["image_download"] = site_data["image_download"]
            elif field == "thumb":
                # è®°å½•æ‰€æœ‰ thumb url ä»¥ä¾¿åç»­ä¸‹è½½
                reduced["thumb_list"].append((website, site_data["thumb"]))
            elif field == "actor":
                if isinstance(site_data["actor"], list):
                    # å¤„ç† actor ä¸ºåˆ—è¡¨çš„æƒ…å†µ
                    site_data["actor"] = ",".join(site_data["actor"])
                reduced["all_actor"] = reduced.get("all_actor", site_data["actor"])
                reduced["all_actor_photo"] = reduced.get("all_actor_photo", site_data.get("actor_photo", ""))
                # è®°å½•æ‰€æœ‰ç½‘ç«™çš„ actor ç”¨äº Amazon æœå›¾, å› ä¸ºæœ‰çš„ç½‘ç«™ actor ä¸å¯¹
                reduced["actor_amazon"].extend(site_data["actor"].split(","))
            elif field == "originaltitle" and site_data.get("actor", ""):
                reduced["amazon_orginaltitle_actor"] = site_data["actor"].split(",")[0]

            # ä¿å­˜æ•°æ®
            reduced[field] = site_data[field]
            reduced["fields_info"] += f"\n     {field:<13}: {website}" + f" ({language})" * bool(language)
            LogBuffer.info().write(f"\n    ğŸŸ¢ {website} (æˆåŠŸ)\n     â†³ {reduced[field]}")

            # æ‰¾åˆ°æœ‰æ•ˆæ•°æ®ï¼Œè·³å‡ºå¾ªç¯ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªå­—æ®µ
            break
        else:  # æ‰€æœ‰æ¥æºéƒ½æ— æ­¤å­—æ®µ
            reduced["fields_info"] += f"\n     {field:<13}: {'-----'} ({'not found'})"

    # å¤„ç† year
    if reduced.get("year", "") and (r := re.search(r"\d{4}", reduced.get("release", ""))):
        reduced["year"] = r.group()

    # å¤„ç† numberï¼šç´ äººå½±ç‰‡æ—¶ä½¿ç”¨æœ‰æ•°å­—å‰ç¼€çš„number
    if short_number:
        reduced["number"] = number

    # å¤„ç† javdbid
    javdb_key = ("javdb", "")
    reduced["javdbid"] = all_res.get(javdb_key, {}).get("javdbid", "")

    # todo ç”±äºå¼‚æ­¥, æ­¤å¤„æ—¥å¿—æ··ä¹±. éœ€ç§»é™¤ LogBuffer.req(), æ”¹ä¸ºè¿”å›æ—¥å¿—ä¿¡æ¯
    reduced["fields_info"] = f"\n ğŸŒ [website] {LogBuffer.req().get().strip('-> ')}{reduced['fields_info']}"

    return reduced


async def _call_specific_crawler(task_input: JsonData, website: str) -> CrawlersResult:
    file_number = task_input["number"]
    short_number = task_input["short_number"]

    title_language = config.title_language
    org_language = title_language
    outline_language = config.outline_language
    actor_language = config.actor_language
    tag_language = config.tag_language
    series_language = config.series_language
    studio_language = config.studio_language
    publisher_language = config.publisher_language
    director_language = config.director_language
    if website not in ["airav_cc", "iqqtv", "airav", "avsex", "javlibrary", "mdtv", "madouqu", "lulubar"]:
        title_language = "jp"
        outline_language = "jp"
        actor_language = "jp"
        tag_language = "jp"
        series_language = "jp"
        studio_language = "jp"
        publisher_language = "jp"
        director_language = "jp"
    elif website == "mdtv":
        title_language = "zh_cn"
        outline_language = "zh_cn"
        actor_language = "zh_cn"
        tag_language = "zh_cn"
        series_language = "zh_cn"
        studio_language = "zh_cn"
        publisher_language = "zh_cn"
        director_language = "zh_cn"
    web_data = await _call_crawler(task_input, website, title_language, org_language)
    web_data_json = web_data.get(website, {}).get(title_language, {})
    res = task_input.copy()
    res.update(web_data_json)
    if not res["title"]:
        return res
    if outline_language != title_language:
        web_data_json = web_data[website][outline_language]
        if web_data_json["outline"]:
            res["outline"] = web_data_json["outline"]
    if actor_language != title_language:
        web_data_json = web_data[website][actor_language]
        if web_data_json["actor"]:
            res["actor"] = web_data_json["actor"]
    if tag_language != title_language:
        web_data_json = web_data[website][tag_language]
        if web_data_json["tag"]:
            res["tag"] = web_data_json["tag"]
    if series_language != title_language:
        web_data_json = web_data[website][series_language]
        if web_data_json["series"]:
            res["series"] = web_data_json["series"]
    if studio_language != title_language:
        web_data_json = web_data[website][studio_language]
        if web_data_json["studio"]:
            res["studio"] = web_data_json["studio"]
    if publisher_language != title_language:
        web_data_json = web_data[website][publisher_language]
        if web_data_json["publisher"]:
            res["publisher"] = web_data_json["publisher"]
    if director_language != title_language:
        web_data_json = web_data[website][director_language]
        if web_data_json["director"]:
            res["director"] = web_data_json["director"]
    if res["thumb"]:
        res["thumb_list"] = [(website, res["thumb"])]

    # åŠ å…¥æ¥æºä¿¡æ¯
    res["outline_from"] = website
    res["poster_from"] = website
    res["thumb_from"] = website
    res["extrafanart_from"] = website
    res["trailer_from"] = website
    # todo
    res["fields_info"] = f"\n ğŸŒ [website] {LogBuffer.req().get().strip('-> ')}"

    if short_number:
        res["number"] = file_number

    temp_actor = (
        web_data[website]["jp"]["actor"]
        + ","
        + web_data[website]["zh_cn"]["actor"]
        + ","
        + web_data[website]["zh_tw"]["actor"]
    )
    res["actor_amazon"] = []
    [res["actor_amazon"].append(i) for i in temp_actor.split(",") if i and i not in res["actor_amazon"]]
    res["all_actor"] = res["all_actor"] if res.get("all_actor") else web_data_json["actor"]
    res["all_actor_photo"] = res["all_actor_photo"] if res.get("all_actor_photo") else web_data_json["actor_photo"]

    return res


async def _crawl(task_input: JsonData, website_name: str) -> CrawlersResult:  # ä»JSONè¿”å›å…ƒæ•°æ®
    file_number = task_input["number"]
    file_path = task_input["file_path"]
    short_number = task_input["short_number"]
    appoint_number = task_input["appoint_number"]
    appoint_url = task_input["appoint_url"]
    has_sub = task_input["has_sub"]
    c_word = task_input["c_word"]
    leak = task_input["leak"]
    wuma = task_input["wuma"]
    youma = task_input["youma"]
    cd_part = task_input["cd_part"]
    destroyed = task_input["destroyed"]
    mosaic = task_input["mosaic"]
    version = task_input["version"]
    # task_input["title"] = ""
    # task_input["fields_info"] = ""
    # task_input["all_actor"] = ""
    # task_input["all_actor_photo"] = {}
    # ================================================ç½‘ç«™è§„åˆ™æ·»åŠ å¼€å§‹================================================

    if website_name == "all":  # ä»å…¨éƒ¨ç½‘ç«™åˆ®å‰Š
        # =======================================================================å…ˆåˆ¤æ–­æ˜¯ä¸æ˜¯å›½äº§ï¼Œé¿å…æµªè´¹æ—¶é—´
        if (
            mosaic == "å›½äº§"
            or mosaic == "åœ‹ç”¢"
            or (re.search(r"([^A-Z]|^)MD[A-Z-]*\d{4,}", file_number) and "MDVR" not in file_number)
            or re.search(r"MKY-[A-Z]+-\d{3,}", file_number)
        ):
            task_input["mosaic"] = "å›½äº§"
            website_list = config.website_guochan.split(",")
            res = await _call_crawlers(task_input, website_list)

        # =======================================================================kin8
        elif file_number.startswith("KIN8"):
            website_name = "kin8"
            res = await _call_specific_crawler(task_input, website_name)

        # =======================================================================åŒäºº
        elif file_number.startswith("DLID"):
            website_name = "getchu"
            res = await _call_specific_crawler(task_input, website_name)

        # =======================================================================é‡Œç•ª
        elif "getchu" in file_path.lower() or "é‡Œç•ª" in file_path or "è£ç•ª" in file_path:
            website_name = "getchu_dmm"
            res = await _call_specific_crawler(task_input, website_name)

        # =======================================================================Mywife No.1111
        elif "mywife" in file_path.lower():
            website_name = "mywife"
            res = await _call_specific_crawler(task_input, website_name)

        # =======================================================================FC2-111111
        elif "FC2" in file_number.upper():
            file_number_1 = re.search(r"\d{5,}", file_number)
            if file_number_1:
                file_number_1.group()
                website_list = config.website_fc2.split(",")
                res = await _call_crawlers(task_input, website_list)
            else:
                LogBuffer.error().write(f"æœªè¯†åˆ«åˆ°FC2ç•ªå·ï¼š{file_number}")
                res = task_input.copy()

        # =======================================================================sexart.15.06.14
        elif re.search(r"[^.]+\.\d{2}\.\d{2}\.\d{2}", file_number) or (
            "æ¬§ç¾" in file_path and "ä¸œæ¬§ç¾" not in file_path
        ):
            website_list = config.website_oumei.split(",")
            res = await _call_crawlers(task_input, website_list)

        # =======================================================================æ— ç æŠ“å–:111111-111,n1111,HEYZO-1111,SMD-115
        elif mosaic == "æ— ç " or mosaic == "ç„¡ç¢¼":
            website_list = config.website_wuma.split(",")
            res = await _call_crawlers(task_input, website_list)

        # =======================================================================259LUXU-1111
        elif short_number or "SIRO" in file_number.upper():
            website_list = config.website_suren.split(",")
            res = await _call_crawlers(task_input, website_list)

        # =======================================================================ssni00321
        elif re.match(r"\D{2,}00\d{3,}", file_number) and "-" not in file_number and "_" not in file_number:
            website_list = ["dmm"]
            res = await _call_crawlers(task_input, website_list)

        # =======================================================================å‰©ä¸‹çš„ï¼ˆå«åŒ¹é…ä¸äº†ï¼‰çš„æŒ‰æœ‰ç æ¥åˆ®å‰Š
        else:
            website_list = config.website_youma.split(",")
            if "official" in config.website_set:  # ä¼˜å…ˆä½¿ç”¨å®˜æ–¹ç½‘ç«™
                website_list.insert(0, "official")
            res = await _call_crawlers(task_input, website_list)
    else:
        res = await _call_specific_crawler(task_input, website_name)

    # ================================================ç½‘ç«™è¯·æ±‚ç»“æŸ================================================
    # ======================================è¶…æ—¶æˆ–æœªæ‰¾åˆ°è¿”å›
    if res["title"] == "":
        return res

    number = res["number"]
    if appoint_number:
        number = appoint_number

    # é©¬èµ›å…‹
    if leak:
        res["mosaic"] = "æ— ç æµå‡º"
    elif destroyed:
        res["mosaic"] = "æ— ç ç ´è§£"
    elif wuma:
        res["mosaic"] = "æ— ç "
    elif youma:
        res["mosaic"] = "æœ‰ç "
    elif mosaic:
        res["mosaic"] = mosaic
    if not res.get("mosaic"):
        if is_uncensored(number):
            res["mosaic"] = "æ— ç "
        else:
            res["mosaic"] = "æœ‰ç "
    print(number, cd_part, res["mosaic"], LogBuffer.req().get().strip("-> "))

    # è½¦ç‰Œå­—æ¯
    letters = get_number_letters(number)

    # åŸæ ‡é¢˜ï¼Œç”¨äºamazonæœç´¢
    originaltitle = res.get("originaltitle", "")
    res["originaltitle_amazon"] = originaltitle
    if res.get("actor_amazon", []):
        for each in res["actor_amazon"]:  # å»é™¤æ¼”å‘˜åï¼Œé¿å…æœç´¢ä¸åˆ°
            try:
                end_actor = re.compile(rf" {each}$")
                res["originaltitle_amazon"] = re.sub(end_actor, "", res["originaltitle_amazon"])
            except Exception:
                pass

    # VR æ—¶ä¸‹è½½å°å°é¢
    if "VR" in number:
        res["image_download"] = True

    # è¿”å›å¤„ç†åçš„json_data
    res["number"] = number
    res["letters"] = letters
    res["has_sub"] = has_sub
    res["c_word"] = c_word
    res["leak"] = leak
    res["wuma"] = wuma
    res["youma"] = youma
    res["cd_part"] = cd_part
    res["destroyed"] = destroyed
    res["version"] = version
    res["file_path"] = file_path
    res["appoint_number"] = appoint_number
    res["appoint_url"] = appoint_url

    return res


def _get_website_name(task_input: JsonData, file_mode: FileMode) -> str:
    # è·å–åˆ®å‰Šç½‘ç«™
    website_name = "all"
    if file_mode == FileMode.Single:  # åˆ®å‰Šå•æ–‡ä»¶ï¼ˆå·¥å…·é¡µé¢ï¼‰
        website_name = Flags.website_name
    elif file_mode == FileMode.Again:  # é‡æ–°åˆ®å‰Š
        website_temp = task_input["website_name"]
        if website_temp:
            website_name = website_temp
    elif config.scrape_like == "single":
        website_name = config.website_single

    return website_name


async def crawl(task_input: JsonData, file_mode: FileMode) -> CrawlersResult:
    # ä»æŒ‡å®šç½‘ç«™è·å–json_data
    website_name = _get_website_name(task_input, file_mode)
    res = await _crawl(task_input, website_name)
    return _deal_res(res)


def _deal_res(res: CrawlersResult) -> CrawlersResult:
    # æ ‡é¢˜ä¸ºç©ºè¿”å›
    title = res["title"]
    if not title:
        return res

    # æ¼”å‘˜
    res["actor"] = (
        str(res["actor"])
        .strip(" [ ]")
        .replace("'", "")
        .replace(", ", ",")
        .replace("<", "(")
        .replace(">", ")")
        .strip(",")
    )  # åˆ—è¡¨è½¬å­—ç¬¦ä¸²ï¼ˆé¿å…ä¸ªåˆ«ç½‘ç«™åˆ®å‰Šè¿”å›çš„æ˜¯åˆ—è¡¨ï¼‰

    # æ ‡ç­¾
    tag = (
        str(res["tag"]).strip(" [ ]").replace("'", "").replace(", ", ",")
    )  # åˆ—è¡¨è½¬å­—ç¬¦ä¸²ï¼ˆé¿å…ä¸ªåˆ«ç½‘ç«™åˆ®å‰Šè¿”å›çš„æ˜¯åˆ—è¡¨ï¼‰
    tag = re.sub(r",\d+[kKpP],", ",", tag)
    tag_rep_word = [",HDé«˜ç”»è´¨", ",HDé«˜ç•«è³ª", ",é«˜ç”»è´¨", ",é«˜ç•«è³ª"]
    for each in tag_rep_word:
        if tag.endswith(each):
            tag = tag.replace(each, "")
        tag = tag.replace(each + ",", ",")
    res["tag"] = tag

    # posterå›¾
    if not res.get("poster"):
        res["poster"] = ""

    # å‘è¡Œæ—¥æœŸ
    release = res["release"]
    if release:
        release = release.replace("/", "-").strip(". ")
        if len(release) < 10:
            release_list = re.findall(r"(\d{4})-(\d{1,2})-(\d{1,2})", release)
            if release_list:
                r_year, r_month, r_day = release_list[0]
                r_month = "0" + r_month if len(r_month) == 1 else r_month
                r_day = "0" + r_day if len(r_day) == 1 else r_day
                release = r_year + "-" + r_month + "-" + r_day
    res["release"] = release

    # è¯„åˆ†
    if res.get("score", ""):
        res["score"] = "%.1f" % float(res.get("score", 0))

    # publisher
    if not res.get("publisher", ""):
        res["publisher"] = res["studio"]

    # å­—ç¬¦è½¬ä¹‰ï¼Œé¿å…æ˜¾ç¤ºé—®é¢˜
    key_word = [
        "title",
        "originaltitle",
        "number",
        "outline",
        "originalplot",
        "actor",
        "tag",
        "series",
        "director",
        "studio",
        "publisher",
    ]
    rep_word = {
        "&amp;": "&",
        "&lt;": "<",
        "&gt;": ">",
        "&apos;": "'",
        "&quot;": '"',
        "&lsquo;": "ã€Œ",
        "&rsquo;": "ã€",
        "&hellip;": "â€¦",
        "<br/>": "",
        "ãƒ»": "Â·",
        "â€œ": "ã€Œ",
        "â€": "ã€",
        "...": "â€¦",
        "\xa0": "",
        "\u3000": "",
        "\u2800": "",
    }
    for each in key_word:
        for key, value in rep_word.items():
            res[each] = res[each].replace(key, value)

    # å‘½åè§„åˆ™
    naming_media = config.naming_media
    naming_file = config.naming_file
    folder_name = config.folder_name
    res["naming_media"] = naming_media
    res["naming_file"] = naming_file
    res["folder_name"] = folder_name
    return res
