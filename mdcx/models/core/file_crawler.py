import asyncio
import os
import re
from itertools import chain
from typing import TYPE_CHECKING

from patchright.async_api import Error as PatchrightError

from mdcx.config.models import Language, Website
from mdcx.gen.field_enums import CrawlerResultFields
from mdcx.manual import ManualConfig
from mdcx.models.enums import FileMode
from mdcx.models.flags import Flags
from mdcx.models.types import CrawlerInput, CrawlerResponse, CrawlerResult, CrawlersResult, CrawlTask
from mdcx.number import is_uncensored
from mdcx.utils.dataclass import update

if TYPE_CHECKING:
    from mdcx.config.models import Config
    from mdcx.crawler import CrawlerProviderProtocol


MULTI_LANGUAGE_WEBSITES = [  # æ”¯æŒå¤šè¯­è¨€, language å‚æ•°æœ‰æ„ä¹‰
    Website.AIRAV_CC,
    Website.AIRAV,
    Website.IQQTV,
    Website.JAVLIBRARY,
]


def sprint_source(website: Website, language: Language) -> str:
    if language == Language.UNDEFINED:
        return f"{website.value}"
    return f"{website.value} ({language.value})"


def _deal_res(res: CrawlersResult) -> CrawlersResult:
    # æ ‡ç­¾
    tag = re.sub(r",\d+[kKpP],", ",", res.tag)
    tag_rep_word = [",HDé«˜ç”»è´¨", ",HDé«˜ç•«è³ª", ",é«˜ç”»è´¨", ",é«˜ç•«è³ª"]
    for each in tag_rep_word:
        if tag.endswith(each):
            tag = tag.replace(each, "")
        tag = tag.replace(each + ",", ",")
    res.tag = tag

    # å‘è¡Œæ—¥æœŸ
    release = res.release
    if release:
        release = release.replace("/", "-").strip(". ")
        if len(release) < 10:
            release_list = re.findall(r"(\d{4})-(\d{1,2})-(\d{1,2})", release)
            if release_list:
                r_year, r_month, r_day = release_list[0]
                r_month = "0" + r_month if len(r_month) == 1 else r_month
                r_day = "0" + r_day if len(r_day) == 1 else r_day
                release = r_year + "-" + r_month + "-" + r_day
    res.release = release

    # è¯„åˆ†
    if res.score:
        res.score = f"{float(res.score):.1f}"

    # publisher
    if not res.publisher:
        res.publisher = res.studio

    # å­—ç¬¦è½¬ä¹‰ï¼Œé¿å…æ˜¾ç¤ºé—®é¢˜
    key_word = [
        "title",
        "originaltitle",
        "number",
        "outline",
        "originalplot",
        "series",
        "studio",
        "publisher",
    ]
    rep_word = {
        "&amp;": "&",
        "&lt;": "<",
        "&gt;": ">",
        "&apos;": "'",
        "&quot;": '"',
        "&lsquo;": "ã€Œ",
        "&rsquo;": "ã€",
        "&hellip;": "â€¦",
        "<br/>": "",
        "ãƒ»": "Â·",
        """: "ã€Œ",
        """: "ã€",
        "...": "â€¦",
        "\xa0": "",
        "\u3000": "",
        "\u2800": "",
    }
    for each in key_word:
        for key, value in rep_word.items():
            # res[each] = res[each].replace(key, value)
            setattr(res, each, getattr(res, each).replace(key, value))

    return res


class FileScraper:
    def __init__(self, config: "Config", crawler_provider: "CrawlerProviderProtocol"):
        self.config = config
        self.crawler_provider = crawler_provider

    async def _call_crawler(
        self, task_input: CrawlerInput, website: Website, timeout: float | None = 30
    ) -> CrawlerResponse:
        """
        è°ƒç”¨æŒ‡å®šç½‘ç«™çš„çˆ¬è™«å‡½æ•°

        Args:
            task_input (CrawlerInput): åŒ…å«çˆ¬è™«æ‰€éœ€çš„è¾“å…¥æ•°æ®
            website (str): ç½‘ç«™åç§°
            timeout (float | None): è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼Œé»˜è®¤ä¸º30ç§’

        Raises:
            asyncio.TimeoutError: å¦‚æœè¯·æ±‚è¶…æ—¶
            Exception: çˆ¬è™«å‡½æ•°æŠ›å‡ºçš„å¼‚å¸¸
        """
        short_number = task_input.short_number

        # 259LUXU-1111ï¼Œ mgstage å’Œ avsex ä¹‹å¤–ä½¿ç”¨ LUXU-1111ï¼ˆç´ äººç•ªå·æ—¶ï¼Œshort_numberæœ‰å€¼ï¼Œä¸å¸¦å‰ç¼€æ•°å­—ï¼›åä¹‹ï¼Œshort_numberä¸ºç©º)
        if short_number and website != "mgstage" and website != "avsex":
            task_input.number = short_number

        c = await self.crawler_provider.get(website)

        # å¯¹çˆ¬è™«å‡½æ•°è°ƒç”¨æ·»åŠ è¶…æ—¶é™åˆ¶, è¶…æ—¶å¼‚å¸¸ç”±è°ƒç”¨è€…å¤„ç†
        if os.getenv("DEBUG"):
            timeout = None
        r = await asyncio.wait_for(c.run(task_input), timeout=timeout)
        return r

    async def _call_crawlers(self, task_input: CrawlerInput, type_sites: set[Website]) -> CrawlersResult:
        """
        è·å–ä¸€ç»„ç½‘ç«™çš„æ•°æ®ï¼šæŒ‰ç…§è®¾ç½®çš„ç½‘ç«™ç»„ï¼Œè¯·æ±‚å„å­—æ®µæ•°æ®ï¼Œå¹¶è¿”å›æœ€ç»ˆçš„æ•°æ®
        é‡‡ç”¨æŒ‰éœ€è¯·æ±‚ç­–ç•¥ï¼šä»…è¯·æ±‚å¿…è¦çš„ç½‘ç«™ï¼Œå¤±è´¥æ—¶æ‰è¯·æ±‚ä¸‹ä¸€ä¼˜å…ˆçº§ç½‘ç«™
        """
        all_res: dict[tuple[Website, Language], CrawlerResult] = {}
        failed: set[tuple[Website, Language]] = set()  # è®°å½•å¤±è´¥çš„ç½‘ç«™
        reduced = CrawlersResult.empty()
        req_info: list[str] = []  # è¯·æ±‚ä¿¡æ¯åˆ—è¡¨

        # æŒ‰å­—æ®µåˆ†åˆ«å¤„ç†ï¼Œæ¯ä¸ªå­—æ®µæŒ‰ä¼˜å…ˆçº§å°è¯•è·å–
        for field in ManualConfig.REDUCED_FIELDS:
            # è·å–è¯¥å­—æ®µçš„ä¼˜å…ˆçº§åˆ—è¡¨
            f_config = self.config.get_field_config(field)
            f_sites = [s for s in f_config.site_prority if s in type_sites]
            f_lang = f_config.language

            reduced.field_log += (
                f"\n\n    ğŸ“Œ {field} \n    ====================================\n"
                f"    ğŸŒ ä¼˜å…ˆçº§è®¾ç½®: {' -> '.join(s.value for s in f_sites)}"
            )

            # æŒ‰ä¼˜å…ˆçº§ä¾æ¬¡å°è¯•è·å–å­—æ®µå€¼
            for site in f_sites:
                # æ£€æŸ¥æ˜¯å¦å·²ç»è¯·æ±‚è¿‡è¯¥ç½‘ç«™
                key = (site, f_lang)

                # å¦‚æœç½‘ç«™ä¸æ”¯æŒå¤šè¯­è¨€, åˆ™ä½¿ç”¨ UNDEFINED
                if site not in MULTI_LANGUAGE_WEBSITES:
                    key = (site, Language.UNDEFINED)

                # å¦‚æœå·²æœ‰è¯¥ç½‘ç«™æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨
                if key in all_res:
                    site_data = all_res[key]
                elif key in failed:
                    # ä¸å†è¯·æ±‚å·²å¤±è´¥çš„ç½‘ç«™
                    reduced.field_log += f"\n    ğŸ”´ {site:<15} (å·²å¤±è´¥, è·³è¿‡)"
                    continue
                else:
                    # å¦‚æœç½‘ç«™æ•°æ®å°šæœªè¯·æ±‚ï¼Œåˆ™è¿›è¡Œè¯·æ±‚
                    try:
                        task_input.language = f_lang
                        task_input.org_language = f_lang
                        # å¤šè¯­è¨€ç½‘ç«™, æŒ‡å®šä¸€ä¸ªé»˜è®¤è¯­è¨€
                        if site in MULTI_LANGUAGE_WEBSITES and key[1] == Language.UNDEFINED:
                            task_input.language = Language.JP
                            task_input.org_language = Language.JP
                        web_data = await self._call_crawler(task_input, site)
                        req_info.append(f"{sprint_source(*key)} ({web_data.debug_info.execution_time:.2f}s)")
                        if web_data.data is None:
                            if e := web_data.debug_info.error:
                                raise e
                            raise ValueError(f"{site} è¿”å›äº†ç©ºæ•°æ®")
                        site_data = web_data.data
                        # å¤„ç†å¹¶ä¿å­˜ç»“æœ
                        all_res[key] = web_data.data
                        # å¤šè¯­è¨€ç½‘ç«™, å¦‚æœ undefined å°šä¸å­˜åœ¨, ä¹Ÿä½¿ç”¨å½“å‰è¯­è¨€æ•°æ®
                        if site in MULTI_LANGUAGE_WEBSITES and (site, Language.UNDEFINED) not in all_res:
                            all_res[(site, Language.UNDEFINED)] = web_data.data
                    except Exception as e:
                        if isinstance(e, PatchrightError):
                            if "BrowserType.launch: Executable doesn't exist" in e.message:
                                e = "æ‰¾ä¸åˆ° Chrome æµè§ˆå™¨, è¯·å®‰è£…æˆ–å…³é—­å¯¹åº”ç½‘ç«™çš„ use_browser é€‰é¡¹"
                        reduced.field_log += f"\n    ğŸ”´ {site:<15} (å¤±è´¥: {str(e)})"
                        failed.add(key)
                        continue

                # æ£€æŸ¥å­—æ®µæ•°æ®
                if not getattr(site_data, field.value, None):
                    reduced.field_log += f"\n    ğŸ”´ {site:<15} (æœªæ‰¾åˆ°)"
                    continue

                # æ·»åŠ æ¥æºä¿¡æ¯
                reduced.field_sources[field] = site.value

                if field == CrawlerResultFields.POSTER:
                    reduced.image_download = site_data.image_download
                elif field == CrawlerResultFields.ORIGINALTITLE and site_data.actor:
                    reduced.amazon_orginaltitle_actor = site_data.actor.split(",")[0]

                # ä¿å­˜æ•°æ®
                setattr(reduced, field.value, getattr(site_data, field.value))
                reduced.field_log += f"\n    ğŸŸ¢ {site}\n     â†³{getattr(reduced, field.value)}"
                # æ‰¾åˆ°æœ‰æ•ˆæ•°æ®ï¼Œè·³å‡ºå¾ªç¯ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªå­—æ®µ
                break
            else:  # æ‰€æœ‰æ¥æºéƒ½æ— æ­¤å­—æ®µ
                reduced.field_log += "\n    ğŸ”´ æ‰€æœ‰æ¥æºå‡æ— æ•°æ®"

        # éœ€å°½åŠ›æ”¶é›†çš„å­—æ®µ
        for data in all_res.values():
            # è®°å½•æ‰€æœ‰æ¥æºçš„ thumb url ä»¥ä¾¿åç»­ä¸‹è½½
            if data.thumb:
                reduced.thumb_list.append((data.source, data.thumb))
            # è®°å½•æ‰€æœ‰æ¥æºçš„ actor ç”¨äº Amazon æœå›¾
            if data.actor:
                reduced.actor_amazon.extend(data.actors)
        # å»é‡
        reduced.thumb_list = list(dict.fromkeys(reduced.thumb_list))  # ä¿åº
        reduced.actor_amazon = list(set(reduced.actor_amazon))

        # å¤„ç† year
        if reduced.year and (r := re.search(r"\d{4}", reduced.release)):
            reduced.year = r.group()

        # å¤„ç† javdbid
        if r := all_res.get((Website.JAVDB, Language.UNDEFINED)):
            reduced.javdbid = r.javdbid

        # ä½¿ç”¨ actors å­—æ®µè¡¥å…¨ all_actors, ç†æƒ³æƒ…å†µä¸‹å‰è€…åº”è¯¥æ˜¯åè€…çš„å­é›†
        # å¯¹ actors çš„æ‰€æœ‰åå¤„ç†éƒ½éœ€è¦åŒæ ·åœ°åº”ç”¨åˆ° all_actors
        reduced.all_actors = list(dict.fromkeys(chain(reduced.all_actors, reduced.actors)))

        reduced.site_log = f"\n ğŸŒ [website] {'-> '.join(req_info)}"

        return reduced

    async def _call_specific_crawler(self, task_input: CrawlerInput, website: Website) -> CrawlersResult:
        file_number = task_input.number
        short_number = task_input.short_number

        title_language = self.config.get_field_config(CrawlerResultFields.TITLE).language
        org_language = title_language

        if website not in ["airav_cc", "iqqtv", "airav", "avsex", "javlibrary", "mdtv", "madouqu", "lulubar"]:
            title_language = Language.JP

        elif website == "mdtv":
            title_language = Language.ZH_CN

        task_input.language = title_language
        task_input.org_language = org_language
        web_data = await self._call_crawler(task_input, website)
        web_data_json = web_data.data
        if web_data_json is None:
            return CrawlersResult.empty()

        res = update(CrawlersResult.empty(), web_data_json)
        if not res.title:
            return res
        if res.thumb:
            res.thumb_list = [(website, res.thumb)]

        # åŠ å…¥æ¥æºä¿¡æ¯
        res.field_sources = dict.fromkeys(CrawlerResultFields, website.value)

        res.site_log = (
            f"\n ğŸŒ [website] {sprint_source(website, title_language)} ({web_data.debug_info.execution_time:.2f}s)"
        )

        if short_number:
            res.number = file_number

        res.actor_amazon = web_data_json.actors
        res.all_actors = list(dict.fromkeys(chain(res.all_actors, web_data_json.actors)))

        return res

    async def _crawl(self, task_input: CrawlTask, website: Website | None) -> CrawlersResult:  # ä»JSONè¿”å›å…ƒæ•°æ®
        appoint_number = task_input.appoint_number
        destroyed = task_input.destroyed
        file_number = task_input.number
        file_path = task_input.file_path
        leak = task_input.leak
        mosaic = task_input.mosaic
        short_number = task_input.short_number
        wuma = task_input.wuma
        youma = task_input.youma

        # ================================================ç½‘ç«™è§„åˆ™æ·»åŠ å¼€å§‹================================================

        if website is None:  # ä»å…¨éƒ¨ç½‘ç«™åˆ®å‰Š
            # =======================================================================å…ˆåˆ¤æ–­æ˜¯ä¸æ˜¯å›½äº§ï¼Œé¿å…æµªè´¹æ—¶é—´
            if (
                mosaic == "å›½äº§"
                or mosaic == "åœ‹ç”¢"
                or (re.search(r"([^A-Z]|^)MD[A-Z-]*\d{4,}", file_number) and "MDVR" not in file_number)
                or re.search(r"MKY-[A-Z]+-\d{3,}", file_number)
            ):
                task_input.mosaic = "å›½äº§"
                res = await self._call_crawlers(task_input, self.config.website_guochan)

            # =======================================================================kin8
            elif file_number.startswith("KIN8"):
                website = Website.KIN8
                res = await self._call_specific_crawler(task_input, website)

            # =======================================================================åŒäºº
            elif file_number.startswith("DLID"):
                website = Website.GETCHU
                res = await self._call_specific_crawler(task_input, website)

            # =======================================================================é‡Œç•ª
            elif "getchu" in file_path.lower() or "é‡Œç•ª" in file_path or "è£ç•ª" in file_path:
                website = Website.GETCHU_DMM
                res = await self._call_specific_crawler(task_input, website)

            # =======================================================================Mywife No.1111
            elif "mywife" in file_path.lower():
                website = Website.MYWIFE
                res = await self._call_specific_crawler(task_input, website)

            # =======================================================================FC2-111111
            elif "FC2" in file_number.upper():
                file_number_1 = re.search(r"\d{5,}", file_number)
                if file_number_1:
                    file_number_1.group()
                    res = await self._call_crawlers(task_input, self.config.website_fc2)
                else:
                    raise Exception(f"æœªè¯†åˆ«çš„ FC2 ç•ªå·: {file_number}")

            # =======================================================================sexart.15.06.14
            elif re.search(r"[^.]+\.\d{2}\.\d{2}\.\d{2}", file_number) or (
                "æ¬§ç¾" in file_path and "ä¸œæ¬§ç¾" not in file_path
            ):
                res = await self._call_crawlers(task_input, self.config.website_oumei)

            # =======================================================================æ— ç æŠ“å–:111111-111,n1111,HEYZO-1111,SMD-115
            elif mosaic == "æ— ç " or mosaic == "ç„¡ç¢¼":
                res = await self._call_crawlers(task_input, self.config.website_wuma)

            # =======================================================================259LUXU-1111
            elif short_number or "SIRO" in file_number.upper():
                res = await self._call_crawlers(task_input, self.config.website_suren)

            # =======================================================================ssni00321
            elif re.match(r"\D{2,}00\d{3,}", file_number) and "-" not in file_number and "_" not in file_number:
                res = await self._call_crawlers(task_input, {Website.DMM})

            # =======================================================================å‰©ä¸‹çš„ï¼ˆå«åŒ¹é…ä¸äº†ï¼‰çš„æŒ‰æœ‰ç æ¥åˆ®å‰Š
            else:
                res = await self._call_crawlers(task_input, self.config.website_youma)
        else:
            res = await self._call_specific_crawler(task_input, website)

        # ================================================ç½‘ç«™è¯·æ±‚ç»“æŸ================================================
        # ======================================è¶…æ—¶æˆ–æœªæ‰¾åˆ°è¿”å›

        number = file_number  # res.number å®é™…ä¸Šå¹¶æœªè®¾ç½®, æ­¤å¤„å– file_number
        if appoint_number:
            number = appoint_number
        res.number = number  # æ­¤å¤„è®¾ç½®

        # é©¬èµ›å…‹
        if leak:
            res.mosaic = "æ— ç æµå‡º"
        elif destroyed:
            res.mosaic = "æ— ç ç ´è§£"
        elif wuma:
            res.mosaic = "æ— ç "
        elif youma:
            res.mosaic = "æœ‰ç "
        elif mosaic:
            res.mosaic = mosaic
        if not res.mosaic:
            if is_uncensored(number):
                res.mosaic = "æ— ç "
            else:
                res.mosaic = "æœ‰ç "

        # åŸæ ‡é¢˜ï¼Œç”¨äºamazonæœç´¢
        res.originaltitle_amazon = res.originaltitle
        if res.actor_amazon:
            for each in res.actor_amazon:  # å»é™¤æ¼”å‘˜åï¼Œé¿å…æœç´¢ä¸åˆ°
                try:
                    end_actor = re.compile(rf" {each}$")
                    res.originaltitle_amazon = re.sub(end_actor, "", res.originaltitle_amazon)
                except Exception:
                    pass

        # VR æ—¶ä¸‹è½½å°å°é¢
        if "VR" in number:
            res.image_download = True

        return res

    def _get_site(self, task_input: CrawlTask, file_mode: FileMode):
        # è·å–åˆ®å‰Šç½‘ç«™
        website_name = None
        if file_mode == FileMode.Single:  # åˆ®å‰Šå•æ–‡ä»¶ï¼ˆå·¥å…·é¡µé¢ï¼‰
            website_name = Flags.website_name
        elif file_mode == FileMode.Again:  # é‡æ–°åˆ®å‰Š
            website_temp = task_input.website_name
            if website_temp:
                website_name = website_temp
        elif self.config.scrape_like == "single":
            website_name = self.config.website_single

        return website_name

    async def run(self, task_input: CrawlTask, file_mode: FileMode) -> CrawlersResult:
        site = self._get_site(task_input, file_mode)
        if site is not None:
            site = Website(site)
        res = await self._crawl(task_input, site)
        return _deal_res(res)
