"""
åˆ®å‰Šè¿‡ç¨‹çš„ç½‘ç»œæ“ä½œ
"""

import asyncio
import os
import re
import shutil
import time
import urllib.parse
from asyncio import to_thread

import aiofiles
import aiofiles.os
from lxml import etree

from mdcx.config.enums import DownloadableFile, HDPicSource, WebsiteSet
from mdcx.config.manager import manager
from mdcx.models.base.web import (
    check_url,
    download_extrafanart_task,
    download_file_with_filepath,
    get_amazon_data,
    get_big_pic_by_google,
    get_dmm_trailer,
    get_imgsize,
)
from mdcx.models.core.image import cut_thumb_to_poster
from mdcx.models.flags import Flags
from mdcx.models.log_buffer import LogBuffer
from mdcx.models.types import CrawlersResult, OtherInfo
from mdcx.signals import signal
from mdcx.utils import convert_half, get_used_time, split_path
from mdcx.utils.file import check_pic_async, copy_file_async, delete_file_async, move_file_async


async def get_big_pic_by_amazon(result: CrawlersResult, originaltitle_amazon: str, actor_amazon: list[str]) -> str:
    if not originaltitle_amazon or not actor_amazon:
        return ""
    hd_pic_url = ""
    originaltitle_amazon = re.sub(r"ã€.*ã€‘", "", originaltitle_amazon)
    originaltitle_amazon_list = [originaltitle_amazon]
    for originaltitle_amazon in originaltitle_amazon_list:
        # éœ€è¦ä¸¤æ¬¡urlencodeï¼Œnb_sb_nossè¡¨ç¤ºæ— æ¨èæ¥æº
        url_search = (
            "https://www.amazon.co.jp/black-curtain/save-eligibility/black-curtain?returnUrl=/s?k="
            + urllib.parse.quote_plus(urllib.parse.quote_plus(originaltitle_amazon.replace("&", " ") + " [DVD]"))
            + "&ref=nb_sb_noss"
        )
        success, html_search = await get_amazon_data(url_search)

        # æ²¡æœ‰ç»“æœï¼Œå°è¯•æ‹†è¯ï¼Œé‡æ–°æœç´¢
        if (
            not success
            or "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒæ­£ã—ãå…¥åŠ›ã•ã‚Œã¦ã„ã¦ã‚‚ä¸€è‡´ã™ã‚‹å•†å“ãŒãªã„å ´åˆã¯ã€åˆ¥ã®è¨€è‘‰ã‚’ãŠè©¦ã—ãã ã•ã„ã€‚" in html_search
            and len(originaltitle_amazon_list) < 2
        ):
            for each_name in originaltitle_amazon.split(" "):
                if each_name not in originaltitle_amazon_list and (
                    len(each_name) > 8
                    or (not each_name.encode("utf-8").isalnum() and len(each_name) > 4)
                    and each_name not in actor_amazon
                ):
                    originaltitle_amazon_list.append(each_name)
            continue

        # æœ‰ç»“æœæ—¶ï¼Œæ£€æŸ¥ç»“æœ
        if result and html_search:
            html = etree.fromstring(html_search, etree.HTMLParser())
            originaltitle_amazon_half = convert_half(originaltitle_amazon)
            originaltitle_amazon_half_no_actor = originaltitle_amazon_half

            # æ ‡é¢˜ç¼©çŸ­åŒ¹é…ï¼ˆå¦‚æ— ç»“æœï¼Œåˆ™ä½¿ç”¨ç¼©å°æ ‡é¢˜å†æ¬¡æœç´¢ï¼‰
            if "æ¤œç´¢ã«ä¸€è‡´ã™ã‚‹å•†å“ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚" in html_search and len(originaltitle_amazon_list) < 2:
                short_originaltitle_amazon = html.xpath(
                    '//div[@class="a-section a-spacing-base a-spacing-top-base"]/span[@class="a-size-base a-color-base"]/text()'
                )
                if short_originaltitle_amazon:
                    short_originaltitle_amazon = short_originaltitle_amazon[0].upper().replace(" DVD", "")
                    if short_originaltitle_amazon in originaltitle_amazon.upper():
                        originaltitle_amazon_list.append(short_originaltitle_amazon)
                        short_originaltitle_amazon = convert_half(short_originaltitle_amazon)
                        if short_originaltitle_amazon in originaltitle_amazon_half:
                            originaltitle_amazon_half = short_originaltitle_amazon
                for each_name in originaltitle_amazon.split(" "):
                    if each_name not in originaltitle_amazon_list and (
                        len(each_name) > 8
                        or (not each_name.encode("utf-8").isalnum() and len(each_name) > 4)
                        and each_name not in actor_amazon
                    ):
                        originaltitle_amazon_list.append(each_name)

            # æ ‡é¢˜ä¸å¸¦æ¼”å‘˜ååŒ¹é…
            for each_actor in actor_amazon:
                originaltitle_amazon_half_no_actor = originaltitle_amazon_half_no_actor.replace(each_actor.upper(), "")

            # æ£€æŸ¥æœç´¢ç»“æœ
            actor_result_list = set()
            title_result_list = []
            # s-card-container s-overflow-hidden aok-relative puis-wide-grid-style puis-wide-grid-style-t2 puis-expand-height puis-include-content-margin puis s-latency-cf-section s-card-border
            pic_card = html.xpath('//div[@class="a-section a-spacing-base"]')
            for each in pic_card:  # tek-077
                pic_ver_list = each.xpath(
                    'div//a[@class="a-size-base a-link-normal s-underline-text s-underline-link-text s-link-style a-text-bold"]/text()'
                )
                pic_title_list = each.xpath(
                    'div//h2[@class="a-size-base-plus a-spacing-none a-color-base a-text-normal"]/span/text()'
                )
                pic_url_list = each.xpath('div//div[@class="a-section aok-relative s-image-square-aspect"]/img/@src')
                detail_url_list = each.xpath('div//a[@class="a-link-normal s-no-outline"]/@href')
                if len(pic_ver_list) and len(pic_url_list) and (len(pic_title_list) and len(detail_url_list)):
                    pic_ver = pic_ver_list[0]  # å›¾ç‰‡ç‰ˆæœ¬
                    pic_title = pic_title_list[0]  # å›¾ç‰‡æ ‡é¢˜
                    pic_url = pic_url_list[0]  # å›¾ç‰‡é“¾æ¥
                    detail_url = detail_url_list[0]  # è¯¦æƒ…é¡µé“¾æ¥ï¼ˆæœ‰æ—¶å¸¦æœ‰æ¼”å‘˜åï¼‰
                    if pic_ver in ["DVD", "Software Download"] and ".jpg" in pic_url:  # æ— å›¾æ—¶æ˜¯.gif
                        pic_title_half = convert_half(re.sub(r"ã€.*ã€‘", "", pic_title))
                        pic_title_half_no_actor = pic_title_half
                        for each_actor in actor_amazon:
                            pic_title_half_no_actor = pic_title_half_no_actor.replace(each_actor, "")

                        # åˆ¤æ–­æ ‡é¢˜æ˜¯å¦å‘½ä¸­
                        if (
                            originaltitle_amazon_half[:15] in pic_title_half
                            or originaltitle_amazon_half_no_actor[:15] in pic_title_half_no_actor
                        ):
                            detail_url = urllib.parse.unquote_plus(detail_url)
                            temp_title = re.findall(r"(.+)keywords=", detail_url)
                            temp_detail_url = (
                                temp_title[0] + pic_title_half if temp_title else detail_url + pic_title_half
                            )
                            url = re.sub(r"\._[_]?AC_[^\.]+\.", ".", pic_url)

                            # åˆ¤æ–­æ¼”å‘˜æ˜¯å¦åœ¨æ ‡é¢˜é‡Œï¼Œé¿å…åŒåæ ‡é¢˜è¯¯åŒ¹é… MOPP-023
                            for each_actor in actor_amazon:
                                if each_actor in temp_detail_url:
                                    actor_result_list.add(url)
                                    if "å†™çœŸä»˜ã" not in pic_title:  # NACR-206
                                        w, h = await get_imgsize(url)
                                        if w > 600 or not w:
                                            hd_pic_url = url
                                            return hd_pic_url
                                        else:
                                            result.poster = pic_url  # ç”¨äº Google æœå›¾
                                            result.poster_from = "Amazon"
                                    break
                            else:
                                title_result_list.append([url, "https://www.amazon.co.jp" + detail_url])

            # å‘½ä¸­æ¼”å‘˜æœ‰å¤šä¸ªç»“æœæ—¶è¿”å›æœ€å¤§çš„ï¼ˆä¸ç­‰äº1759/1758ï¼‰
            if len(actor_result_list):
                pic_w = 0
                for each in actor_result_list:
                    new_pic_w, _ = await get_imgsize(each)
                    if new_pic_w > pic_w:
                        if new_pic_w >= 1770 or (1750 > new_pic_w > 600):  # ä¸è¦å°å›¾ FCDSS-001ï¼ŒæˆªçŸ­çš„å›¾ï¼ˆ1758/1759ï¼‰
                            pic_w = new_pic_w
                            hd_pic_url = each
                        else:
                            result.poster = each  # ç”¨äº Google æœå›¾
                            result.poster_from = "Amazon"

                if hd_pic_url:
                    return hd_pic_url

            # å½“æœç´¢ç»“æœå‘½ä¸­äº†æ ‡é¢˜ï¼Œæ²¡æœ‰å‘½ä¸­æ¼”å‘˜æ—¶ï¼Œå°è¯•å»è¯¦æƒ…é¡µè·å–æ¼”å‘˜ä¿¡æ¯
            elif (
                len(title_result_list) <= 20
                and "s-pagination-item s-pagination-next s-pagination-button s-pagination-separator" not in html_search
            ):
                for each in title_result_list[:4]:
                    try:
                        url_new = "https://www.amazon.co.jp" + re.findall(r"(/dp/[^/]+)", each[1])[0]
                    except Exception:
                        url_new = each[1]
                    success, html_detail = await get_amazon_data(url_new)
                    if success and html_detail:
                        html = etree.fromstring(html_detail, etree.HTMLParser())
                        detail_actor = str(html.xpath('//span[@class="author notFaded"]/a/text()')).replace(" ", "")
                        detail_info_1 = str(
                            html.xpath('//ul[@class="a-unordered-list a-vertical a-spacing-mini"]//text()')
                        ).replace(" ", "")
                        detail_info_2 = str(
                            html.xpath('//div[@id="detailBulletsWrapper_feature_div"]//text()')
                        ).replace(" ", "")
                        detail_info_3 = str(html.xpath('//div[@id="productDescription"]//text()')).replace(" ", "")
                        all_info = detail_actor + detail_info_1 + detail_info_2 + detail_info_3
                        for each_actor in actor_amazon:
                            if each_actor in all_info:
                                w, h = await get_imgsize(each[0])
                                if w > 720 or not w:
                                    return each[0]
                                else:
                                    result.poster = each[0]  # ç”¨äº Google æœå›¾
                                    result.poster_from = "Amazon"

            # æœ‰å¾ˆå¤šç»“æœæ—¶ï¼ˆæœ‰ä¸‹ä¸€é¡µæŒ‰é’®ï¼‰ï¼ŒåŠ æ¼”å‘˜åå­—é‡æ–°æœç´¢
            if (
                "s-pagination-item s-pagination-next s-pagination-button s-pagination-separator" in html_search
                or len(title_result_list) > 5
            ):
                amazon_orginaltitle_actor = result.amazon_orginaltitle_actor
                if amazon_orginaltitle_actor and amazon_orginaltitle_actor not in originaltitle_amazon:
                    originaltitle_amazon_list.append(f"{originaltitle_amazon} {amazon_orginaltitle_actor}")

    return hd_pic_url


async def trailer_download(
    result: CrawlersResult,
    folder_new: str,
    folder_old: str,
    naming_rule: str,
) -> bool | None:
    start_time = time.time()
    download_files = manager.config.download_files
    keep_files = manager.config.keep_files
    trailer_name = manager.config.trailer_simple_name
    result.trailer = await get_dmm_trailer(result.trailer)  # todo æˆ–è®¸æ‰¾ä¸€ä¸ªæ›´åˆé€‚çš„åœ°æ–¹è¿›è¡Œç»Ÿä¸€åå¤„ç†
    trailer_url = result.trailer
    trailer_old_folder_path = os.path.join(folder_old, "trailers")
    trailer_new_folder_path = os.path.join(folder_new, "trailers")

    # é¢„å‘Šç‰‡åå­—ä¸å«è§†é¢‘æ–‡ä»¶åï¼ˆåªè®©ä¸€ä¸ªè§†é¢‘å»ä¸‹è½½å³å¯ï¼‰
    if trailer_name:
        trailer_folder_path = os.path.join(folder_new, "trailers")
        trailer_file_name = "trailer.mp4"
        trailer_file_path = os.path.join(trailer_folder_path, trailer_file_name)

        # é¢„å‘Šç‰‡æ–‡ä»¶å¤¹å·²åœ¨å·²å¤„ç†åˆ—è¡¨æ—¶ï¼Œè¿”å›ï¼ˆè¿™æ—¶åªéœ€è¦ä¸‹è½½ä¸€ä¸ªï¼Œå…¶ä»–åˆ†é›†ä¸éœ€è¦ä¸‹è½½ï¼‰
        if trailer_folder_path in Flags.trailer_deal_set:
            return
        Flags.trailer_deal_set.add(trailer_folder_path)

        # ä¸ä¸‹è½½ä¸ä¿ç•™æ—¶åˆ é™¤è¿”å›
        if DownloadableFile.TRAILER not in download_files and DownloadableFile.TRAILER not in keep_files:
            # åˆ é™¤ç›®æ ‡æ–‡ä»¶å¤¹å³å¯ï¼Œå…¶ä»–æ–‡ä»¶å¤¹å’Œæ–‡ä»¶å·²ç»åˆ é™¤äº†
            if await aiofiles.os.path.exists(trailer_folder_path):
                await to_thread(shutil.rmtree, trailer_folder_path, ignore_errors=True)
            return

    else:
        # é¢„å‘Šç‰‡å¸¦æ–‡ä»¶åï¼ˆæ¯ä¸ªè§†é¢‘éƒ½æœ‰æœºä¼šä¸‹è½½ï¼Œå¦‚æœå·²æœ‰ä¸‹è½½å¥½çš„ï¼Œåˆ™ä½¿ç”¨å·²ä¸‹è½½çš„ï¼‰
        trailer_file_name = naming_rule + "-trailer.mp4"
        trailer_folder_path = folder_new
        trailer_file_path = os.path.join(trailer_folder_path, trailer_file_name)

        # ä¸ä¸‹è½½ä¸ä¿ç•™æ—¶åˆ é™¤è¿”å›
        if DownloadableFile.TRAILER not in download_files and DownloadableFile.TRAILER not in keep_files:
            # åˆ é™¤ç›®æ ‡æ–‡ä»¶ï¼Œåˆ é™¤é¢„å‘Šç‰‡æ—§æ–‡ä»¶å¤¹ã€æ–°æ–‡ä»¶å¤¹ï¼ˆdeal old fileæ—¶æ²¡åˆ é™¤ï¼‰
            if await aiofiles.os.path.exists(trailer_file_path):
                await delete_file_async(trailer_file_path)
            if await aiofiles.os.path.exists(trailer_old_folder_path):
                await to_thread(shutil.rmtree, trailer_old_folder_path, ignore_errors=True)
            if trailer_new_folder_path != trailer_old_folder_path and await aiofiles.os.path.exists(
                trailer_new_folder_path
            ):
                await to_thread(shutil.rmtree, trailer_new_folder_path, ignore_errors=True)
            return

    # é€‰æ‹©ä¿ç•™æ–‡ä»¶ï¼Œå½“å­˜åœ¨æ–‡ä»¶æ—¶ï¼Œä¸ä¸‹è½½ã€‚ï¼ˆdone trailer path æœªè®¾ç½®æ—¶ï¼ŒæŠŠå½“å‰æ–‡ä»¶è®¾ç½®ä¸º done trailer pathï¼Œä»¥ä¾¿å…¶ä»–åˆ†é›†å¤åˆ¶ï¼‰
    if DownloadableFile.TRAILER in keep_files and await aiofiles.os.path.exists(trailer_file_path):
        if not Flags.file_done_dic.get(result.number, {}).get("trailer"):
            Flags.file_done_dic[result.number].update({"trailer": trailer_file_path})
            # å¸¦æ–‡ä»¶åæ—¶ï¼Œåˆ é™¤æ‰æ–°ã€æ—§æ–‡ä»¶å¤¹ï¼Œç”¨ä¸åˆ°äº†ã€‚ï¼ˆå…¶ä»–åˆ†é›†å¦‚æœæ²¡æœ‰ï¼Œå¯ä»¥å¤åˆ¶ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„é¢„å‘Šç‰‡ã€‚æ­¤æ—¶ä¸åˆ ï¼Œæ²¡æœºä¼šåˆ é™¤äº†ï¼‰
            if not trailer_name:
                if await aiofiles.os.path.exists(trailer_old_folder_path):
                    await to_thread(shutil.rmtree, trailer_old_folder_path, ignore_errors=True)
                if trailer_new_folder_path != trailer_old_folder_path and await aiofiles.os.path.exists(
                    trailer_new_folder_path
                ):
                    await to_thread(shutil.rmtree, trailer_new_folder_path, ignore_errors=True)
        LogBuffer.log().write(f"\n ğŸ€ Trailer done! (old)({get_used_time(start_time)}s) ")
        return True

    # å¸¦æ–‡ä»¶åæ—¶ï¼Œé€‰æ‹©ä¸‹è½½ä¸ä¿ç•™ï¼Œæˆ–è€…é€‰æ‹©ä¿ç•™ä½†æ²¡æœ‰é¢„å‘Šç‰‡ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–åˆ†é›†å·²ä¸‹è½½æˆ–æœ¬åœ°é¢„å‘Šç‰‡
    # é€‰æ‹©ä¸‹è½½ä¸ä¿ç•™ï¼Œå½“æ²¡æœ‰ä¸‹è½½æˆåŠŸæ—¶ï¼Œä¸ä¼šåˆ é™¤ä¸ä¿ç•™çš„æ–‡ä»¶
    done_trailer_path = Flags.file_done_dic.get(result.number, {}).get("trailer")
    if not trailer_name and done_trailer_path and await aiofiles.os.path.exists(done_trailer_path):
        if await aiofiles.os.path.exists(trailer_file_path):
            await delete_file_async(trailer_file_path)
        await copy_file_async(done_trailer_path, trailer_file_path)
        LogBuffer.log().write(f"\n ğŸ€ Trailer done! (copy trailer)({get_used_time(start_time)}s)")
        return

    # ä¸ä¸‹è½½æ—¶è¿”å›ï¼ˆé€‰æ‹©ä¸ä¸‹è½½ä¿ç•™ï¼Œä½†æœ¬åœ°å¹¶ä¸å­˜åœ¨ï¼Œæ­¤æ—¶è¿”å›ï¼‰
    if DownloadableFile.TRAILER not in download_files:
        return

    # ä¸‹è½½é¢„å‘Šç‰‡,æ£€æµ‹é“¾æ¥æœ‰æ•ˆæ€§
    content_length = await check_url(trailer_url, length=True)
    if content_length:
        # åˆ›å»ºæ–‡ä»¶å¤¹
        if trailer_name == 1 and not await aiofiles.os.path.exists(trailer_folder_path):
            await aiofiles.os.makedirs(trailer_folder_path)

        # å¼€å§‹ä¸‹è½½
        download_files = manager.config.download_files
        signal.show_traceback_log(f"ğŸ” {result.number} download trailer... {trailer_url}")
        trailer_file_path_temp = trailer_file_path
        if await aiofiles.os.path.exists(trailer_file_path):
            trailer_file_path_temp = trailer_file_path + ".[DOWNLOAD].mp4"
        if await download_file_with_filepath(trailer_url, trailer_file_path_temp, trailer_folder_path):
            file_size = await aiofiles.os.path.getsize(trailer_file_path_temp)
            if file_size >= content_length or DownloadableFile.IGNORE_SIZE in download_files:
                LogBuffer.log().write(
                    f"\n ğŸ€ Trailer done! ({result.trailer_from} {file_size}/{content_length})({get_used_time(start_time)}s) "
                )
                signal.show_traceback_log(f"âœ… {result.number} trailer done!")
                if trailer_file_path_temp != trailer_file_path:
                    await move_file_async(trailer_file_path_temp, trailer_file_path)
                    await delete_file_async(trailer_file_path_temp)
                done_trailer_path = Flags.file_done_dic.get(result.number, {}).get("trailer")
                if not done_trailer_path:
                    Flags.file_done_dic[result.number].update({"trailer": trailer_file_path})
                    if trailer_name == 0:  # å¸¦æ–‡ä»¶åï¼Œå·²ä¸‹è½½æˆåŠŸï¼Œåˆ é™¤æ‰é‚£äº›ä¸ç”¨çš„æ–‡ä»¶å¤¹å³å¯
                        if await aiofiles.os.path.exists(trailer_old_folder_path):
                            await to_thread(shutil.rmtree, trailer_old_folder_path, ignore_errors=True)
                        if trailer_new_folder_path != trailer_old_folder_path and await aiofiles.os.path.exists(
                            trailer_new_folder_path
                        ):
                            await to_thread(shutil.rmtree, trailer_new_folder_path, ignore_errors=True)
                return True
            else:
                LogBuffer.log().write(
                    f"\n ğŸŸ  Trailer size is incorrect! delete it! ({result.trailer_from} {file_size}/{content_length}) "
                )

        # åˆ é™¤ä¸‹è½½å¤±è´¥çš„æ–‡ä»¶
        await delete_file_async(trailer_file_path_temp)
        LogBuffer.log().write(f"\n ğŸŸ  Trailer download failed! ({trailer_url}) ")

    if await aiofiles.os.path.exists(trailer_file_path):  # ä½¿ç”¨æ—§æ–‡ä»¶
        done_trailer_path = Flags.file_done_dic.get(result.number, {}).get("trailer")
        if not done_trailer_path:
            Flags.file_done_dic[result.number].update({"trailer": trailer_file_path})
            if trailer_name == 0:  # å¸¦æ–‡ä»¶åï¼Œå·²ä¸‹è½½æˆåŠŸï¼Œåˆ é™¤æ‰é‚£äº›ä¸ç”¨çš„æ–‡ä»¶å¤¹å³å¯
                if await aiofiles.os.path.exists(trailer_old_folder_path):
                    await to_thread(shutil.rmtree, trailer_old_folder_path, ignore_errors=True)
                if trailer_new_folder_path != trailer_old_folder_path and await aiofiles.os.path.exists(
                    trailer_new_folder_path
                ):
                    await to_thread(shutil.rmtree, trailer_new_folder_path, ignore_errors=True)
        LogBuffer.log().write("\n ğŸŸ  Trailer download failed! å°†ç»§ç»­ä½¿ç”¨ä¹‹å‰çš„æœ¬åœ°æ–‡ä»¶ï¼")
        LogBuffer.log().write(f"\n ğŸ€ Trailer done! (old)({get_used_time(start_time)}s)")
        return True


async def _get_big_thumb(result: CrawlersResult, other: OtherInfo):
    """
    è·å–èƒŒæ™¯å¤§å›¾ï¼š
    1ï¼Œå®˜ç½‘å›¾ç‰‡
    2ï¼ŒAmazon å›¾ç‰‡
    3ï¼ŒGoogle æœå›¾
    """
    start_time = time.time()
    if "thumb" not in manager.config.download_hd_pics:
        return
    number = result.number
    letters = result.letters
    number_lower_line = number.lower()
    number_lower_no_line = number_lower_line.replace("-", "")
    thumb_width = 0

    # faleno.jp ç•ªå·æ£€æŸ¥ï¼Œéƒ½æ˜¯å¤§å›¾ï¼Œè¿”å›å³å¯
    if result.thumb_from in ["faleno", "dahlia"]:
        if result.thumb:
            LogBuffer.log().write(f"\n ğŸ–¼ HD Thumb found! ({result.thumb_from})({get_used_time(start_time)}s)")
        other.poster_big = True
        return result

    # prestige å›¾ç‰‡æœ‰çš„æ˜¯å¤§å›¾ï¼Œéœ€è¦æ£€æµ‹å›¾ç‰‡åˆ†è¾¨ç‡
    elif result.thumb_from in ["prestige", "mgstage"]:
        if result.thumb:
            thumb_width, h = await get_imgsize(result.thumb)

    # ç‰‡å•†å®˜ç½‘æŸ¥è¯¢
    elif HDPicSource.OFFICIAL in manager.config.download_hd_pics:
        # faleno.jp ç•ªå·æ£€æŸ¥
        if re.findall(r"F[A-Z]{2}SS", number):
            req_url = f"https://faleno.jp/top/works/{number_lower_no_line}/"
            response, error = await manager.computed.async_client.get_text(req_url)
            if response is not None:
                temp_url = re.findall(
                    r'src="((https://cdn.faleno.net/top/wp-content/uploads/[^_]+_)([^?]+))\?output-quality=', response
                )
                if temp_url:
                    result.thumb = temp_url[0][0]
                    result.poster = temp_url[0][1] + "2125.jpg"
                    result.thumb_from = "faleno"
                    result.poster_from = "faleno"
                    other.poster_big = True
                    trailer_temp = re.findall(r'class="btn09"><a class="pop_sample" href="([^"]+)', response)
                    if trailer_temp:
                        result.trailer = trailer_temp[0]
                        result.trailer_from = "faleno"
                    LogBuffer.log().write(f"\n ğŸ–¼ HD Thumb found! (faleno)({get_used_time(start_time)}s)")
                    return result

        # km-produce.com ç•ªå·æ£€æŸ¥
        number_letter = letters.lower()
        kmp_key = ["vrkm", "mdtm", "mkmp", "savr", "bibivr", "scvr", "slvr", "averv", "kbvr", "cbikmv"]
        prestige_key = ["abp", "abw", "aka", "prdvr", "pvrbst", "sdvr", "docvr"]
        if number_letter in kmp_key:
            req_url = f"https://km-produce.com/img/title1/{number_lower_line}.jpg"
            real_url = await check_url(req_url)
            if real_url:
                result.thumb = real_url
                result.thumb_from = "km-produce"
                LogBuffer.log().write(f"\n ğŸ–¼ HD Thumb found! (km-produce)({get_used_time(start_time)}s)")
                return result

        # www.prestige-av.com ç•ªå·æ£€æŸ¥
        elif number_letter in prestige_key:
            number_num = re.findall(r"\d+", number)[0]
            if number_letter == "abw" and int(number_num) > 280:
                pass
            else:
                req_url = f"https://www.prestige-av.com/api/media/goods/prestige/{number_letter}/{number_num}/pb_{number_lower_line}.jpg"
                if number_letter == "docvr":
                    req_url = f"https://www.prestige-av.com/api/media/goods/doc/{number_letter}/{number_num}/pb_{number_lower_line}.jpg"
                if (await get_imgsize(req_url))[0] >= 800:
                    result.thumb = req_url
                    result.poster = req_url.replace("/pb_", "/pf_")
                    result.thumb_from = "prestige"
                    result.poster_from = "prestige"
                    other.poster_big = True
                    LogBuffer.log().write(f"\n ğŸ–¼ HD Thumb found! (prestige)({get_used_time(start_time)}s)")
                    return result

    # ä½¿ç”¨googleä»¥å›¾æœå›¾
    pic_url = result.thumb
    if HDPicSource.GOOGLE in manager.config.download_hd_pics and pic_url and result.thumb_from != "theporndb":
        thumb_url, cover_size = await get_big_pic_by_google(pic_url)
        if thumb_url and cover_size[0] > thumb_width:
            other.thumb_size = cover_size
            pic_domain = re.findall(r"://([^/]+)", thumb_url)[0]
            result.thumb_from = f"Google({pic_domain})"
            result.thumb = thumb_url
            LogBuffer.log().write(f"\n ğŸ–¼ HD Thumb found! ({result.thumb_from})({get_used_time(start_time)}s)")

    return result


async def _get_big_poster(result: CrawlersResult, other: OtherInfo):
    start_time = time.time()

    # æœªå‹¾é€‰ä¸‹è½½é«˜æ¸…å›¾posteræ—¶ï¼Œè¿”å›
    if "poster" not in manager.config.download_hd_pics:
        return

    # å¦‚æœæœ‰å¤§å›¾æ—¶ï¼Œç›´æ¥ä¸‹è½½
    if other.poster_big and (await get_imgsize(result.poster))[1] > 600:
        result.image_download = True
        LogBuffer.log().write(f"\n ğŸ–¼ HD Poster found! ({result.poster_from})({get_used_time(start_time)}s)")
        return

    # åˆå§‹åŒ–æ•°æ®
    number = result.number
    poster_url = result.poster
    hd_pic_url = ""
    poster_width = 0

    # é€šè¿‡åŸæ ‡é¢˜å» amazon æŸ¥è¯¢
    if HDPicSource.AMAZON in manager.config.download_hd_pics and result.mosaic in [
        "æœ‰ç ",
        "æœ‰ç¢¼",
        "æµå‡º",
        "æ— ç ç ´è§£",
        "ç„¡ç¢¼ç ´è§£",
        "é‡Œç•ª",
        "è£ç•ª",
        "åŠ¨æ¼«",
        "å‹•æ¼«",
    ]:
        hd_pic_url = await get_big_pic_by_amazon(result, result.originaltitle_amazon, result.actor_amazon)
        if hd_pic_url:
            result.poster = hd_pic_url
            result.poster_from = "Amazon"
        if result.poster_from == "Amazon":
            result.image_download = True

    # é€šè¿‡ç•ªå·å» å®˜ç½‘ æŸ¥è¯¢è·å–ç¨å¾®å¤§ä¸€äº›çš„å°é¢å›¾ï¼Œä»¥ä¾¿å» Google æœç´¢
    if (
        not hd_pic_url
        and HDPicSource.OFFICIAL in manager.config.download_hd_pics
        and WebsiteSet.OFFICIAL not in manager.config.website_set
        and result.poster_from != "Amazon"
    ):
        letters = result.letters.upper()
        official_url = manager.computed.official_websites.get(letters)
        if official_url:
            url_search = official_url + "/search/list?keyword=" + number.replace("-", "")
            html_search, error = await manager.computed.async_client.get_text(url_search)
            if html_search is not None:
                poster_url_list = re.findall(r'img class="c-main-bg lazyload" data-src="([^"]+)"', html_search)
                if poster_url_list:
                    # ä½¿ç”¨å®˜ç½‘å›¾ä½œä¸ºå°é¢å» google æœç´¢
                    poster_url = poster_url_list[0]
                    result.poster = poster_url
                    result.poster_from = official_url.split(".")[-2].replace("https://", "")
                    # vrä½œå“æˆ–è€…å®˜ç½‘å›¾ç‰‡é«˜åº¦å¤§äº500æ—¶ï¼Œä¸‹è½½å°é¢å›¾å¼€
                    if "VR" in number.upper() or (await get_imgsize(poster_url))[1] > 500:
                        result.image_download = True

    # ä½¿ç”¨googleä»¥å›¾æœå›¾ï¼Œæ”¾åœ¨æœ€åæ˜¯å› ä¸ºæœ‰æ—¶æœ‰é”™è¯¯ï¼Œæ¯”å¦‚ kawd-943
    poster_url = result.poster
    if (
        not hd_pic_url
        and poster_url
        and HDPicSource.GOOGLE in manager.config.download_hd_pics
        and result.poster_from != "theporndb"
    ):
        hd_pic_url, poster_size = await get_big_pic_by_google(poster_url, poster=True)
        if hd_pic_url:
            if "prestige" in result.poster or result.poster_from == "Amazon":
                poster_width, _ = await get_imgsize(poster_url)
            if poster_size[0] > poster_width:
                result.poster = hd_pic_url
                other.poster_size = poster_size
                pic_domain = re.findall(r"://([^/]+)", hd_pic_url)[0]
                result.poster_from = f"Google({pic_domain})"

    # å¦‚æœæ‰¾åˆ°äº†é«˜æ¸…é“¾æ¥ï¼Œåˆ™æ›¿æ¢
    if hd_pic_url:
        result.image_download = True
        LogBuffer.log().write(f"\n ğŸ–¼ HD Poster found! ({result.poster_from})({get_used_time(start_time)}s)")

    return result


async def thumb_download(
    result: CrawlersResult,
    other: OtherInfo,
    cd_part: str,
    folder_new_path: str,
    thumb_final_path: str,
) -> bool:
    start_time = time.time()
    poster_path = other.poster_path
    thumb_path = other.thumb_path
    fanart_path = other.fanart_path

    # æœ¬åœ°å­˜åœ¨ thumb.jpgï¼Œä¸”å‹¾é€‰ä¿ç•™æ—§æ–‡ä»¶æ—¶ï¼Œä¸ä¸‹è½½
    if thumb_path and DownloadableFile.THUMB in manager.config.keep_files:
        LogBuffer.log().write(f"\n ğŸ€ Thumb done! (old)({get_used_time(start_time)}s) ")
        return True

    # å¦‚æœthumbä¸ä¸‹è½½ï¼Œçœ‹fanartã€posterè¦ä¸è¦ä¸‹è½½ï¼Œéƒ½ä¸ä¸‹è½½åˆ™è¿”å›
    if DownloadableFile.THUMB not in manager.config.download_files:
        if (
            DownloadableFile.POSTER in manager.config.download_files
            and (DownloadableFile.POSTER not in manager.config.keep_files or not poster_path)
            or DownloadableFile.FANART in manager.config.download_files
            and (DownloadableFile.FANART not in manager.config.keep_files or not fanart_path)
        ):
            pass
        else:
            return True

    # å°è¯•å¤åˆ¶å…¶ä»–åˆ†é›†ã€‚çœ‹åˆ†é›†æœ‰æ²¡æœ‰ä¸‹è½½ï¼Œå¦‚æœä¸‹è½½å®Œæˆåˆ™å¯ä»¥å¤åˆ¶ï¼Œå¦åˆ™å°±è‡ªè¡Œä¸‹è½½
    if cd_part:
        done_thumb_path = Flags.file_done_dic.get(result.number, {}).get("thumb")
        if (
            done_thumb_path
            and await aiofiles.os.path.exists(done_thumb_path)
            and split_path(done_thumb_path)[0] == split_path(thumb_final_path)[0]
        ):
            await copy_file_async(done_thumb_path, thumb_final_path)
            LogBuffer.log().write(f"\n ğŸ€ Thumb done! (copy cd-thumb)({get_used_time(start_time)}s) ")
            result.thumb_from = "copy cd-thumb"
            other.thumb_path = thumb_final_path
            return True

    # è·å–é«˜æ¸…èƒŒæ™¯å›¾
    await _get_big_thumb(result, other)

    # ä¸‹è½½å›¾ç‰‡
    cover_url = result.thumb
    cover_from = result.thumb_from
    if cover_url:
        cover_list = result.thumb_list
        while (cover_from, cover_url) in cover_list:
            cover_list.remove((cover_from, cover_url))
        cover_list.insert(0, (cover_from, cover_url))

        thumb_final_path_temp = thumb_final_path
        if await aiofiles.os.path.exists(thumb_final_path):
            thumb_final_path_temp = thumb_final_path + ".[DOWNLOAD].jpg"
        for each in cover_list:
            if not each[1]:
                continue
            cover_from, cover_url = each
            cover_url = await check_url(cover_url)
            if not cover_url:
                LogBuffer.log().write(
                    f"\n ğŸŸ  æ£€æµ‹åˆ° Thumb å›¾ç‰‡å¤±æ•ˆ! è·³è¿‡ï¼({cover_from})({get_used_time(start_time)}s) " + each[1]
                )
                continue
            result.thumb_from = cover_from
            if await download_file_with_filepath(cover_url, thumb_final_path_temp, folder_new_path):
                cover_size = await check_pic_async(thumb_final_path_temp)
                if cover_size:
                    if (
                        not cover_from.startswith("Google")
                        or cover_size == other.thumb_size
                        or (
                            cover_size[0] >= 800
                            and abs(cover_size[0] / cover_size[1] - other.thumb_size[0] / other.thumb_size[1]) <= 0.1
                        )
                    ):
                        # å›¾ç‰‡ä¸‹è½½æ­£å¸¸ï¼Œæ›¿æ¢æ—§çš„ thumb.jpg
                        if thumb_final_path_temp != thumb_final_path:
                            await move_file_async(thumb_final_path_temp, thumb_final_path)
                            await delete_file_async(thumb_final_path_temp)
                        if cd_part:
                            dic = {"thumb": thumb_final_path}
                            Flags.file_done_dic[result.number].update(dic)
                        other.thumb_marked = False  # è¡¨ç¤ºè¿˜æ²¡æœ‰èµ°åŠ æ°´å°æµç¨‹
                        LogBuffer.log().write(f"\n ğŸ€ Thumb done! ({result.thumb_from})({get_used_time(start_time)}s) ")
                        other.thumb_path = thumb_final_path
                        return True
                    else:
                        await delete_file_async(thumb_final_path_temp)
                        LogBuffer.log().write(
                            f"\n ğŸŸ  æ£€æµ‹åˆ° Thumb åˆ†è¾¨ç‡ä¸å¯¹{str(cover_size)}! å·²åˆ é™¤ ({cover_from})({get_used_time(start_time)}s)"
                        )
                        continue
                LogBuffer.log().write(f"\n ğŸŸ  Thumb download failed! {cover_from}: {cover_url} ")
    else:
        LogBuffer.log().write("\n ğŸŸ  Thumb url is empty! ")

    # ä¸‹è½½å¤±è´¥ï¼Œæœ¬åœ°æœ‰å›¾
    if thumb_path:
        LogBuffer.log().write("\n ğŸŸ  Thumb download failed! å°†ç»§ç»­ä½¿ç”¨ä¹‹å‰çš„å›¾ç‰‡ï¼")
        LogBuffer.log().write(f"\n ğŸ€ Thumb done! (old)({get_used_time(start_time)}s) ")
        return True
    else:
        if DownloadableFile.IGNORE_PIC_FAIL in manager.config.download_files:
            LogBuffer.log().write("\n ğŸŸ  Thumb download failed! (ä½ å·²å‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€) ")
            LogBuffer.log().write(f"\n ğŸ€ Thumb done! (none)({get_used_time(start_time)}s)")
            return True
        else:
            LogBuffer.log().write(
                "\n ğŸ”´ Thumb download failed! ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€ "
            )
            LogBuffer.error().write(
                "Thumb download failed! ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€"
            )
            return False


async def poster_download(
    result: CrawlersResult,
    other: OtherInfo,
    cd_part: str,
    folder_new_path: str,
    poster_final_path: str,
) -> bool:
    start_time = time.time()
    download_files = manager.config.download_files
    keep_files = manager.config.keep_files
    poster_path = other.poster_path
    thumb_path = other.thumb_path
    fanart_path = other.fanart_path
    image_cut = ""

    # ä¸ä¸‹è½½posterã€ä¸ä¿ç•™posteræ—¶ï¼Œè¿”å›
    if DownloadableFile.POSTER not in download_files and DownloadableFile.POSTER not in keep_files:
        if poster_path:
            await delete_file_async(poster_path)
        return True

    # æœ¬åœ°æœ‰posteræ—¶ï¼Œä¸”å‹¾é€‰ä¿ç•™æ—§æ–‡ä»¶æ—¶ï¼Œä¸ä¸‹è½½
    if poster_path and DownloadableFile.POSTER in keep_files:
        LogBuffer.log().write(f"\n ğŸ€ Poster done! (old)({get_used_time(start_time)}s)")
        return True

    # ä¸ä¸‹è½½æ—¶è¿”å›
    if DownloadableFile.POSTER not in download_files:
        return True

    # å°è¯•å¤åˆ¶å…¶ä»–åˆ†é›†ã€‚çœ‹åˆ†é›†æœ‰æ²¡æœ‰ä¸‹è½½ï¼Œå¦‚æœä¸‹è½½å®Œæˆåˆ™å¯ä»¥å¤åˆ¶ï¼Œå¦åˆ™å°±è‡ªè¡Œä¸‹è½½
    if cd_part:
        done_poster_path = Flags.file_done_dic.get(result.number, {}).get("poster")
        if (
            done_poster_path
            and await aiofiles.os.path.exists(done_poster_path)
            and split_path(done_poster_path)[0] == split_path(poster_final_path)[0]
        ):
            await copy_file_async(done_poster_path, poster_final_path)
            result.poster_from = "copy cd-poster"
            other.poster_path = poster_final_path
            LogBuffer.log().write(f"\n ğŸ€ Poster done! (copy cd-poster)({get_used_time(start_time)}s)")
            return True

    # å‹¾é€‰å¤åˆ¶ thumbæ—¶ï¼šå›½äº§ï¼Œå¤åˆ¶thumbï¼›æ— ç ï¼Œå‹¾é€‰ä¸è£å‰ªæ—¶ï¼Œä¹Ÿå¤åˆ¶thumb
    if thumb_path:
        mosaic = result.mosaic
        number = result.number
        copy_flag = False
        if number.startswith("FC2"):
            image_cut = "center"
            if DownloadableFile.IGNORE_FC2 in download_files:
                copy_flag = True
        elif mosaic == "å›½äº§" or mosaic == "åœ‹ç”¢":
            image_cut = "right"
            if DownloadableFile.IGNORE_GUOCHAN in download_files:
                copy_flag = True
        elif mosaic == "æ— ç " or mosaic == "ç„¡ç¢¼" or mosaic == "ç„¡ä¿®æ­£":
            image_cut = "center"
            if DownloadableFile.IGNORE_WUMA in download_files:
                copy_flag = True
        elif mosaic == "æœ‰ç " or mosaic == "æœ‰ç¢¼":
            if DownloadableFile.IGNORE_YOUMA in download_files:
                copy_flag = True
        if copy_flag:
            await copy_file_async(thumb_path, poster_final_path)
            other.poster_marked = other.thumb_marked
            result.poster_from = "copy thumb"
            other.poster_path = poster_final_path
            LogBuffer.log().write(f"\n ğŸ€ Poster done! (copy thumb)({get_used_time(start_time)}s)")
            return True

    # è·å–é«˜æ¸… poster
    await _get_big_poster(result, other)

    # ä¸‹è½½å›¾ç‰‡
    poster_url = result.poster
    poster_from = result.poster_from
    poster_final_path_temp = poster_final_path
    if await aiofiles.os.path.exists(poster_final_path):
        poster_final_path_temp = poster_final_path + ".[DOWNLOAD].jpg"
    if result.image_download:
        start_time = time.time()
        if await download_file_with_filepath(poster_url, poster_final_path_temp, folder_new_path):
            poster_size = await check_pic_async(poster_final_path_temp)
            if poster_size:
                if (
                    not poster_from.startswith("Google")
                    or poster_size == other.poster_size
                    or "media-amazon.com" in poster_url
                ):
                    if poster_final_path_temp != poster_final_path:
                        await move_file_async(poster_final_path_temp, poster_final_path)
                        await delete_file_async(poster_final_path_temp)
                    if cd_part:
                        dic = {"poster": poster_final_path}
                        Flags.file_done_dic[result.number].update(dic)
                    other.poster_marked = False  # ä¸‹è½½çš„å›¾ï¼Œè¿˜æ²¡åŠ æ°´å°
                    other.poster_path = poster_final_path
                    LogBuffer.log().write(f"\n ğŸ€ Poster done! ({poster_from})({get_used_time(start_time)}s)")
                    return True
                else:
                    await delete_file_async(poster_final_path_temp)
                    LogBuffer.log().write(f"\n ğŸŸ  æ£€æµ‹åˆ° Poster åˆ†è¾¨ç‡ä¸å¯¹{str(poster_size)}! å·²åˆ é™¤ ({poster_from})")

    # åˆ¤æ–­ä¹‹å‰æœ‰æ²¡æœ‰ poster å’Œ thumb
    if not poster_path and not thumb_path:
        other.poster_path = ""
        if DownloadableFile.IGNORE_PIC_FAIL in download_files:
            LogBuffer.log().write("\n ğŸŸ  Poster download failed! (ä½ å·²å‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€) ")
            LogBuffer.log().write(f"\n ğŸ€ Poster done! (none)({get_used_time(start_time)}s)")
            return True
        else:
            LogBuffer.log().write(
                "\n ğŸ”´ Poster download failed! ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€ "
            )
            LogBuffer.error().write(
                "Poster download failed! ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€"
            )
            return False

    # ä½¿ç”¨thumbè£å‰ª
    poster_final_path_temp = poster_final_path + ".[CUT].jpg"
    if fanart_path:
        thumb_path = fanart_path
    if await asyncio.to_thread(cut_thumb_to_poster, result, thumb_path, poster_final_path_temp, image_cut):
        # è£å‰ªæˆåŠŸï¼Œæ›¿æ¢æ—§å›¾
        await move_file_async(poster_final_path_temp, poster_final_path)
        if cd_part:
            dic = {"poster": poster_final_path}
            Flags.file_done_dic[result.number].update(dic)
        other.poster_path = poster_final_path
        other.poster_marked = False
        return True

    # è£å‰ªå¤±è´¥ï¼Œæœ¬åœ°æœ‰å›¾
    if poster_path:
        LogBuffer.log().write("\n ğŸŸ  Poster cut failed! å°†ç»§ç»­ä½¿ç”¨ä¹‹å‰çš„å›¾ç‰‡ï¼")
        LogBuffer.log().write(f"\n ğŸ€ Poster done! (old)({get_used_time(start_time)}s) ")
        return True
    else:
        if DownloadableFile.IGNORE_PIC_FAIL in download_files:
            LogBuffer.log().write("\n ğŸŸ  Poster cut failed! (ä½ å·²å‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€) ")
            LogBuffer.log().write(f"\n ğŸ€ Poster done! (none)({get_used_time(start_time)}s)")
            return True
        else:
            LogBuffer.log().write(
                "\n ğŸ”´ Poster cut failed! ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€ "
            )
            LogBuffer.error().write("Poster failedï¼ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€")
            return False


async def fanart_download(
    number: str,
    other: OtherInfo,
    cd_part: str,
    fanart_final_path: str,
) -> bool:
    """
    å¤åˆ¶thumbä¸ºfanart
    """
    start_time = time.time()
    thumb_path = other.thumb_path
    fanart_path = other.fanart_path
    download_files = manager.config.download_files
    keep_files = manager.config.keep_files

    # ä¸ä¿ç•™ä¸ä¸‹è½½æ—¶åˆ é™¤è¿”å›
    if DownloadableFile.FANART not in keep_files and DownloadableFile.FANART not in download_files:
        if fanart_path and await aiofiles.os.path.exists(fanart_path):
            await delete_file_async(fanart_path)
        return True

    # ä¿ç•™ï¼Œå¹¶ä¸”æœ¬åœ°å­˜åœ¨ fanart.jpgï¼Œä¸ä¸‹è½½è¿”å›
    if DownloadableFile.FANART in keep_files and fanart_path:
        LogBuffer.log().write(f"\n ğŸ€ Fanart done! (old)({get_used_time(start_time)}s)")
        return True

    # ä¸ä¸‹è½½æ—¶ï¼Œè¿”å›
    if DownloadableFile.FANART not in download_files:
        return True

    # å°è¯•å¤åˆ¶å…¶ä»–åˆ†é›†ã€‚çœ‹åˆ†é›†æœ‰æ²¡æœ‰ä¸‹è½½ï¼Œå¦‚æœä¸‹è½½å®Œæˆåˆ™å¯ä»¥å¤åˆ¶ï¼Œå¦åˆ™å°±è‡ªè¡Œä¸‹è½½
    if cd_part:
        done_fanart_path = Flags.file_done_dic.get(number, {}).get("fanart")
        if (
            done_fanart_path
            and await aiofiles.os.path.exists(done_fanart_path)
            and split_path(done_fanart_path)[0] == split_path(fanart_final_path)[0]
        ):
            if fanart_path:
                await delete_file_async(fanart_path)
            await copy_file_async(done_fanart_path, fanart_final_path)
            other.fanart_path = fanart_final_path
            LogBuffer.log().write(f"\n ğŸ€ Fanart done! (copy cd-fanart)({get_used_time(start_time)}s)")
            return True

    # å¤åˆ¶thumb
    if thumb_path:
        if fanart_path:
            await delete_file_async(fanart_path)
        await copy_file_async(thumb_path, fanart_final_path)
        other.fanart_path = fanart_final_path
        other.fanart_marked = other.thumb_marked
        LogBuffer.log().write(f"\n ğŸ€ Fanart done! (copy thumb)({get_used_time(start_time)}s)")
        if cd_part:
            dic = {"fanart": fanart_final_path}
            Flags.file_done_dic[number].update(dic)
        return True
    else:
        # æœ¬åœ°æœ‰ fanart æ—¶ï¼Œä¸ä¸‹è½½
        if fanart_path:
            LogBuffer.log().write("\n ğŸŸ  Fanart copy failed! æœªæ‰¾åˆ° thumb å›¾ç‰‡ï¼Œå°†ç»§ç»­ä½¿ç”¨ä¹‹å‰çš„å›¾ç‰‡ï¼")
            LogBuffer.log().write(f"\n ğŸ€ Fanart done! (old)({get_used_time(start_time)}s)")
            return True

        else:
            if DownloadableFile.IGNORE_PIC_FAIL in download_files:
                LogBuffer.log().write("\n ğŸŸ  Fanart failed! (ä½ å·²å‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€) ")
                LogBuffer.log().write(f"\n ğŸ€ Fanart done! (none)({get_used_time(start_time)}s)")
                return True
            else:
                LogBuffer.log().write(
                    "\n ğŸ”´ Fanart failed! ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€ "
                )
                LogBuffer.error().write(
                    "Fanart ä¸‹è½½å¤±è´¥ï¼ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€"
                )
                return False


async def extrafanart_download(extrafanart: list[str], extrafanart_from: str, folder_new_path: str) -> bool | None:
    start_time = time.time()
    download_files = manager.config.download_files
    keep_files = manager.config.keep_files
    extrafanart_list = extrafanart
    extrafanart_folder_path = os.path.join(folder_new_path, "extrafanart")

    # ä¸ä¸‹è½½ä¸ä¿ç•™æ—¶åˆ é™¤è¿”å›
    if DownloadableFile.EXTRAFANART not in download_files and DownloadableFile.EXTRAFANART not in keep_files:
        if await aiofiles.os.path.exists(extrafanart_folder_path):
            await to_thread(shutil.rmtree, extrafanart_folder_path, ignore_errors=True)
        return

    # æœ¬åœ°å­˜åœ¨ extrafanart_folderï¼Œä¸”å‹¾é€‰ä¿ç•™æ—§æ–‡ä»¶æ—¶ï¼Œä¸ä¸‹è½½
    if DownloadableFile.EXTRAFANART in keep_files and await aiofiles.os.path.exists(extrafanart_folder_path):
        LogBuffer.log().write(f"\n ğŸ€ Extrafanart done! (old)({get_used_time(start_time)}s) ")
        return True

    # å¦‚æœ extrafanart ä¸ä¸‹è½½
    if DownloadableFile.EXTRAFANART not in download_files:
        return True

    # æ£€æµ‹é“¾æ¥æœ‰æ•ˆæ€§
    if extrafanart_list and await check_url(extrafanart_list[0]):
        extrafanart_folder_path_temp = extrafanart_folder_path
        if await aiofiles.os.path.exists(extrafanart_folder_path_temp):
            extrafanart_folder_path_temp = extrafanart_folder_path + "[DOWNLOAD]"
            if not await aiofiles.os.path.exists(extrafanart_folder_path_temp):
                await aiofiles.os.makedirs(extrafanart_folder_path_temp)
        else:
            await aiofiles.os.makedirs(extrafanart_folder_path_temp)

        extrafanart_count = 0
        extrafanart_count_succ = 0
        task_list = []
        for extrafanart_url in extrafanart_list:
            extrafanart_count += 1
            extrafanart_name = "fanart" + str(extrafanart_count) + ".jpg"
            extrafanart_file_path = os.path.join(extrafanart_folder_path_temp, extrafanart_name)
            task_list.append((extrafanart_url, extrafanart_file_path, extrafanart_folder_path_temp, extrafanart_name))

        # ä½¿ç”¨å¼‚æ­¥å¹¶å‘æ‰§è¡Œä¸‹è½½ä»»åŠ¡
        tasks = [download_extrafanart_task(task) for task in task_list]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        for res in results:
            if res is True:
                extrafanart_count_succ += 1
        if extrafanart_count_succ == extrafanart_count:
            if extrafanart_folder_path_temp != extrafanart_folder_path:
                await to_thread(shutil.rmtree, extrafanart_folder_path)
                await aiofiles.os.rename(extrafanart_folder_path_temp, extrafanart_folder_path)
            LogBuffer.log().write(
                f"\n ğŸ€ ExtraFanart done! ({extrafanart_from} {extrafanart_count_succ}/{extrafanart_count})({get_used_time(start_time)}s)"
            )
            return True
        else:
            LogBuffer.log().write(
                f"\n ğŸŸ  ExtraFanart download failed! ({extrafanart_from} {extrafanart_count_succ}/{extrafanart_count})({get_used_time(start_time)}s)"
            )
            if extrafanart_folder_path_temp != extrafanart_folder_path:
                await to_thread(shutil.rmtree, extrafanart_folder_path_temp)
            else:
                LogBuffer.log().write(f"\n ğŸ€ ExtraFanart done! (incomplete)({get_used_time(start_time)}s)")
                return False
        LogBuffer.log().write("\n ğŸŸ  ExtraFanart download failed! å°†ç»§ç»­ä½¿ç”¨ä¹‹å‰çš„æœ¬åœ°æ–‡ä»¶ï¼")
    if await aiofiles.os.path.exists(extrafanart_folder_path):  # ä½¿ç”¨æ—§æ–‡ä»¶
        LogBuffer.log().write(f"\n ğŸ€ ExtraFanart done! (old)({get_used_time(start_time)}s)")
        return True
