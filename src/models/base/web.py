#!/usr/bin/env python3
import re
import socket
import threading
from concurrent.futures import ThreadPoolExecutor
from io import BytesIO
from threading import Lock
from typing import Any, Dict, List, Literal, Optional, Tuple, Union, overload
from urllib.parse import quote

# import cloudscraper
import curl_cffi.requests
import requests
import urllib3.util.connection as urllib3_cn
from PIL import Image
from ping3 import ping
from requests.exceptions import (
    ChunkedEncodingError,
    ConnectionError,
    ConnectTimeout,
    ContentDecodingError,
    HTTPError,
    InvalidHeader,
    InvalidProxyURL,
    InvalidURL,
    ProxyError,
    ReadTimeout,
    SSLError,
    StreamConsumedError,
    Timeout,
    TooManyRedirects,
    URLRequired,
)

from ..config.manager import config
from ..signals import signal
from .utils import get_user_agent, singleton
from .web_compat import *


def _allowed_gai_family():
    """
    https://github.com/shazow/urllib3/blob/master/urllib3/util/connection.py
    """
    family = socket.AF_INET
    return family


try:
    if config.ipv4_only:
        urllib3_cn.allowed_gai_family = _allowed_gai_family
except Exception:
    urllib3_cn.allowed_gai_family = _allowed_gai_family


@singleton
class WebRequests:
    def __init__(self):
        self.session_g = requests.Session()
        self.session_g.mount("https://", requests.adapters.HTTPAdapter(pool_connections=100, pool_maxsize=100))
        self.session_g.mount("http://", requests.adapters.HTTPAdapter(pool_connections=100, pool_maxsize=100))
        # self.scraper = cloudscraper.create_scraper(
        #     browser={'browser': 'firefox', 'platform': 'windows', 'mobile': False})  # returns a CloudScraper instance
        self.lock = Lock()
        self.pool = ThreadPoolExecutor(32)
        self.curl_session = curl_cffi.requests.Session(max_redirects=10)

    def _prepare_request_params(
        self,
        url: Optional[str] = None,
        headers: Optional[Dict[str, str]] = None,
        proxy: Union[bool, Optional[Dict[str, str]]] = True,
        timeout: Optional[float] = None,
    ) -> Tuple[Dict[str, str], Optional[Dict[str, str]], float]:
        """é¢„å¤„ç†è¯·æ±‚å‚æ•°"""
        # todo ç”¨ä¸€ä¸ªå•ç‹¬å‚æ•°æ§åˆ¶æ˜¯å¦ä½¿ç”¨ä»£ç†
        if proxy is True:
            proxy = config.proxies
        elif proxy is False:
            proxy = None

        # å¤„ç†è¯·æ±‚å¤´
        if not headers:
            headers = config.headers.copy()

        if url:
            if "getchu" in url:
                headers.update({"Referer": "http://www.getchu.com/top.html"})
            elif "xcity" in url:
                headers.update(
                    {"referer": "https://xcity.jp/result_published/?genre=%2Fresult_published%2F&q=2&sg=main&num=60"}
                )
            elif "javbus" in url:
                headers.update({"Referer": "https://www.javbus.com/"})
            elif "giga" in url and "cookie_set.php" not in url:
                headers.update({"Referer": "https://www.giga-web.jp/top.html"})

        # å¤„ç†è¶…æ—¶
        timeout = timeout or config.timeout

        return headers, proxy, timeout

    @overload
    def get_html(
        self,
        url: str,
        *,
        content: Literal[True],
        headers: Optional[Dict[str, str]] = None,
        cookies: Optional[Dict[str, str]] = None,
        proxies: Union[bool, Optional[Dict[str, str]]] = True,
        allow_redirects: bool = True,
        json_data: bool = False,
        res: bool = False,
        keep: bool = True,
        timeout: Union[bool, float] = False,
        encoding: str = "utf-8",
        back_cookie: bool = False,
    ) -> Tuple[bool, Union[bytes, str]]: ...

    @overload
    def get_html(
        self,
        url: str,
        *,
        json_data: Literal[True],
        headers: Optional[Dict[str, str]] = None,
        cookies: Optional[Dict[str, str]] = None,
        proxies: Union[bool, Optional[Dict[str, str]]] = True,
        allow_redirects: bool = True,
        content: bool = False,
        res: bool = False,
        keep: bool = True,
        timeout: Union[bool, float] = False,
        encoding: str = "utf-8",
        back_cookie: bool = False,
    ) -> Tuple[bool, Union[Dict[str, Any], str]]: ...

    @overload
    def get_html(
        self,
        url: str,
        *,
        res: Literal[True],
        headers: Optional[Dict[str, str]] = None,
        cookies: Optional[Dict[str, str]] = None,
        proxies: Union[bool, Optional[Dict[str, str]]] = True,
        allow_redirects: bool = True,
        json_data: bool = False,
        content: bool = False,
        keep: bool = True,
        timeout: Union[bool, float] = False,
        encoding: str = "utf-8",
        back_cookie: bool = False,
    ) -> Tuple[Any, Union[Any, str]]: ...

    def get_html(
        self,
        url: str,
        headers: Optional[Dict[str, str]] = None,
        cookies: Optional[Dict[str, str]] = None,
        proxies: Union[bool, Optional[Dict[str, str]]] = True,
        allow_redirects: bool = True,
        json_data: bool = False,
        content: bool = False,
        res: bool = False,
        keep: bool = True,
        timeout: Union[bool, float] = False,
        encoding: str = "utf-8",
        back_cookie: bool = False,
    ):
        headers, proxy, timeout = self._prepare_request_params(url, headers, proxies, timeout)

        if content:
            return self._get_content(url, headers, cookies, proxy, timeout, allow_redirects)
        elif json_data:
            return self._get_json(url, headers, cookies, proxy, timeout, allow_redirects, encoding)
        elif res:
            return self._get_response(url, headers, cookies, proxy, timeout, allow_redirects)
        else:
            return self._get_text(url, headers, cookies, proxy, timeout, allow_redirects, encoding, keep, back_cookie)

    def _get_content(
        self,
        url: str,
        headers: Dict[str, str],
        cookies: Optional[Dict[str, str]],
        proxies: Optional[Dict[str, str]],
        timeout: float,
        allow_redirects: bool,
    ) -> Tuple[bool, Union[bytes, str]]:
        """è·å–äºŒè¿›åˆ¶å†…å®¹"""
        for i in range(config.retry):
            try:
                response = self.session_g.get(
                    url,
                    headers=headers,
                    cookies=cookies,
                    proxies=proxies,
                    timeout=timeout,
                    verify=False,
                    allow_redirects=allow_redirects,
                )
                if self._check_response(response, url, i):
                    return True, response.content
            except Exception as e:
                self._handle_request_error(e, url, i)
        return False, f"è¯·æ±‚å¤±è´¥! {url}"

    def _get_json(
        self,
        url: str,
        headers: Dict[str, str],
        cookies: Optional[Dict[str, str]],
        proxies: Optional[Dict[str, str]],
        timeout: float,
        allow_redirects: bool,
        encoding: str,
    ) -> Tuple[bool, Union[Dict[str, Any], str]]:
        """è·å–JSONå“åº”"""
        for i in range(config.retry):
            try:
                response = self.session_g.get(
                    url,
                    headers=headers,
                    cookies=cookies,
                    proxies=proxies,
                    timeout=timeout,
                    verify=False,
                    allow_redirects=allow_redirects,
                )
                if self._check_response(response, url, i):
                    response.encoding = encoding
                    return True, response.json()
            except Exception as e:
                self._handle_request_error(e, url, i)
        return False, f"è¯·æ±‚å¤±è´¥! {url}"

    def _get_response(
        self,
        url: str,
        headers: Dict[str, str],
        cookies: Optional[Dict[str, str]],
        proxies: Optional[Dict[str, str]],
        timeout: float,
        allow_redirects: bool,
    ) -> Tuple[Any, Union[Any, str]]:
        """è·å–å®Œæ•´å“åº”å¯¹è±¡"""
        for i in range(config.retry):
            try:
                response = self.session_g.get(
                    url,
                    headers=headers,
                    cookies=cookies,
                    proxies=proxies,
                    timeout=timeout,
                    verify=False,
                    allow_redirects=allow_redirects,
                )
                if self._check_response(response, url, i):
                    return response.headers, response
            except Exception as e:
                self._handle_request_error(e, url, i)
        return False, f"è¯·æ±‚å¤±è´¥! {url}"

    def _get_text(
        self,
        url: str,
        headers: Dict[str, str],
        cookies: Optional[Dict[str, str]],
        proxies: Optional[Dict[str, str]],
        timeout: float,
        allow_redirects: bool,
        encoding: str,
        keep: bool,
        back_cookie: bool,
    ) -> Tuple[Any, str]:
        """è·å–æ–‡æœ¬å“åº”"""
        session = self.session_g if keep else requests
        for i in range(config.retry):
            try:
                response = session.get(
                    url,
                    headers=headers,
                    cookies=cookies,
                    proxies=proxies,
                    timeout=timeout,
                    verify=False,
                    allow_redirects=allow_redirects,
                )
                if self._check_response(response, url, i):
                    _header = response.cookies if back_cookie else response.headers
                    response.encoding = encoding
                    return _header, response.text
            except Exception as e:
                self._handle_request_error(e, url, i)
        return False, f"è¯·æ±‚å¤±è´¥! {url}"

    @overload
    def post_html(
        self,
        url: str,
        *,
        data: Optional[Dict[str, Any]] = None,
        json: Optional[Dict[str, Any]] = None,
        headers: Optional[Dict[str, str]] = None,
        cookies: Optional[Dict[str, str]] = None,
        use_proxy: Union[bool, Optional[Dict[str, str]]] = True,
        json_data: Literal[True],
        keep: bool = True,
    ) -> Union[Tuple[Literal[True], Dict[str, Any]], Tuple[Literal[False], str]]:
        """mypy æ— æ³•æ ¹æ®ç¬¬ä¸€ä¸ªè¿”å›å€¼ narrow ç¬¬äºŒä¸ªè¿”å›å€¼çš„ç±»å‹, éœ€è¦ cast é¿å…ç±»å‹é”™è¯¯"""
        ...

    @overload
    def post_html(
        self,
        url: str,
        *,
        data: Optional[Dict[str, Any]] = None,
        json: Optional[Dict[str, Any]] = None,
        headers: Optional[Dict[str, str]] = None,
        cookies: Optional[Dict[str, str]] = None,
        use_proxy: Union[bool, Optional[Dict[str, str]]] = True,
        json_data: Literal[False] = False,
        keep: bool = True,
    ) -> Tuple[bool, str]: ...

    def post_html(
        self,
        url: str,
        *,
        data: Optional[Dict[str, Any]] = None,
        json: Optional[Dict[str, Any]] = None,
        headers: Optional[Dict[str, str]] = None,
        cookies: Optional[Dict[str, str]] = None,
        use_proxy: Union[bool, Optional[Dict[str, str]]] = True,
        json_data: bool = False,
        keep: bool = True,
    ):
        # é¢„å¤„ç†è¯·æ±‚å‚æ•°
        headers, proxies, timeout = self._prepare_request_params(headers=headers, proxy=use_proxy)

        # æ ¹æ®å‚æ•°ç»„åˆè·¯ç”±åˆ°ä¸åŒçš„å¤„ç†å­æ–¹æ³•
        if json_data:
            return self._post_json(url, data, json, headers, cookies, proxies, timeout, keep)
        else:
            return self._post_text(url, data, json, headers, cookies, proxies, timeout, keep)

    def _post_json(
        self,
        url: str,
        data: Optional[Dict[str, Any]],
        json_data: Optional[Dict[str, Any]],
        headers: Dict[str, str],
        cookies: Optional[Dict[str, str]],
        proxies: Optional[Dict[str, str]],
        timeout: float,
        keep: bool,
    ):
        """å‘é€POSTè¯·æ±‚å¹¶è·å–JSONå“åº”"""
        session = self.session_g if keep else requests
        for i in range(config.retry):
            try:
                response = session.post(
                    url,
                    data=data,
                    json=json_data,
                    headers=headers,
                    cookies=cookies,
                    proxies=proxies,
                    timeout=timeout,
                    verify=False,
                )
                if self._check_response(response, url, i):
                    return True, response.json()
            except Exception as e:
                self._handle_request_error(e, url, i)
        return False, f"è¯·æ±‚å¤±è´¥! {url}"

    def _post_text(
        self,
        url: str,
        data: Optional[Dict[str, Any]],
        json_data: Optional[Dict[str, Any]],
        headers: Dict[str, str],
        cookies: Optional[Dict[str, str]],
        proxies: Optional[Dict[str, str]],
        timeout: float,
        keep: bool,
    ) -> Tuple[bool, str]:
        """å‘é€POSTè¯·æ±‚å¹¶è·å–æ–‡æœ¬å“åº”"""
        session = self.session_g if keep else requests
        for i in range(config.retry):
            try:
                response = session.post(
                    url,
                    data=data,
                    json=json_data,
                    headers=headers,
                    cookies=cookies,
                    proxies=proxies,
                    timeout=timeout,
                    verify=False,
                )
                if self._check_response(response, url, i):
                    response.encoding = "utf-8"
                    return True, response.text
            except Exception as e:
                self._handle_request_error(e, url, i)
        return False, f"è¯·æ±‚å¤±è´¥! {url}"

    def _check_response(self, response: Any, url: str, retry_index: int) -> bool:
        """æ£€æŸ¥å“åº”çŠ¶æ€"""
        if response.status_code > 299:
            if not (response.status_code == 302 and response.headers.get("Location")):
                error_info = f"{response.status_code} {url}"
                signal.add_log(f"ğŸ”´ é‡è¯• [{retry_index + 1}/{config.retry}] {error_info}")
                return False
        else:
            signal.add_log(f"âœ… æˆåŠŸ {url}")
        return True

    def _handle_request_error(self, error: Exception, url: str, retry_index: int) -> None:
        """å¤„ç†è¯·æ±‚å¼‚å¸¸"""
        error_info = f"{url}\nError: {error}"
        signal.add_log(f"[{retry_index + 1}/{config.retry}] {error_info}")

    def _get_filesize(self, url: str) -> Optional[str]:
        proxies = config.proxies
        timeout = config.timeout
        retry_times = config.retry
        headers = config.headers

        for _ in range(int(retry_times)):
            try:
                response = self.session_g.head(url, headers=headers, proxies=proxies, timeout=timeout, verify=False)
                file_size = response.headers.get("Content-Length")
                return file_size
            except Exception:
                pass
        return None

    def multi_download(self, url: str, file_path: str) -> bool:
        # è·å–æ–‡ä»¶å¤§å°
        file_size = self._get_filesize(url)

        # åˆ¤æ–­æ˜¯ä¸æ˜¯webpæ–‡ä»¶
        webp = False
        if file_path.endswith("jpg") and ".webp" in url:
            webp = True

        # æ²¡æœ‰å¤§å°æ—¶ï¼Œä¸æ”¯æŒåˆ†æ®µä¸‹è½½ï¼Œç›´æ¥ä¸‹è½½ï¼›< 2 MB çš„ç›´æ¥ä¸‹è½½
        MB = 1024**2
        if not file_size or int(file_size) <= 2 * MB or webp:
            result, response = get_content(url)
            if result:
                if webp:
                    if isinstance(response, bytes):
                        byte_stream = BytesIO(response)
                        img = Image.open(byte_stream)
                        if img.mode == "RGBA":
                            img = img.convert("RGB")
                        img.save(file_path, quality=95, subsampling=0)
                        img.close()
                else:
                    with open(file_path, "wb") as f:
                        if isinstance(response, bytes):
                            f.write(response)
                return True
            return False

        return self._multi_download2(url, file_path, int(file_size))

    def _multi_download2(self, url: str, file_path: str, file_size: int) -> bool:
        # åˆ†å—ï¼Œæ¯å— 1 MB
        MB = 1024**2
        file_size = int(file_size)
        each_size = min(int(1 * MB), file_size)
        parts = [(s, min(s + each_size, file_size)) for s in range(0, file_size, each_size)]
        # print(f'åˆ†å—æ•°ï¼š{len(parts)} \n')

        # å…ˆå†™å…¥ä¸€ä¸ªæ–‡ä»¶
        f = open(file_path, "wb")
        f.truncate(file_size)
        f.close()

        # å¼€å§‹ä¸‹è½½
        i = 0
        task_list = []
        for part in parts:
            i += 1
            start, end = part
            task_list.append([start, end, i, url, file_path])
        result = self.pool.map(self._start_download, task_list)
        for res in result:
            if not res:
                # bar.close()
                return False
        # bar.close()
        return True

    def _start_download(self, task) -> bool:
        start, end, i, url, file_path = task

        proxies = config.proxies
        timeout = config.timeout
        retry_times = config.retry
        headers = config.headers
        _headers = headers.copy()
        _headers["Range"] = f"bytes={start}-{end}"
        for _ in range(int(retry_times)):
            try:
                response = self.session_g.get(
                    url, headers=_headers, proxies=proxies, timeout=timeout, verify=False, stream=True
                )
                chunk_size = 128
                chunks = []
                for chunk in response.iter_content(chunk_size=chunk_size):
                    chunks.append(chunk)  # bar.update(chunk_size)
                self.lock.acquire()
                with open(file_path, "rb+") as fp:
                    fp.seek(start)
                    for chunk in chunks:
                        fp.write(chunk)
                    self.lock.release()
                del chunks
                return True
            except Exception:
                pass
        return False

    def curl_html(
        self,
        url: str,
        headers: Optional[Dict[str, str]] = None,
        proxies: Union[bool, Optional[Dict[str, str]]] = True,
        cookies: Optional[Dict[str, str]] = None,
    ) -> Tuple[Union[bool, Any], Union[str, Any]]:
        """
        curlè¯·æ±‚(æ¨¡æ‹Ÿæµè§ˆå™¨æŒ‡çº¹)
        """
        # è·å–ä»£ç†ä¿¡æ¯
        retry_times = config.retry
        _, proxies, _ = self._prepare_request_params(url, headers, proxies)

        signal.add_log(f"ğŸ” è¯·æ±‚ {url}")
        error_info = ""
        for i in range(int(retry_times)):
            try:
                response = self.curl_session.get(
                    url_encode(url), headers=headers, cookies=cookies, proxies=proxies, impersonate="chrome120"
                )
                if "amazon" in url:
                    response.encoding = "Shift_JIS"
                else:
                    response.encoding = "UTF-8"
                if response.status_code == 200:
                    signal.add_log(f"âœ… æˆåŠŸ {url}")
                    return response.headers, response.text
                else:
                    error_info = f"{response.status_code} {url}"
                    signal.add_log(f"ğŸ”´ é‡è¯• [{i + 1}/{retry_times}] {error_info}")
                    continue
            except Exception as e:
                error_info = f"{url}\nError: {e}"
                signal.add_log(f"[{i + 1}/{retry_times}] {error_info}")
                continue
        signal.add_log(f"ğŸ”´ è¯·æ±‚å¤±è´¥ï¼{error_info}")
        return False, error_info


# web = WebRequests()
# get_html = web.get_html
# post_html = web.post_html
# scraper_html = web.curl_html
# multi_download = web.multi_download
# curl_html = web.curl_html


def url_encode(url: str) -> str:
    new_url = ""
    for i in url:
        if i not in [":", "/", "&", "?", "=", "%"]:
            i = quote(i)
        new_url += i
    return new_url


def check_url(url: str, length: bool = False, real_url: bool = False) -> Union[int, str]:
    proxies = config.proxies
    timeout = config.timeout
    retry_times = config.retry
    headers = config.headers

    if not url:
        return 0

    signal.add_log(f"â›‘ï¸ æ£€æµ‹é“¾æ¥ {url}")
    if "http" not in url:
        signal.add_log(f"ğŸ”´ æ£€æµ‹æœªé€šè¿‡ï¼é“¾æ¥æ ¼å¼é”™è¯¯ï¼ {url}")
        return 0

    if "getchu" in url:
        headers_o = {
            "Referer": "http://www.getchu.com/top.html",
        }
        headers.update(headers_o)
    # javbuså°é¢å›¾éœ€æºå¸¦referï¼Œreferä¼¼ä¹æ²¡æœ‰åšå¼ºæ ¡éªŒï¼Œä½†é¡»ç¬¦åˆæ ¼å¼è¦æ±‚ï¼Œå¦åˆ™403
    elif "javbus" in url:
        headers_o = {
            "Referer": "https://www.javbus.com/",
        }
        headers.update(headers_o)

    for j in range(retry_times):
        try:
            r = requests.head(
                url, headers=headers, proxies=proxies, timeout=timeout, verify=False, allow_redirects=True
            )

            # ä¸è¾“å‡ºè·å– dmmé¢„è§ˆè§†é¢‘(trailer) æœ€é«˜åˆ†è¾¨ç‡çš„æµ‹è¯•ç»“æœåˆ°æ—¥å¿—ä¸­
            # get_dmm_trailer() å‡½æ•°åœ¨å¤šæ¡é”™è¯¯çš„é“¾æ¥ä¸­æ‰¾æœ€é«˜åˆ†è¾¨ç‡çš„é“¾æ¥ï¼Œé”™è¯¯æ²¡æœ‰å¿…è¦è¾“å‡ºï¼Œé¿å…è¯¯è§£ä¸ºç½‘ç»œæˆ–è½¯ä»¶é—®é¢˜
            if r.status_code == 404 and "_w.mp4" in url:
                if j + 1 < retry_times:
                    continue
                else:
                    return 0

            # çŠ¶æ€ç  > 299ï¼Œè¡¨ç¤ºè¯·æ±‚å¤±è´¥ï¼Œè§†ä¸ºä¸å¯ç”¨
            if r.status_code > 299:
                error_info = f"{r.status_code} {url}"
                signal.add_log(f"ğŸ”´ è¯·æ±‚å¤±è´¥ï¼ é‡è¯•: [{j + 1}/{retry_times}] {error_info}")
                continue

            # è¿”å›é‡å®šå‘çš„url
            true_url = r.url
            if real_url:
                return true_url

            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½• https://lookaside.fbsbx.com/lookaside/crawler/media/?media_id=637921621668064
            if "login" in true_url:
                signal.add_log(f"ğŸ”´ æ£€æµ‹æœªé€šè¿‡ï¼éœ€è¦ç™»å½•æŸ¥çœ‹ {true_url}")
                return 0

            # æ£€æŸ¥æ˜¯å¦å¸¦æœ‰å›¾ç‰‡ä¸å­˜åœ¨çš„å…³é”®è¯
            """
            å¦‚æœè·³è½¬åçš„çœŸå®é“¾æ¥å­˜åœ¨åˆ å›¾æ ‡è¯†ï¼Œè§†ä¸ºä¸å¯ç”¨
            https://pics.dmm.co.jp/mono/movie/n/now_printing/now_printing.jpg dmm åˆ å›¾çš„æ ‡è¯†ï¼Œjavbusã€javlib ç”¨çš„æ˜¯ dmm å›¾
            https://static.mgstage.com/mgs/img/common/actress/nowprinting.jpg mgstage åˆ å›¾çš„æ ‡è¯†
            https://jdbimgs.com/images/noimage_600x404.jpg javdbåˆ é™¤çš„å›¾ WANZ-921
            https://www.javbus.com/imgs/cover/nopic.jpg
            https://assets.tumblr.com/images/media_violation/community_guidelines_v1_1280.png tumblråˆ é™¤çš„å›¾
            """
            bad_url_keys = ["now_printing", "nowprinting", "noimage", "nopic", "media_violation"]
            for each_key in bad_url_keys:
                if each_key in true_url:
                    signal.add_log(f"ğŸ”´ æ£€æµ‹æœªé€šè¿‡ï¼å½“å‰å›¾ç‰‡å·²è¢«ç½‘ç«™åˆ é™¤ {url}")
                    return 0

            # è·å–æ–‡ä»¶å¤§å°ã€‚å¦‚æœæ²¡æœ‰è·å–åˆ°æ–‡ä»¶å¤§å°ï¼Œå°è¯•ä¸‹è½½15kæ•°æ®ï¼Œå¦‚æœå¤±è´¥ï¼Œè§†ä¸ºä¸å¯ç”¨
            content_length = r.headers.get("Content-Length")
            if not content_length:
                response = requests.get(
                    true_url, headers=headers, proxies=proxies, timeout=timeout, verify=False, stream=True
                )
                i = 0
                chunk_size = 5120
                for _ in response.iter_content(chunk_size):
                    i += 1
                    if i == 3:
                        response.close()
                        signal.add_log(f"âœ… æ£€æµ‹é€šè¿‡ï¼æœªè¿”å›å¤§å°ï¼Œé¢„ä¸‹è½½15ké€šè¿‡ {true_url}")
                        return 10240 if length else true_url
                signal.add_log(f"ğŸ”´ æ£€æµ‹æœªé€šè¿‡ï¼æœªè¿”å›å¤§å°ï¼Œé¢„ä¸‹è½½15kå¤±è´¥ {true_url}")
                return 0

            # å¦‚æœè¿”å›å†…å®¹çš„æ–‡ä»¶å¤§å° < 8kï¼Œè§†ä¸ºä¸å¯ç”¨
            elif int(content_length) < 8192:
                signal.add_log(f"ğŸ”´ æ£€æµ‹æœªé€šè¿‡ï¼è¿”å›å¤§å°({content_length}) < 8k {true_url}")
                return 0
            signal.add_log(f"âœ… æ£€æµ‹é€šè¿‡ï¼è¿”å›å¤§å°({content_length}) {true_url}")
            return int(content_length) if length else true_url
        except InvalidProxyURL as e:
            error_info = f" æ— æ•ˆçš„ä»£ç†é“¾æ¥ ({e}) {url}"
        except ProxyError as e:
            error_info = f" ä»£ç†é”™è¯¯ {e} {url}"
        except SSLError as e:
            error_info = f" SSLé”™è¯¯ ({e}) {url}"
        except ConnectTimeout as e:
            error_info = f" å°è¯•è¿æ¥åˆ°è¿œç¨‹æœåŠ¡å™¨æ—¶è¶…æ—¶ ({e}) {url}"
        except ReadTimeout as e:
            error_info = f" æœåŠ¡å™¨æœªåœ¨åˆ†é…çš„æ—¶é—´å†…å‘é€ä»»ä½•æ•°æ® ({e}) {url}"
        except Timeout as e:
            error_info = f" è¯·æ±‚è¶…æ—¶é”™è¯¯ ({e}) {url}"
        except ConnectionError as e:
            error_info = f" è¿æ¥é”™è¯¯ {e} {url}"
        except URLRequired as e:
            error_info = f" URLæ ¼å¼é”™è¯¯ ({e}) {url}"
        except TooManyRedirects as e:
            error_info = f" è¿‡å¤šçš„é‡å®šå‘ ({e}) {url}"
        except InvalidURL as e:
            error_info = f" æ— æ•ˆçš„url ({e}) {url}"
        except InvalidHeader as e:
            error_info = f" æ— æ•ˆçš„è¯·æ±‚å¤´ ({e}) {url}"
        except HTTPError as e:
            error_info = f" HTTPé”™è¯¯ {e} {url}"
        except ChunkedEncodingError as e:
            error_info = f" æœåŠ¡å™¨å£°æ˜äº†åˆ†å—ç¼–ç ï¼Œä½†å‘é€äº†æ— æ•ˆçš„åˆ†å— ({e}) {url}"
        except ContentDecodingError as e:
            error_info = f" è§£ç å“åº”å†…å®¹å¤±è´¥ ({e}) {url}"
        except StreamConsumedError as e:
            error_info = f" è¯¥å“åº”çš„å†…å®¹å·²è¢«å ç”¨ ({e}) {url}"
        except Exception as e:
            error_info = f" Error ({e}) {url}"
        signal.add_log(f"ğŸ”´ é‡è¯• [{j + 1}/{retry_times}] {error_info}")
    signal.add_log(f"ğŸ”´ æ£€æµ‹æœªé€šè¿‡ï¼ {url}")
    return 0


def get_avsox_domain() -> str:
    issue_url = "https://tellme.pw/avsox"
    result, response = get_text(issue_url)
    domain = "https://avsox.click"
    if result and isinstance(response, str):
        res = re.findall(r'(https://[^"]+)', response)
        for s in res:
            if s and "https://avsox.com" not in s or "api.qrserver.com" not in s:
                return s
    return domain


def get_amazon_data(req_url: str) -> Tuple[bool, str]:
    """
    è·å– Amazon æ•°æ®
    """
    headers = {
        "accept-encoding": "gzip, deflate, br",
        "Host": "www.amazon.co.jp",
        "User-Agent": get_user_agent(),
    }
    try:
        result, html_info = curl_html(req_url)
    except Exception:
        result, html_info = curl_html(req_url, headers=headers)
        session_id = ""
        ubid_acbjp = ""
        if x := re.findall(r'sessionId: "([^"]+)', html_info):
            session_id = x[0]
        if x := re.findall(r"ubid-acbjp=([^ ]+)", html_info):
            ubid_acbjp = x[0]
        headers_o = {
            "cookie": f"session-id={session_id}; ubid_acbjp={ubid_acbjp}",
        }
        headers.update(headers_o)
        result, html_info = curl_html(req_url, headers=headers)

    if not result:
        if "503 http" in html_info:
            headers = {
                "Host": "www.amazon.co.jp",
                "User-Agent": get_user_agent(),
            }
            result, html_info = get_html(req_url, headers=headers, keep=False, back_cookie=True)

        if not result:
            return False, html_info

    return result, html_info


if "__main__" == __name__:
    # æµ‹è¯•ä¸‹è½½æ–‡ä»¶
    list1 = [
        "https://issuecdn.baidupcs.com/issue/netdisk/yunguanjia/BaiduNetdisk_7.2.8.9.exe",
        "https://cc3001.dmm.co.jp/litevideo/freepv/1/118/118abw015/118abw015_mhb_w.mp4",
        "https://cc3001.dmm.co.jp/litevideo/freepv/1/118/118abw00016/118abw00016_mhb_w.mp4",
        "https://cc3001.dmm.co.jp/litevideo/freepv/1/118/118abw00017/118abw00017_mhb_w.mp4",
        "https://cc3001.dmm.co.jp/litevideo/freepv/1/118/118abw00018/118abw00018_mhb_w.mp4",
        "https://cc3001.dmm.co.jp/litevideo/freepv/1/118/118abw00019/118abw00019_mhb_w.mp4",
        "https://www.prestige-av.com/images/corner/goods/prestige/tktabw/018/pb_tktabw-018.jpg",
        "https://iqq1.one/preview/80/b/3SBqI8OjheI-800.jpg?v=1636404497",
    ]
    for each in list1:
        url = each
        file_path = each.split("/")[-1]
        t = threading.Thread(target=multi_download, args=(url, file_path))
        t.start()

    # æ­»å¾ªç¯ï¼Œé¿å…ç¨‹åºç¨‹åºå®Œåï¼Œpoolè‡ªåŠ¨å…³é—­
    while True:
        pass


def get_imgsize(url):
    proxies = config.proxies
    timeout = config.timeout
    retry_times = config.retry
    headers = config.headers

    for _ in range(int(retry_times)):
        try:
            response = requests.get(url, headers=headers, proxies=proxies, timeout=timeout, verify=False, stream=True)
            if response.status_code == 200:
                file_head = BytesIO()
                chunk_size = 1024 * 10
                for chunk in response.iter_content(chunk_size):
                    file_head.write(chunk)
                    response.close()
                    try:
                        img = Image.open(file_head)
                        return img.size
                    except Exception:
                        return 0, 0
        except Exception:
            return 0, 0
    return 0, 0


def get_dmm_trailer(trailer_url):  # å¦‚æœé¢„è§ˆç‰‡åœ°å€ä¸º dmm ï¼Œå°è¯•è·å– dmm é¢„è§ˆç‰‡æœ€é«˜åˆ†è¾¨ç‡
    if ".dmm.co" not in trailer_url:
        return trailer_url
    if trailer_url.startswith("//"):
        trailer_url = "https:" + trailer_url
    """
    '_sm_w.mp4': 320*180, 3.8MB     # æœ€ä½åˆ†è¾¨ç‡
    '_dm_w.mp4': 560*316, 10.1MB    # ä¸­ç­‰åˆ†è¾¨ç‡
    '_dmb_w.mp4': 720*404, 14.6MB   # æ¬¡é«˜åˆ†è¾¨ç‡
    '_mhb_w.mp4': 720*404, 27.9MB   # æœ€é«˜åˆ†è¾¨ç‡
    https://cc3001.dmm.co.jp/litevideo/freepv/s/ssi/ssis00090/ssis00090_sm_w.mp4
    https://cc3001.dmm.co.jp/litevideo/freepv/s/ssi/ssis00090/ssis00090_dm_w.mp4
    https://cc3001.dmm.co.jp/litevideo/freepv/s/ssi/ssis00090/ssis00090_dmb_w.mp4
    https://cc3001.dmm.co.jp/litevideo/freepv/s/ssi/ssis00090/ssis00090_mhb_w.mp4
    """

    # keylist = ['_sm_w.mp4', '_dm_w.mp4', '_dmb_w.mp4', '_mhb_w.mp4']
    if "_mhb_w.mp4" not in trailer_url:
        t = re.findall(r"(.+)(_[sd]mb?_w.mp4)", trailer_url)
        if t:
            s, e = t[0]
            mhb_w = s + "_mhb_w.mp4"
            dmb_w = s + "_dmb_w.mp4"
            dm_w = s + "_dm_w.mp4"
            # æ¬¡é«˜åˆ†è¾¨ç‡åªéœ€æ£€æŸ¥æœ€é«˜
            if e == "_dmb_w.mp4":
                if check_url(mhb_w):
                    trailer_url = mhb_w
            elif e == "_dm_w.mp4":
                if check_url(mhb_w):
                    trailer_url = mhb_w
                elif check_url(dmb_w):
                    trailer_url = dmb_w
            # æœ€å·®åˆ†è¾¨ç‡åˆ™ä¾æ¬¡æ£€æŸ¥æœ€é«˜ï¼Œæ¬¡é«˜ï¼Œä¸­ç­‰
            elif e == "_sm_w.mp4":
                if check_url(mhb_w):
                    trailer_url = mhb_w
                elif check_url(dmb_w):
                    trailer_url = dmb_w
                elif check_url(dm_w):
                    trailer_url = dm_w
    return trailer_url


def _ping_host_thread(host_address: str, result_list: List[Optional[int]], i: int) -> None:
    response = ping(host_address, timeout=1)
    result_list[i] = int(response * 1000) if response else 0


def ping_host(host_address: str) -> str:
    count = config.retry
    result_list: List[Optional[int]] = [None] * count
    thread_list: List[threading.Thread] = [None] * count  # type: ignore # todo
    for i in range(count):
        thread_list[i] = threading.Thread(target=_ping_host_thread, args=(host_address, result_list, i))
        thread_list[i].start()
    for i in range(count):
        thread_list[i].join()
    new_list = [each for each in result_list if each]
    return (
        f"  â± Ping {int(sum(new_list) / len(new_list))} ms ({len(new_list)}/{count})"
        if new_list
        else f"  ğŸ”´ Ping - ms (0/{count})"
    )


def check_version() -> Optional[int]:
    if config.update_check:
        url = "https://api.github.com/repos/sqzw-x/mdcx/releases/latest"
        _, res_json = get_json(url)
        if isinstance(res_json, dict):
            try:
                latest_version = res_json["tag_name"]
                latest_version = int(latest_version)
                return latest_version
            except Exception:
                signal.add_log(f"âŒ è·å–æœ€æ–°ç‰ˆæœ¬å¤±è´¥ï¼{res_json}")
    return None


def check_theporndb_api_token() -> str:
    tips = "âœ… è¿æ¥æ­£å¸¸! "
    headers = config.headers
    proxies = config.proxies
    timeout = config.timeout
    api_token = config.theporndb_api_token
    url = "https://api.theporndb.net/scenes/hash/8679fcbdd29fa735"
    headers = {
        "Authorization": f"Bearer {api_token}",
        "Content-Type": "application/json",
        "Accept": "application/json",
        "User-Agent": get_user_agent(),
    }
    if not api_token:
        tips = "âŒ æœªå¡«å†™ API Tokenï¼Œå½±å“æ¬§ç¾åˆ®å‰Šï¼å¯åœ¨ã€Œè®¾ç½®ã€-ã€Œç½‘ç»œã€æ·»åŠ ï¼"
    else:
        try:
            response = requests.get(url, headers=headers, proxies=proxies, timeout=timeout, verify=False)
            if response.status_code == 401 and "Unauthenticated" in str(response.text):
                tips = "âŒ API Token é”™è¯¯ï¼å½±å“æ¬§ç¾åˆ®å‰Šï¼è¯·åˆ°ã€Œè®¾ç½®ã€-ã€Œç½‘ç»œã€ä¸­ä¿®æ”¹ã€‚"
            elif response.status_code == 200:
                if response.json().get("data"):
                    tips = "âœ… è¿æ¥æ­£å¸¸ï¼"
                else:
                    tips = "âŒ è¿”å›æ•°æ®å¼‚å¸¸ï¼"
            else:
                tips = f"âŒ è¿æ¥å¤±è´¥ï¼è¯·æ£€æŸ¥ç½‘ç»œæˆ–ä»£ç†è®¾ç½®ï¼ {response.status_code} {response.text}"
        except Exception as e:
            tips = f"âŒ è¿æ¥å¤±è´¥!è¯·æ£€æŸ¥ç½‘ç»œæˆ–ä»£ç†è®¾ç½®ï¼ {e}"
    signal.show_log_text(tips.replace("âŒ", " âŒ ThePornDB").replace("âœ…", " âœ… ThePornDB"))
    return tips


def _get_pic_by_google(pic_url):
    google_keyused = config.google_keyused
    google_keyword = config.google_keyword
    req_url = f"https://www.google.com/searchbyimage?sbisrc=2&image_url={pic_url}"
    # req_url = f'https://lens.google.com/uploadbyurl?url={pic_url}&hl=zh-CN&re=df&ep=gisbubu'
    result, response = get_html(req_url, keep=False)
    big_pic = True
    if result:
        url_list = re.findall(r'a href="([^"]+isz:l[^"]+)">', response)
        url_list_middle = re.findall(r'a href="([^"]+isz:m[^"]+)">', response)
        if not url_list and url_list_middle:
            url_list = url_list_middle
            big_pic = False
        if url_list:
            req_url = "https://www.google.com" + url_list[0].replace("amp;", "")
            result, response = get_html(req_url, keep=False)
            if result:
                url_list = re.findall(r'\["(http[^"]+)",(\d{3,4}),(\d{3,4})\],[^[]', response)
                # ä¼˜å…ˆä¸‹è½½æ”¾å‰é¢
                new_url_list = []
                for each_url in url_list.copy():
                    if int(each_url[2]) < 800:
                        url_list.remove(each_url)

                for each_key in google_keyused:
                    for each_url in url_list.copy():
                        if each_key in each_url[0]:
                            new_url_list.append(each_url)
                            url_list.remove(each_url)
                # åªä¸‹è½½å…³æ—¶ï¼Œè¿½åŠ å‰©ä½™åœ°å€
                if "goo_only" not in config.download_hd_pics:
                    new_url_list += url_list
                # è§£æåœ°å€
                for each in new_url_list:
                    temp_url = each[0]
                    for temp_keyword in google_keyword:
                        if temp_keyword in temp_url:
                            break
                    else:
                        h = int(each[1])
                        w = int(each[2])
                        if w > h and w / h < 1.4:  # thumb è¢«æ‹‰é«˜æ—¶è·³è¿‡
                            continue

                        p_url = temp_url.encode("utf-8").decode(
                            "unicode_escape"
                        )  # urlä¸­çš„Unicodeå­—ç¬¦è½¬ä¹‰ï¼Œä¸è½¬ä¹‰ï¼Œurlè¯·æ±‚ä¼šå¤±è´¥
                        if "m.media-amazon.com" in p_url:
                            p_url = re.sub(r"\._[_]?AC_[^\.]+\.", ".", p_url)
                            pic_size = get_imgsize(p_url)
                            if pic_size[0]:
                                return p_url, pic_size, big_pic
                        else:
                            url = check_url(p_url)
                            if url:
                                pic_size = (w, h)
                                return url, pic_size, big_pic
    return "", "", ""


def get_big_pic_by_google(pic_url, poster=False):
    url, pic_size, big_pic = _get_pic_by_google(pic_url)
    if not poster:
        if big_pic or (
            pic_size and int(pic_size[0]) > 800 and int(pic_size[1]) > 539
        ):  # cover æœ‰å¤§å›¾æ—¶æˆ–è€…å›¾ç‰‡é«˜åº¦ > 800 æ—¶ä½¿ç”¨è¯¥å›¾ç‰‡
            return url, pic_size
        return "", ""
    if url and int(pic_size[1]) < 1000:  # posterï¼Œå›¾ç‰‡é«˜åº¦å°äº 1500ï¼Œé‡æ–°æœç´¢ä¸€æ¬¡
        url, pic_size, big_pic = _get_pic_by_google(url)
    if pic_size and (
        big_pic or "blogger.googleusercontent.com" in url or int(pic_size[1]) > 560
    ):  # posterï¼Œå¤§å›¾æˆ–é«˜åº¦ > 560 æ—¶ï¼Œä½¿ç”¨è¯¥å›¾ç‰‡
        return url, pic_size
    else:
        return "", ""
