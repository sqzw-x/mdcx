#!/usr/bin/env python3
import re
import threading
from io import BytesIO
from typing import List, Optional, Tuple, Union
from urllib.parse import quote

import requests
import urllib3.util.connection as urllib3_cn
from PIL import Image
from ping3 import ping
from requests.exceptions import (
    ChunkedEncodingError,
    ConnectionError,
    ConnectTimeout,
    ContentDecodingError,
    HTTPError,
    InvalidHeader,
    InvalidProxyURL,
    InvalidURL,
    ProxyError,
    ReadTimeout,
    SSLError,
    StreamConsumedError,
    Timeout,
    TooManyRedirects,
    URLRequired,
)

from ..config.manager import config
from ..signals import signal
from .utils import get_user_agent
from .web_sync import get_json, get_text


def _allowed_gai_family():
    """
    https://github.com/shazow/urllib3/blob/master/urllib3/util/connection.py
    """
    family = socket.AF_INET
    return family


try:
    if config.ipv4_only:
        urllib3_cn.allowed_gai_family = _allowed_gai_family
except Exception:
    urllib3_cn.allowed_gai_family = _allowed_gai_family


def url_encode(url: str) -> str:
    new_url = ""
    for i in url:
        if i not in [":", "/", "&", "?", "=", "%"]:
            i = quote(i)
        new_url += i
    return new_url


def check_url(url: str, length: bool = False, real_url: bool = False) -> Union[int, str]:
    proxies = config.proxies
    timeout = config.timeout
    retry_times = config.retry
    headers = config.headers

    if not url:
        return 0

    signal.add_log(f"â›‘ï¸ æ£€æµ‹é“¾æ¥ {url}")
    if "http" not in url:
        signal.add_log(f"ğŸ”´ æ£€æµ‹æœªé€šè¿‡ï¼é“¾æ¥æ ¼å¼é”™è¯¯ï¼ {url}")
        return 0

    if "getchu" in url:
        headers_o = {
            "Referer": "http://www.getchu.com/top.html",
        }
        headers.update(headers_o)
    # javbuså°é¢å›¾éœ€æºå¸¦referï¼Œreferä¼¼ä¹æ²¡æœ‰åšå¼ºæ ¡éªŒï¼Œä½†é¡»ç¬¦åˆæ ¼å¼è¦æ±‚ï¼Œå¦åˆ™403
    elif "javbus" in url:
        headers_o = {
            "Referer": "https://www.javbus.com/",
        }
        headers.update(headers_o)

    for j in range(retry_times):
        try:
            r = requests.head(
                url, headers=headers, proxies=proxies, timeout=timeout, verify=False, allow_redirects=True
            )

            # ä¸è¾“å‡ºè·å– dmmé¢„è§ˆè§†é¢‘(trailer) æœ€é«˜åˆ†è¾¨ç‡çš„æµ‹è¯•ç»“æœåˆ°æ—¥å¿—ä¸­
            # get_dmm_trailer() å‡½æ•°åœ¨å¤šæ¡é”™è¯¯çš„é“¾æ¥ä¸­æ‰¾æœ€é«˜åˆ†è¾¨ç‡çš„é“¾æ¥ï¼Œé”™è¯¯æ²¡æœ‰å¿…è¦è¾“å‡ºï¼Œé¿å…è¯¯è§£ä¸ºç½‘ç»œæˆ–è½¯ä»¶é—®é¢˜
            if r.status_code == 404 and "_w.mp4" in url:
                if j + 1 < retry_times:
                    continue
                else:
                    return 0

            # çŠ¶æ€ç  > 299ï¼Œè¡¨ç¤ºè¯·æ±‚å¤±è´¥ï¼Œè§†ä¸ºä¸å¯ç”¨
            if r.status_code > 299:
                error_info = f"{r.status_code} {url}"
                signal.add_log(f"ğŸ”´ è¯·æ±‚å¤±è´¥ï¼ é‡è¯•: [{j + 1}/{retry_times}] {error_info}")
                continue

            # è¿”å›é‡å®šå‘çš„url
            true_url = r.url
            if real_url:
                return true_url

            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½• https://lookaside.fbsbx.com/lookaside/crawler/media/?media_id=637921621668064
            if "login" in true_url:
                signal.add_log(f"ğŸ”´ æ£€æµ‹æœªé€šè¿‡ï¼éœ€è¦ç™»å½•æŸ¥çœ‹ {true_url}")
                return 0

            # æ£€æŸ¥æ˜¯å¦å¸¦æœ‰å›¾ç‰‡ä¸å­˜åœ¨çš„å…³é”®è¯
            """
            å¦‚æœè·³è½¬åçš„çœŸå®é“¾æ¥å­˜åœ¨åˆ å›¾æ ‡è¯†ï¼Œè§†ä¸ºä¸å¯ç”¨
            https://pics.dmm.co.jp/mono/movie/n/now_printing/now_printing.jpg dmm åˆ å›¾çš„æ ‡è¯†ï¼Œjavbusã€javlib ç”¨çš„æ˜¯ dmm å›¾
            https://static.mgstage.com/mgs/img/common/actress/nowprinting.jpg mgstage åˆ å›¾çš„æ ‡è¯†
            https://jdbimgs.com/images/noimage_600x404.jpg javdbåˆ é™¤çš„å›¾ WANZ-921
            https://www.javbus.com/imgs/cover/nopic.jpg
            https://assets.tumblr.com/images/media_violation/community_guidelines_v1_1280.png tumblråˆ é™¤çš„å›¾
            """
            bad_url_keys = ["now_printing", "nowprinting", "noimage", "nopic", "media_violation"]
            for each_key in bad_url_keys:
                if each_key in true_url:
                    signal.add_log(f"ğŸ”´ æ£€æµ‹æœªé€šè¿‡ï¼å½“å‰å›¾ç‰‡å·²è¢«ç½‘ç«™åˆ é™¤ {url}")
                    return 0

            # è·å–æ–‡ä»¶å¤§å°ã€‚å¦‚æœæ²¡æœ‰è·å–åˆ°æ–‡ä»¶å¤§å°ï¼Œå°è¯•ä¸‹è½½15kæ•°æ®ï¼Œå¦‚æœå¤±è´¥ï¼Œè§†ä¸ºä¸å¯ç”¨
            content_length = r.headers.get("Content-Length")
            if not content_length:
                response = requests.get(
                    true_url, headers=headers, proxies=proxies, timeout=timeout, verify=False, stream=True
                )
                i = 0
                chunk_size = 5120
                for _ in response.iter_content(chunk_size):
                    i += 1
                    if i == 3:
                        response.close()
                        signal.add_log(f"âœ… æ£€æµ‹é€šè¿‡ï¼æœªè¿”å›å¤§å°ï¼Œé¢„ä¸‹è½½15ké€šè¿‡ {true_url}")
                        return 10240 if length else true_url
                signal.add_log(f"ğŸ”´ æ£€æµ‹æœªé€šè¿‡ï¼æœªè¿”å›å¤§å°ï¼Œé¢„ä¸‹è½½15kå¤±è´¥ {true_url}")
                return 0

            # å¦‚æœè¿”å›å†…å®¹çš„æ–‡ä»¶å¤§å° < 8kï¼Œè§†ä¸ºä¸å¯ç”¨
            elif int(content_length) < 8192:
                signal.add_log(f"ğŸ”´ æ£€æµ‹æœªé€šè¿‡ï¼è¿”å›å¤§å°({content_length}) < 8k {true_url}")
                return 0
            signal.add_log(f"âœ… æ£€æµ‹é€šè¿‡ï¼è¿”å›å¤§å°({content_length}) {true_url}")
            return int(content_length) if length else true_url
        except InvalidProxyURL as e:
            error_info = f" æ— æ•ˆçš„ä»£ç†é“¾æ¥ ({e}) {url}"
        except ProxyError as e:
            error_info = f" ä»£ç†é”™è¯¯ {e} {url}"
        except SSLError as e:
            error_info = f" SSLé”™è¯¯ ({e}) {url}"
        except ConnectTimeout as e:
            error_info = f" å°è¯•è¿æ¥åˆ°è¿œç¨‹æœåŠ¡å™¨æ—¶è¶…æ—¶ ({e}) {url}"
        except ReadTimeout as e:
            error_info = f" æœåŠ¡å™¨æœªåœ¨åˆ†é…çš„æ—¶é—´å†…å‘é€ä»»ä½•æ•°æ® ({e}) {url}"
        except Timeout as e:
            error_info = f" è¯·æ±‚è¶…æ—¶é”™è¯¯ ({e}) {url}"
        except ConnectionError as e:
            error_info = f" è¿æ¥é”™è¯¯ {e} {url}"
        except URLRequired as e:
            error_info = f" URLæ ¼å¼é”™è¯¯ ({e}) {url}"
        except TooManyRedirects as e:
            error_info = f" è¿‡å¤šçš„é‡å®šå‘ ({e}) {url}"
        except InvalidURL as e:
            error_info = f" æ— æ•ˆçš„url ({e}) {url}"
        except InvalidHeader as e:
            error_info = f" æ— æ•ˆçš„è¯·æ±‚å¤´ ({e}) {url}"
        except HTTPError as e:
            error_info = f" HTTPé”™è¯¯ {e} {url}"
        except ChunkedEncodingError as e:
            error_info = f" æœåŠ¡å™¨å£°æ˜äº†åˆ†å—ç¼–ç ï¼Œä½†å‘é€äº†æ— æ•ˆçš„åˆ†å— ({e}) {url}"
        except ContentDecodingError as e:
            error_info = f" è§£ç å“åº”å†…å®¹å¤±è´¥ ({e}) {url}"
        except StreamConsumedError as e:
            error_info = f" è¯¥å“åº”çš„å†…å®¹å·²è¢«å ç”¨ ({e}) {url}"
        except Exception as e:
            error_info = f" Error ({e}) {url}"
        signal.add_log(f"ğŸ”´ é‡è¯• [{j + 1}/{retry_times}] {error_info}")
    signal.add_log(f"ğŸ”´ æ£€æµ‹æœªé€šè¿‡ï¼ {url}")
    return 0


def get_avsox_domain() -> str:
    issue_url = "https://tellme.pw/avsox"
    response, error = get_text(issue_url)
    domain = "https://avsox.click"
    if response is not None:
        res = re.findall(r'(https://[^"]+)', response)
        for s in res:
            if s and "https://avsox.com" not in s or "api.qrserver.com" not in s:
                return s
    return domain


def get_amazon_data(req_url: str) -> Tuple[bool, str]:
    """
    è·å– Amazon æ•°æ®
    """
    headers = {
        "accept-encoding": "gzip, deflate, br",
        "Host": "www.amazon.co.jp",
        "User-Agent": get_user_agent(),
    }
    html_info, error = get_text(req_url, encoding="Shift_JIS")
    if html_info is None:
        html_info, error = get_text(req_url, headers=headers, encoding="Shift_JIS")
    if html_info is None:
        session_id = ""
        ubid_acbjp = ""
        if x := re.findall(r'sessionId: "([^"]+)', html_info or ""):
            session_id = x[0]
        if x := re.findall(r"ubid-acbjp=([^ ]+)", html_info or ""):
            ubid_acbjp = x[0]
        headers_o = {
            "cookie": f"session-id={session_id}; ubid_acbjp={ubid_acbjp}",
        }
        headers.update(headers_o)
        html_info, error = get_text(req_url, headers=headers, encoding="Shift_JIS")
    if html_info is None:
        return False, error
    if "HTTP 503" in html_info:
        headers = {
            "Host": "www.amazon.co.jp",
            "User-Agent": get_user_agent(),
        }
        html_info, error = get_text(req_url, headers=headers, encoding="Shift_JIS")
    if html_info is None:
        return False, error
    return True, html_info


def get_imgsize(url):
    proxies = config.proxies
    timeout = config.timeout
    retry_times = config.retry
    headers = config.headers

    for _ in range(int(retry_times)):
        try:
            response = requests.get(url, headers=headers, proxies=proxies, timeout=timeout, verify=False, stream=True)
            if response.status_code == 200:
                file_head = BytesIO()
                chunk_size = 1024 * 10
                for chunk in response.iter_content(chunk_size):
                    file_head.write(chunk)
                    response.close()
                    try:
                        img = Image.open(file_head)
                        return img.size
                    except Exception:
                        return 0, 0
        except Exception:
            return 0, 0
    return 0, 0


def get_dmm_trailer(trailer_url):  # å¦‚æœé¢„è§ˆç‰‡åœ°å€ä¸º dmm ï¼Œå°è¯•è·å– dmm é¢„è§ˆç‰‡æœ€é«˜åˆ†è¾¨ç‡
    if ".dmm.co" not in trailer_url:
        return trailer_url
    if trailer_url.startswith("//"):
        trailer_url = "https:" + trailer_url
    """
    '_sm_w.mp4': 320*180, 3.8MB     # æœ€ä½åˆ†è¾¨ç‡
    '_dm_w.mp4': 560*316, 10.1MB    # ä¸­ç­‰åˆ†è¾¨ç‡
    '_dmb_w.mp4': 720*404, 14.6MB   # æ¬¡é«˜åˆ†è¾¨ç‡
    '_mhb_w.mp4': 720*404, 27.9MB   # æœ€é«˜åˆ†è¾¨ç‡
    https://cc3001.dmm.co.jp/litevideo/freepv/s/ssi/ssis00090/ssis00090_sm_w.mp4
    https://cc3001.dmm.co.jp/litevideo/freepv/s/ssi/ssis00090/ssis00090_dm_w.mp4
    https://cc3001.dmm.co.jp/litevideo/freepv/s/ssi/ssis00090/ssis00090_dmb_w.mp4
    https://cc3001.dmm.co.jp/litevideo/freepv/s/ssi/ssis00090/ssis00090_mhb_w.mp4
    """

    # keylist = ['_sm_w.mp4', '_dm_w.mp4', '_dmb_w.mp4', '_mhb_w.mp4']
    if "_mhb_w.mp4" not in trailer_url:
        t = re.findall(r"(.+)(_[sd]mb?_w.mp4)", trailer_url)
        if t:
            s, e = t[0]
            mhb_w = s + "_mhb_w.mp4"
            dmb_w = s + "_dmb_w.mp4"
            dm_w = s + "_dm_w.mp4"
            # æ¬¡é«˜åˆ†è¾¨ç‡åªéœ€æ£€æŸ¥æœ€é«˜
            if e == "_dmb_w.mp4":
                if check_url(mhb_w):
                    trailer_url = mhb_w
            elif e == "_dm_w.mp4":
                if check_url(mhb_w):
                    trailer_url = mhb_w
                elif check_url(dmb_w):
                    trailer_url = dmb_w
            # æœ€å·®åˆ†è¾¨ç‡åˆ™ä¾æ¬¡æ£€æŸ¥æœ€é«˜ï¼Œæ¬¡é«˜ï¼Œä¸­ç­‰
            elif e == "_sm_w.mp4":
                if check_url(mhb_w):
                    trailer_url = mhb_w
                elif check_url(dmb_w):
                    trailer_url = dmb_w
                elif check_url(dm_w):
                    trailer_url = dm_w
    return trailer_url


def _ping_host_thread(host_address: str, result_list: List[Optional[int]], i: int) -> None:
    response = ping(host_address, timeout=1)
    result_list[i] = int(response * 1000) if response else 0


def ping_host(host_address: str) -> str:
    count = config.retry
    result_list: List[Optional[int]] = [None] * count
    thread_list: List[threading.Thread] = [None] * count  # type: ignore # todo
    for i in range(count):
        thread_list[i] = threading.Thread(target=_ping_host_thread, args=(host_address, result_list, i))
        thread_list[i].start()
    for i in range(count):
        thread_list[i].join()
    new_list = [each for each in result_list if each]
    return (
        f"  â± Ping {int(sum(new_list) / len(new_list))} ms ({len(new_list)}/{count})"
        if new_list
        else f"  ğŸ”´ Ping - ms (0/{count})"
    )


def check_version() -> Optional[int]:
    if config.update_check:
        url = "https://api.github.com/repos/sqzw-x/mdcx/releases/latest"
        res_json, error = get_json(url)
        if res_json is not None:
            try:
                latest_version = res_json["tag_name"]
                latest_version = int(latest_version)
                return latest_version
            except Exception:
                signal.add_log(f"âŒ è·å–æœ€æ–°ç‰ˆæœ¬å¤±è´¥ï¼{res_json}")
    return None


def check_theporndb_api_token() -> str:
    tips = "âœ… è¿æ¥æ­£å¸¸! "
    headers = config.headers
    proxies = config.proxies
    timeout = config.timeout
    api_token = config.theporndb_api_token
    url = "https://api.theporndb.net/scenes/hash/8679fcbdd29fa735"
    headers = {
        "Authorization": f"Bearer {api_token}",
        "Content-Type": "application/json",
        "Accept": "application/json",
        "User-Agent": get_user_agent(),
    }
    if not api_token:
        tips = "âŒ æœªå¡«å†™ API Tokenï¼Œå½±å“æ¬§ç¾åˆ®å‰Šï¼å¯åœ¨ã€Œè®¾ç½®ã€-ã€Œç½‘ç»œã€æ·»åŠ ï¼"
    else:
        try:
            response = requests.get(url, headers=headers, proxies=proxies, timeout=timeout, verify=False)
            if response.status_code == 401 and "Unauthenticated" in str(response.text):
                tips = "âŒ API Token é”™è¯¯ï¼å½±å“æ¬§ç¾åˆ®å‰Šï¼è¯·åˆ°ã€Œè®¾ç½®ã€-ã€Œç½‘ç»œã€ä¸­ä¿®æ”¹ã€‚"
            elif response.status_code == 200:
                if response.json().get("data"):
                    tips = "âœ… è¿æ¥æ­£å¸¸ï¼"
                else:
                    tips = "âŒ è¿”å›æ•°æ®å¼‚å¸¸ï¼"
            else:
                tips = f"âŒ è¿æ¥å¤±è´¥ï¼è¯·æ£€æŸ¥ç½‘ç»œæˆ–ä»£ç†è®¾ç½®ï¼ {response.status_code} {response.text}"
        except Exception as e:
            tips = f"âŒ è¿æ¥å¤±è´¥!è¯·æ£€æŸ¥ç½‘ç»œæˆ–ä»£ç†è®¾ç½®ï¼ {e}"
    signal.show_log_text(tips.replace("âŒ", " âŒ ThePornDB").replace("âœ…", " âœ… ThePornDB"))
    return tips


def _get_pic_by_google(pic_url):
    google_keyused = config.google_keyused
    google_keyword = config.google_keyword
    req_url = f"https://www.google.com/searchbyimage?sbisrc=2&image_url={pic_url}"
    # req_url = f'https://lens.google.com/uploadbyurl?url={pic_url}&hl=zh-CN&re=df&ep=gisbubu'
    response, error = get_text(req_url)
    big_pic = True
    if response is not None:
        url_list = re.findall(r'a href="([^"]+isz:l[^"]+)">', response)
        url_list_middle = re.findall(r'a href="([^"]+isz:m[^"]+)">', response)
        if not url_list and url_list_middle:
            url_list = url_list_middle
            big_pic = False
        if url_list:
            req_url = "https://www.google.com" + url_list[0].replace("amp;", "")
            response, error = get_text(req_url)
            if response is not None:
                url_list = re.findall(r'\["(http[^"]+)",(\d{3,4}),(\d{3,4})\],[^[]', response)
                # ä¼˜å…ˆä¸‹è½½æ”¾å‰é¢
                new_url_list = []
                for each_url in url_list.copy():
                    if int(each_url[2]) < 800:
                        url_list.remove(each_url)

                for each_key in google_keyused:
                    for each_url in url_list.copy():
                        if each_key in each_url[0]:
                            new_url_list.append(each_url)
                            url_list.remove(each_url)
                # åªä¸‹è½½å…³æ—¶ï¼Œè¿½åŠ å‰©ä½™åœ°å€
                if "goo_only" not in config.download_hd_pics:
                    new_url_list += url_list
                # è§£æåœ°å€
                for each in new_url_list:
                    temp_url = each[0]
                    for temp_keyword in google_keyword:
                        if temp_keyword in temp_url:
                            break
                    else:
                        h = int(each[1])
                        w = int(each[2])
                        if w > h and w / h < 1.4:  # thumb è¢«æ‹‰é«˜æ—¶è·³è¿‡
                            continue

                        p_url = temp_url.encode("utf-8").decode(
                            "unicode_escape"
                        )  # urlä¸­çš„Unicodeå­—ç¬¦è½¬ä¹‰ï¼Œä¸è½¬ä¹‰ï¼Œurlè¯·æ±‚ä¼šå¤±è´¥
                        if "m.media-amazon.com" in p_url:
                            p_url = re.sub(r"\._[_]?AC_[^\.]+\.", ".", p_url)
                            pic_size = get_imgsize(p_url)
                            if pic_size[0]:
                                return p_url, pic_size, big_pic
                        else:
                            url = check_url(p_url)
                            if url:
                                pic_size = (w, h)
                                return url, pic_size, big_pic
    return "", "", ""


def get_big_pic_by_google(pic_url, poster=False):
    url, pic_size, big_pic = _get_pic_by_google(pic_url)
    if not poster:
        if big_pic or (
            pic_size and int(pic_size[0]) > 800 and int(pic_size[1]) > 539
        ):  # cover æœ‰å¤§å›¾æ—¶æˆ–è€…å›¾ç‰‡é«˜åº¦ > 800 æ—¶ä½¿ç”¨è¯¥å›¾ç‰‡
            return url, pic_size
        return "", ""
    if url and int(pic_size[1]) < 1000:  # posterï¼Œå›¾ç‰‡é«˜åº¦å°äº 1500ï¼Œé‡æ–°æœç´¢ä¸€æ¬¡
        url, pic_size, big_pic = _get_pic_by_google(url)
    if pic_size and (
        big_pic or "blogger.googleusercontent.com" in url or int(pic_size[1]) > 560
    ):  # posterï¼Œå¤§å›¾æˆ–é«˜åº¦ > 560 æ—¶ï¼Œä½¿ç”¨è¯¥å›¾ç‰‡
        return url, pic_size
    else:
        return "", ""
