#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import re
import socket
import threading
import traceback
from concurrent.futures import ThreadPoolExecutor
from io import BytesIO
from threading import Lock
from urllib.parse import quote

import cloudscraper
import curl_cffi.requests
import requests
import urllib3.util.connection as urllib3_cn
from PIL import Image
from ping3 import ping
from requests.exceptions import ChunkedEncodingError, ConnectTimeout, ConnectionError, ContentDecodingError, HTTPError, \
    InvalidHeader, InvalidProxyURL, InvalidURL, ProxyError, ReadTimeout, SSLError, StreamConsumedError, Timeout, \
    TooManyRedirects, URLRequired

from models.base.utils import get_user_agent, singleton
from models.config.config import config
from models.signals import signal


def _allowed_gai_family():
    """
     https://github.com/shazow/urllib3/blob/master/urllib3/util/connection.py
    """
    family = socket.AF_INET
    return family


try:
    if config.ipv4_only:
        urllib3_cn.allowed_gai_family = _allowed_gai_family
except:
    urllib3_cn.allowed_gai_family = _allowed_gai_family


@singleton
class WebRequests:
    def __init__(self):
        self.session_g = requests.Session()
        self.session_g.mount('https://', requests.adapters.HTTPAdapter(pool_connections=100, pool_maxsize=100))
        self.session_g.mount('http://', requests.adapters.HTTPAdapter(pool_connections=100, pool_maxsize=100))
        self.scraper = cloudscraper.create_scraper(
            browser={'browser': 'firefox', 'platform': 'windows', 'mobile': False})  # returns a CloudScraper instance
        self.lock = Lock()
        self.pool = ThreadPoolExecutor(32)
        self.curl_session = curl_cffi.requests.Session()

    def get_html(self, url: str, headers=None, cookies=None, proxies=True, allow_redirects=True, json_data=False,
                 content=False,
                 res=False, keep=True, timeout=False, encoding='utf-8', back_cookie=False):
        # è·å–ä»£ç†ä¿¡æ¯
        retry_times = config.retry
        if proxies:
            proxies = config.proxies
        else:
            proxies = {
                "http": None,
                "https": None,
            }

        if not headers:
            headers = config.headers
        if not timeout:
            timeout = config.timeout
        if 'getchu' in url:
            headers_o = {
                'Referer': 'http://www.getchu.com/top.html',
            }
            headers.update(headers_o)
        elif 'xcity' in url:
            headers_o = {
                'referer': 'https://xcity.jp/result_published/?genre=%2Fresult_published%2F&q=2&sg=main&num=60',
            }
            headers.update(headers_o)

        signal.add_log(f'ğŸ” è¯·æ±‚ {url}')
        for i in range(int(retry_times)):
            try:
                if keep:
                    response = self.session_g.get(url, headers=headers, cookies=cookies, proxies=proxies,
                                                  timeout=timeout,
                                                  verify=False, allow_redirects=allow_redirects)
                else:
                    response = requests.get(url, headers=headers, cookies=cookies, proxies=proxies, timeout=timeout,
                                            verify=False, allow_redirects=allow_redirects)
                # print(response.headers.items())
                # print(response.status_code, url)
                _header = response.headers
                if back_cookie:
                    _header = response.cookies if response.cookies else _header
                if response.status_code > 299:
                    if response.status_code == 302 and allow_redirects:
                        pass
                    else:
                        error_info = f"{response.status_code} {url}"
                        signal.add_log('ğŸ”´ é‡è¯• [%s/%s] %s' % (i + 1, retry_times, error_info))
                        continue
                else:
                    signal.add_log(f'âœ… æˆåŠŸ {url}')
                if res:
                    return _header, response
                if content:
                    return _header, response.content
                response.encoding = encoding
                if json_data:
                    return _header, response.json()
                return _header, response.text
            except Exception as e:
                error_info = '%s\nError: %s' % (url, e)
                signal.add_log('[%s/%s] %s' % (i + 1, retry_times, error_info))
        signal.add_log(f"ğŸ”´ è¯·æ±‚å¤±è´¥ï¼{error_info}")
        return False, error_info

    def post_html(self, url: str, data=None, json=None, headers=None, cookies=None, proxies=True, json_data=False,
                  keep=True):
        # è·å–ä»£ç†ä¿¡æ¯
        timeout = config.timeout
        retry_times = config.retry
        if not headers:
            headers = config.headers
        if proxies:
            proxies = config.proxies
        else:
            proxies = {
                "http": None,
                "https": None,
            }

        signal.add_log(f'ğŸ” POSTè¯·æ±‚ {url}')
        for i in range(int(retry_times)):
            try:
                if keep:
                    response = self.session_g.post(url=url, data=data, json=json, headers=headers, cookies=cookies,
                                                   proxies=proxies, timeout=timeout, verify=False)
                else:
                    response = requests.post(url=url, data=data, json=json, headers=headers, cookies=cookies,
                                             proxies=proxies, timeout=timeout, verify=False)
                if response.status_code > 299:
                    error_info = f"{response.status_code} {url}"
                    signal.add_log('ğŸ”´ é‡è¯• [%s/%s] %s' % (i + 1, retry_times, error_info))
                    continue
                else:
                    signal.add_log(f'âœ… POSTæˆåŠŸ {url}')
                response.encoding = 'utf-8'
                if json_data:
                    return True, response.json()
                return True, response.text
            except Exception as e:
                error_info = '%s\nError: %s' % (url, e)
                signal.add_log('[%s/%s] %s' % (i + 1, retry_times, error_info))
        signal.add_log(f"ğŸ”´ è¯·æ±‚å¤±è´¥ï¼{error_info}")
        return False, error_info

    def scraper_html(self, url: str, proxies=True, cookies=None, headers=None):
        # è·å–ä»£ç†ä¿¡æ¯
        is_docker = config.is_docker
        timeout = config.timeout
        retry_times = config.retry
        if is_docker:
            return self.get_html(url, proxies=proxies, cookies=cookies)
        if proxies:
            proxies = config.proxies
        else:
            proxies = {
                "http": None,
                "https": None,
            }

        signal.add_log(f'ğŸ” Scraperè¯·æ±‚ {url}')
        for i in range(retry_times):
            try:
                with self.scraper.get(url, headers=headers, proxies=proxies, cookies=cookies, timeout=timeout) as f:
                    response = f

                if response.status_code > 299:
                    error_info = f"{response.status_code} {url} {str(f.cookies).replace('<RequestsCookieJar[', '').replace(']>', '')}"
                    return False, error_info
                else:
                    signal.add_log(f'âœ… ScraperæˆåŠŸ {url}')
                response.encoding = 'utf-8'
                return True, f.text
            except Exception as e:
                error_info = '%s\nError: %s' % (url, e)
                signal.add_log('ğŸ”´ é‡è¯• [%s/%s] %s' % (i + 1, retry_times, error_info))
        signal.add_log(f"ğŸ”´ è¯·æ±‚å¤±è´¥ï¼{error_info}")
        return False, error_info

    def _get_filesize(self, url):
        proxies = config.proxies
        timeout = config.timeout
        retry_times = config.retry
        headers = config.headers

        for _ in range(int(retry_times)):
            try:
                response = self.session_g.head(url, headers=headers, proxies=proxies, timeout=timeout, verify=False)
                file_size = response.headers.get('Content-Length')
                return file_size
            except:
                pass
        return False

    def multi_download(self, url, file_path):
        # è·å–æ–‡ä»¶å¤§å°
        file_size = self._get_filesize(url)

        # åˆ¤æ–­æ˜¯ä¸æ˜¯webpæ–‡ä»¶
        webp = False
        if file_path.endswith('jpg') and '.webp' in url:
            webp = True

        # æ²¡æœ‰å¤§å°æ—¶ï¼Œä¸æ”¯æŒåˆ†æ®µä¸‹è½½ï¼Œç›´æ¥ä¸‹è½½ï¼›< 2 MB çš„ç›´æ¥ä¸‹è½½
        MB = 1024 ** 2
        if not file_size or int(file_size) <= 2 * MB or webp:
            result, response = get_html(url, content=True)
            if result:
                if webp:
                    byte_stream = BytesIO(response)
                    img = Image.open(byte_stream)
                    if img.mode == 'RGBA':
                        img = img.convert('RGB')
                    img.save(file_path, quality=95, subsampling=0)
                    img.close()
                else:
                    with open(file_path, "wb") as f:
                        f.write(response)
                return True
            return False

        return self._multi_download2(url, file_path, int(file_size))

    def _multi_download2(self, url, file_path, file_size) -> bool:
        # åˆ†å—ï¼Œæ¯å— 1 MB
        MB = 1024 ** 2
        file_size = int(file_size)
        each_size = min(int(1 * MB), file_size)
        parts = [(s, min(s + each_size, file_size)) for s in range(0, file_size, each_size)]
        # print(f'åˆ†å—æ•°ï¼š{len(parts)} \n')

        # å…ˆå†™å…¥ä¸€ä¸ªæ–‡ä»¶
        f = open(file_path, 'wb')
        f.truncate(file_size)
        f.close()

        # å¼€å§‹ä¸‹è½½
        i = 0
        task_list = []
        for part in parts:
            i += 1
            start, end = part
            task_list.append([start, end, i, url, file_path])
        result = self.pool.map(self._start_download, task_list)
        for res in result:
            if not res:
                # bar.close()
                return False
        # bar.close()
        return True

    def _start_download(self, task) -> bool:
        start, end, i, url, file_path = task

        proxies = config.proxies
        timeout = config.timeout
        retry_times = config.retry
        headers = config.headers
        _headers = headers.copy()
        _headers['Range'] = f'bytes={start}-{end}'
        for _ in range(int(retry_times)):
            try:
                response = self.session_g.get(url, headers=_headers, proxies=proxies, timeout=timeout, verify=False,
                                              stream=True)
                chunk_size = 128
                chunks = []
                for chunk in response.iter_content(chunk_size=chunk_size):
                    chunks.append(chunk)
                    # bar.update(chunk_size)
                self.lock.acquire()
                with open(file_path, "rb+") as fp:
                    fp.seek(start)
                    for chunk in chunks:
                        fp.write(chunk)
                    self.lock.release()
                    # é‡Šæ”¾é”
                del chunks
                return True
            except:
                pass
        return False

    def curl_html(self, url, headers=None, proxies=True, cookies=None):
        """
        curlè¯·æ±‚(æ¨¡æ‹Ÿæµè§ˆå™¨æŒ‡çº¹)
        """
        # è·å–ä»£ç†ä¿¡æ¯
        retry_times = config.retry
        if proxies:
            proxies = config.proxies
        else:
            proxies = {
                "http": None,
                "https": None,
            }

        signal.add_log(f'ğŸ” è¯·æ±‚ {url}')
        for i in range(int(retry_times)):
            try:
                response = self.curl_session.get(url_encode(url), headers=headers, cookies=cookies, proxies=proxies,
                                                 impersonate="edge99")
                if 'amazon' in url:
                    response.encoding = 'Shift_JIS'
                else:
                    response.encoding = 'UFT-8'
                if response.status_code == 200:
                    signal.add_log(f'âœ… æˆåŠŸ {url}')
                    return response.headers, response.text
                else:
                    error_info = f"{response.status_code} {url}"
                    signal.add_log('ğŸ”´ é‡è¯• [%s/%s] %s' % (i + 1, retry_times, error_info))
                    continue
            except Exception as e:
                error_info = '%s\nError: %s' % (url, e)
                signal.add_log('[%s/%s] %s' % (i + 1, retry_times, error_info))
        signal.add_log(f"ğŸ”´ è¯·æ±‚å¤±è´¥ï¼{error_info}")
        return False, error_info


web = WebRequests()
get_html = web.get_html
post_html = web.post_html
scraper_html = web.scraper_html
multi_download = web.multi_download
curl_html = web.curl_html


def url_encode(url):
    new_url = ''
    for i in url:
        if i not in [':', '/', '&', '?', '=', '%']:
            i = quote(i)
        new_url += i
    return new_url


def check_url(url, length=False, real_url=False):
    proxies = config.proxies
    timeout = config.timeout
    retry_times = config.retry
    headers = config.headers

    if not url:
        return 0

    signal.add_log(f'â›‘ï¸ æ£€æµ‹é“¾æ¥ {url}')
    if 'http' not in url:
        signal.add_log(f'ğŸ”´ æ£€æµ‹æœªé€šè¿‡ï¼é“¾æ¥æ ¼å¼é”™è¯¯ï¼ {url}')
        return 0

    if 'getchu' in url:
        headers_o = {
            'Referer': 'http://www.getchu.com/top.html',
        }
        headers.update(headers_o)

    for j in range(retry_times):
        try:
            r = requests.head(url, headers=headers, proxies=proxies, timeout=timeout, verify=False,
                              allow_redirects=True)

            # çŠ¶æ€ç  > 299ï¼Œè¡¨ç¤ºè¯·æ±‚å¤±è´¥ï¼Œè§†ä¸ºä¸å¯ç”¨
            if r.status_code > 299:
                error_info = f"{r.status_code} {url}"
                signal.add_log('ğŸ”´ è¯·æ±‚å¤±è´¥ï¼ é‡è¯•: [%s/%s] %s' % (j + 1, retry_times, error_info))
                continue

            # è¿”å›é‡å®šå‘çš„url
            true_url = r.url
            if real_url:
                return true_url

            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½• https://lookaside.fbsbx.com/lookaside/crawler/media/?media_id=637921621668064
            if 'login' in true_url:
                signal.add_log(f'ğŸ”´ æ£€æµ‹æœªé€šè¿‡ï¼éœ€è¦ç™»å½•æŸ¥çœ‹ {true_url}')
                return 0

            # æ£€æŸ¥æ˜¯å¦å¸¦æœ‰å›¾ç‰‡ä¸å­˜åœ¨çš„å…³é”®è¯
            '''
            å¦‚æœè·³è½¬åçš„çœŸå®é“¾æ¥å­˜åœ¨åˆ å›¾æ ‡è¯†ï¼Œè§†ä¸ºä¸å¯ç”¨
            https://pics.dmm.co.jp/mono/movie/n/now_printing/now_printing.jpg dmm åˆ å›¾çš„æ ‡è¯†ï¼Œjavbusã€javlib ç”¨çš„æ˜¯ dmm å›¾
            https://static.mgstage.com/mgs/img/common/actress/nowprinting.jpg mgstage åˆ å›¾çš„æ ‡è¯†
            https://jdbimgs.com/images/noimage_600x404.jpg javdbåˆ é™¤çš„å›¾ WANZ-921
            https://www.javbus.com/imgs/cover/nopic.jpg
            https://assets.tumblr.com/images/media_violation/community_guidelines_v1_1280.png tumblråˆ é™¤çš„å›¾
            '''
            bad_url_keys = ['now_printing', 'nowprinting', 'noimage', 'nopic', 'media_violation']
            for each_key in bad_url_keys:
                if each_key in true_url:
                    signal.add_log(f'ğŸ”´ æ£€æµ‹æœªé€šè¿‡ï¼å½“å‰å›¾ç‰‡å·²è¢«ç½‘ç«™åˆ é™¤ {url}')
                    return 0

            # è·å–æ–‡ä»¶å¤§å°ã€‚å¦‚æœæ²¡æœ‰è·å–åˆ°æ–‡ä»¶å¤§å°ï¼Œå°è¯•ä¸‹è½½15kæ•°æ®ï¼Œå¦‚æœå¤±è´¥ï¼Œè§†ä¸ºä¸å¯ç”¨
            content_length = r.headers.get('Content-Length')
            if not content_length:
                response = requests.get(true_url, headers=headers, proxies=proxies, timeout=timeout, verify=False,
                                        stream=True)
                i = 0
                chunk_size = 5120
                for _ in response.iter_content(chunk_size):
                    i += 1
                    if i == 3:
                        response.close()
                        signal.add_log(f'âœ… æ£€æµ‹é€šè¿‡ï¼æœªè¿”å›å¤§å°ï¼Œé¢„ä¸‹è½½15ké€šè¿‡ {true_url}')
                        return 10240 if length else true_url
                signal.add_log(f'ğŸ”´ æ£€æµ‹æœªé€šè¿‡ï¼æœªè¿”å›å¤§å°ï¼Œé¢„ä¸‹è½½15kå¤±è´¥ {true_url}')
                return 0

            # å¦‚æœè¿”å›å†…å®¹çš„æ–‡ä»¶å¤§å° < 8kï¼Œè§†ä¸ºä¸å¯ç”¨
            elif int(content_length) < 8192:
                signal.add_log(f'ğŸ”´ æ£€æµ‹æœªé€šè¿‡ï¼è¿”å›å¤§å°({content_length}) < 8k {true_url}')
                return 0
            signal.add_log(f'âœ… æ£€æµ‹é€šè¿‡ï¼è¿”å›å¤§å°({content_length}) {true_url}')
            return int(content_length) if length else true_url
        except InvalidProxyURL as e:
            error_info = f' æ— æ•ˆçš„ä»£ç†é“¾æ¥ ({e}) {url}'
        except ProxyError as e:
            error_info = f' ä»£ç†é”™è¯¯ {e} {url}'
        except SSLError as e:
            error_info = f' SSLé”™è¯¯ ({e}) {url}'
        except ConnectTimeout as e:
            error_info = f' å°è¯•è¿æ¥åˆ°è¿œç¨‹æœåŠ¡å™¨æ—¶è¶…æ—¶ ({e}) {url}'
        except ReadTimeout as e:
            error_info = f' æœåŠ¡å™¨æœªåœ¨åˆ†é…çš„æ—¶é—´å†…å‘é€ä»»ä½•æ•°æ® ({e}) {url}'
        except Timeout as e:
            error_info = f' è¯·æ±‚è¶…æ—¶é”™è¯¯ ({e}) {url}'
        except ConnectionError as e:
            error_info = f' è¿æ¥é”™è¯¯ {e} {url}'
        except URLRequired as e:
            error_info = f' URLæ ¼å¼é”™è¯¯ ({e}) {url}'
        except TooManyRedirects as e:
            error_info = f' è¿‡å¤šçš„é‡å®šå‘ ({e}) {url}'
        except InvalidURL as e:
            error_info = f' æ— æ•ˆçš„url ({e}) {url}'
        except InvalidHeader as e:
            error_info = f' æ— æ•ˆçš„è¯·æ±‚å¤´ ({e}) {url}'
        except HTTPError as e:
            error_info = f' HTTPé”™è¯¯ {e} {url}'
        except ChunkedEncodingError as e:
            error_info = f' æœåŠ¡å™¨å£°æ˜äº†åˆ†å—ç¼–ç ï¼Œä½†å‘é€äº†æ— æ•ˆçš„åˆ†å— ({e}) {url}'
        except ContentDecodingError as e:
            error_info = f' è§£ç å“åº”å†…å®¹å¤±è´¥ ({e}) {url}'
        except StreamConsumedError as e:
            error_info = f' è¯¥å“åº”çš„å†…å®¹å·²è¢«å ç”¨ ({e}) {url}'
        except Exception as e:
            error_info = f' Error ({e}) {url}'
        signal.add_log('ğŸ”´ é‡è¯• [%s/%s] %s' % (j + 1, retry_times, error_info))
    signal.add_log(f'ğŸ”´ æ£€æµ‹æœªé€šè¿‡ï¼ {url}')
    return 0


def get_avsox_domain():
    issue_url = 'https://tellme.pw/avsox'
    result, response = get_html(issue_url)
    domain = 'https://avsox.click'
    if result:
        res = re.findall(r'(https://[^"]+)', response)
        for s in res:
            if s and 'https://avsox.com' not in s or 'api.qrserver.com' not in s:
                return s
    return domain


def get_amazon_data(req_url):
    """
    è·å– Amazon æ•°æ®ï¼Œä¿®æ”¹åœ°åŒºä¸º540-0002
    """
    headers = {
        "accept-encoding": "gzip, deflate, br",
        'Host': 'www.amazon.co.jp',
        'User-Agent': get_user_agent(),
    }
    try:
        result, html_info = curl_html(req_url)
    except:
        result, html_info = curl_html(req_url, headers=headers)
        session_id = ''
        ubid_acbjp = ''
        if x := re.findall(r'sessionId: "([^"]+)', html_info):
            session_id = x[0]
        if x := re.findall(r'ubid-acbjp=([^ ]+)', html_info):
            ubid_acbjp = x[0]
        headers_o = {
            'cookie': f'session-id={session_id}; ubid_acbjp={ubid_acbjp}',
        }
        headers.update(headers_o)
        result, html_info = curl_html(req_url, headers=headers)

    if not result:
        if '503 http' in html_info:
            headers = {
                'Host': 'www.amazon.co.jp',
                'User-Agent': get_user_agent(),
            }
            result, html_info = get_html(req_url, headers=headers, keep=False, back_cookie=True)

        if not result:
            return False, html_info

    if '540-0002' not in html_info:
        try:
            # è·å– anti_csrftoken_a2z
            anti_csrftoken_a2z = re.findall(r'anti-csrftoken-a2z([^}]+)', html_info)[0].replace('&quot;', '').strip(':')
            session_id = re.findall(r'sessionId: "([^"]+)', html_info)[0]
            ubid_acbjp = ''
            if 'ubid-acbjp' in str(result):
                try:
                    ubid_acbjp = result['set-cookie']
                except:
                    try:
                        ubid_acbjp = re.findall(r'ubid-acbjp=([^ ]+)', str(result))[0]
                    except:
                        pass
            headers_o = {
                'Anti-csrftoken-a2z': anti_csrftoken_a2z,
                'cookie': f'session-id={session_id}; ubid_acbjp={ubid_acbjp}',
            }
            headers.update(headers_o)
            mid_url = 'https://www.amazon.co.jp/portal-migration/hz/glow/get-rendered-toaster' \
                      '?pageType=Search&aisTransitionState=in&rancorLocationSource=REALM_DEFAULT&_='
            result, html = curl_html(mid_url, headers=headers)
            anti_csrftoken_a2z = re.findall(r'csrfToken="([^"]+)', html)[0]
            try:
                ubid_acbjp = re.findall(r'ubid-acbjp=([^ ]+)', str(result))[0]
            except:
                pass

            # ä¿®æ”¹é…é€åœ°å€ä¸ºæ—¥æœ¬ï¼Œè¿™æ ·ç»“æœå¤šä¸€äº›
            headers_o = {
                'Anti-csrftoken-a2z': anti_csrftoken_a2z,
                'Content-length': '140',
                'Content-Type': 'application/json',
                'cookie': f'session-id={session_id}; ubid_acbjp={ubid_acbjp}',
            }
            headers.update(headers_o)
            post_url = 'https://www.amazon.co.jp/portal-migration/hz/glow/address-change?actionSource=glow'
            data = {"locationType": "LOCATION_INPUT", "zipCode": "540-0002", "storeContext": "generic",
                    "deviceType": "web", "pageType": "Search", "actionSource": "glow"}
            result, html = post_html(post_url, json=data, headers=headers)
            if result:
                if '540-0002' in str(html):
                    headers = {
                        'Host': 'www.amazon.co.jp',
                        'User-Agent': get_user_agent(),
                    }
                    result, html_info = curl_html(req_url, headers=headers)
                else:
                    print('Amazon ä¿®æ”¹åœ°åŒºå¤±è´¥: ', req_url, str(result), str(html))
            else:
                print('Amazon ä¿®æ”¹åœ°åŒºå¼‚å¸¸: ', req_url, str(result), str(html))

        except Exception as e:
            print('Amazon ä¿®æ”¹åœ°åŒºå‡ºé”™: ', req_url, str(e))
            print(traceback.format_exc())

    return result, html_info


if "__main__" == __name__:
    # æµ‹è¯•ä¸‹è½½æ–‡ä»¶
    list1 = [
        'https://issuecdn.baidupcs.com/issue/netdisk/yunguanjia/BaiduNetdisk_7.2.8.9.exe',
        'https://cc3001.dmm.co.jp/litevideo/freepv/1/118/118abw015/118abw015_mhb_w.mp4',
        'https://cc3001.dmm.co.jp/litevideo/freepv/1/118/118abw00016/118abw00016_mhb_w.mp4',
        'https://cc3001.dmm.co.jp/litevideo/freepv/1/118/118abw00017/118abw00017_mhb_w.mp4',
        'https://cc3001.dmm.co.jp/litevideo/freepv/1/118/118abw00018/118abw00018_mhb_w.mp4',
        'https://cc3001.dmm.co.jp/litevideo/freepv/1/118/118abw00019/118abw00019_mhb_w.mp4',
        'https://www.prestige-av.com/images/corner/goods/prestige/tktabw/018/pb_tktabw-018.jpg',
        'https://iqq1.one/preview/80/b/3SBqI8OjheI-800.jpg?v=1636404497',
    ]
    for each in list1:
        url = each
        file_path = each.split('/')[-1]
        t = threading.Thread(target=multi_download, args=(url, file_path))
        t.start()

    # æ­»å¾ªç¯ï¼Œé¿å…ç¨‹åºç¨‹åºå®Œåï¼Œpoolè‡ªåŠ¨å…³é—­
    while True:
        pass


def get_imgsize(url):
    proxies = config.proxies
    timeout = config.timeout
    retry_times = config.retry
    headers = config.headers

    for _ in range(int(retry_times)):
        try:
            response = requests.get(url, headers=headers, proxies=proxies, timeout=timeout, verify=False, stream=True)
            if response.status_code == 200:
                file_head = BytesIO()
                chunk_size = 1024 * 10
                for chunk in response.iter_content(chunk_size):
                    file_head.write(chunk)
                    response.close()
                    try:
                        img = Image.open(file_head)
                        return img.size
                    except:
                        return 0, 0
        except:
            return 0, 0
    return 0, 0


def get_dmm_trailer(trailer_url):  # è·å–é¢„è§ˆç‰‡
    if '.dmm.co' not in trailer_url:
        return trailer_url
    if trailer_url.startswith('//'):
        trailer_url = 'https:' + trailer_url
    '''
    '_sm_w.mp4': 320*180, 3.8MB
    '_dm_w.mp4': 560*316, 10.1MB
    '_dmb_w.mp4': 720*404, 14.6MB
    '_mhb_w.mp4': 720*404, 27.9MB
    https://cc3001.dmm.co.jp/litevideo/freepv/s/ssi/ssis00090/ssis00090_sm_w.mp4
    https://cc3001.dmm.co.jp/litevideo/freepv/s/ssi/ssis00090/ssis00090_dm_w.mp4
    https://cc3001.dmm.co.jp/litevideo/freepv/s/ssi/ssis00090/ssis00090_dmb_w.mp4
    https://cc3001.dmm.co.jp/litevideo/freepv/s/ssi/ssis00090/ssis00090_mhb_w.mp4
    '''

    # keylist = ['_sm_w.mp4', '_dm_w.mp4', '_dmb_w.mp4', '_mhb_w.mp4']
    if '_mhb_w.mp4' not in trailer_url:
        t = re.findall(r'(.+)(_[sd]mb?_w.mp4)', trailer_url)
        if t:
            s, e = t[0]
            mhb_w = s + '_mhb_w.mp4'
            dmb_w = s + '_dmb_w.mp4'
            dm_w = s + '_dm_w.mp4'
            if e == '_dmb_w.mp4':
                if check_url(mhb_w):
                    trailer_url = mhb_w
            elif e == '_dm_w.mp4':
                if check_url(mhb_w):
                    trailer_url = mhb_w
                elif check_url(dmb_w):
                    trailer_url = dmb_w
            elif e == '_sm_w.mp4':
                if check_url(mhb_w):
                    trailer_url = mhb_w
                elif check_url(dmb_w):
                    trailer_url = dmb_w
                elif check_url(dm_w):
                    trailer_url = dm_w
    return trailer_url


def _ping_host_thread(host_address, result_list, i):
    response = ping(host_address, timeout=1)
    result_list[i] = int(response * 1000) if response else 0


def ping_host(host_address):
    count = config.retry
    result_list = [None] * count
    thread_list = [0] * count
    for i in range(count):
        thread_list[i] = threading.Thread(target=_ping_host_thread, args=(host_address, result_list, i))
        thread_list[i].start()
    for i in range(count):
        thread_list[i].join()
    new_list = [each for each in result_list if each]
    return f'  â± Ping {int(sum(new_list) / len(new_list))} ms ({len(new_list)}/{count})' \
        if new_list else f'  ğŸ”´ Ping - ms (0/{count})'


def check_version():
    if config.update_check == 'on':
        url = 'https://api.github.com/repos/sqzw-x/mdcx/releases/latest'
        _, res_json = get_html(url, json_data=True)
        if isinstance(res_json, dict):
            try:
                latest_version = res_json['tag_name']
                latest_version = int(latest_version)
                return latest_version
            except:
                signal.add_log(f'âŒ è·å–æœ€æ–°ç‰ˆæœ¬å¤±è´¥ï¼{res_json}')


def check_theporndb_api_token():
    tips = 'âœ… è¿æ¥æ­£å¸¸! '
    headers = config.headers
    proxies = config.proxies
    timeout = config.timeout
    api_token = config.theporndb_api_token
    url = 'https://api.metadataapi.net/scenes/hash/8679fcbdd29fa735'
    headers = {
        'Authorization': f'Bearer {api_token}',
        'Content-Type': 'application/json',
        'Accept': 'application/json',
        'User-Agent': get_user_agent(),
    }
    if not api_token:
        tips = 'âŒ æœªå¡«å†™ API Tokenï¼Œå½±å“æ¬§ç¾åˆ®å‰Šï¼å¯åœ¨ã€Œè®¾ç½®ã€-ã€Œç½‘ç»œã€æ·»åŠ ï¼'
    else:
        try:
            response = requests.get(url, headers=headers, proxies=proxies, timeout=timeout, verify=False)
            if response.status_code == 401 and 'Unauthenticated' in str(response.text):
                tips = 'âŒ API Token é”™è¯¯ï¼å½±å“æ¬§ç¾åˆ®å‰Šï¼è¯·åˆ°ã€Œè®¾ç½®ã€-ã€Œç½‘ç»œã€ä¸­ä¿®æ”¹ã€‚'
            elif response.status_code == 200:
                if response.json().get('data'):
                    tips = 'âœ… è¿æ¥æ­£å¸¸ï¼'
                else:
                    tips = 'âŒ è¿”å›æ•°æ®å¼‚å¸¸ï¼'
            else:
                tips = f'âŒ è¿æ¥å¤±è´¥ï¼è¯·æ£€æŸ¥ç½‘ç»œæˆ–ä»£ç†è®¾ç½®ï¼ {response.status_code} {response.text}'
        except Exception as e:
            tips = f'âŒ è¿æ¥å¤±è´¥!è¯·æ£€æŸ¥ç½‘ç»œæˆ–ä»£ç†è®¾ç½®ï¼ {e}'
    signal.show_log_text(tips.replace('âŒ', ' âŒ ThePornDB').replace('âœ…', ' âœ… ThePornDB'))
    return tips


def _get_pic_by_google(pic_url):
    google_keyused = config.google_keyused
    google_keyword = config.google_keyword
    req_url = f'https://www.google.com/searchbyimage?sbisrc=2&image_url={pic_url}'
    # req_url = f'https://lens.google.com/uploadbyurl?url={pic_url}&hl=zh-CN&re=df&ep=gisbubu'
    result, response = get_html(req_url, keep=False)
    big_pic = True
    if result:
        url_list = re.findall(r'a href="([^"]+isz:l[^"]+)">', response)
        url_list_middle = re.findall(r'a href="([^"]+isz:m[^"]+)">', response)
        if not url_list and url_list_middle:
            url_list = url_list_middle
            big_pic = False
        if url_list:
            req_url = 'https://www.google.com' + url_list[0].replace('amp;', '')
            result, response = get_html(req_url, keep=False)
            if result:
                url_list = re.findall(r'\["(http[^"]+)",(\d{3,4}),(\d{3,4})\],[^[]', response)
                # ä¼˜å…ˆä¸‹è½½æ”¾å‰é¢
                new_url_list = []
                for each_url in url_list.copy():
                    if int(each_url[2]) < 800:
                        url_list.remove(each_url)

                for each_key in google_keyused:
                    for each_url in url_list.copy():
                        if each_key in each_url[0]:
                            new_url_list.append(each_url)
                            url_list.remove(each_url)
                # åªä¸‹è½½å…³æ—¶ï¼Œè¿½åŠ å‰©ä½™åœ°å€
                if 'goo_only' not in config.download_hd_pics:
                    new_url_list += url_list
                # è§£æåœ°å€
                for each in new_url_list:
                    temp_url = each[0]
                    for temp_keyword in google_keyword:
                        if temp_keyword in temp_url:
                            break
                    else:
                        h = int(each[1])
                        w = int(each[2])
                        if w > h and w / h < 1.4:  # thumb è¢«æ‹‰é«˜æ—¶è·³è¿‡
                            continue

                        p_url = temp_url.encode('utf-8').decode('unicode_escape')  # urlä¸­çš„Unicodeå­—ç¬¦è½¬ä¹‰ï¼Œä¸è½¬ä¹‰ï¼Œurlè¯·æ±‚ä¼šå¤±è´¥
                        if 'm.media-amazon.com' in p_url:
                            p_url = re.sub(r'\._[_]?AC_[^\.]+\.', '.', p_url)
                            pic_size = get_imgsize(p_url)
                            if pic_size[0]:
                                return p_url, pic_size, big_pic
                        else:
                            url = check_url(p_url)
                            if url:
                                pic_size = (w, h)
                                return url, pic_size, big_pic
    return '', '', ''


def get_big_pic_by_google(pic_url, poster=False):
    url, pic_size, big_pic = _get_pic_by_google(pic_url)
    if not poster:
        if big_pic or (
                pic_size and int(pic_size[0]) > 800 and int(pic_size[1]) > 539):  # cover æœ‰å¤§å›¾æ—¶æˆ–è€…å›¾ç‰‡é«˜åº¦ > 800 æ—¶ä½¿ç”¨è¯¥å›¾ç‰‡
            return url, pic_size
        return '', ''
    if url and int(pic_size[1]) < 1000:  # posterï¼Œå›¾ç‰‡é«˜åº¦å°äº 1500ï¼Œé‡æ–°æœç´¢ä¸€æ¬¡
        url, pic_size, big_pic = _get_pic_by_google(url)
    if pic_size and (big_pic or 'blogger.googleusercontent.com' in url or int(
            pic_size[1]) > 560):  # posterï¼Œå¤§å›¾æˆ–é«˜åº¦ > 560 æ—¶ï¼Œä½¿ç”¨è¯¥å›¾ç‰‡
        return url, pic_size
    else:
        return '', ''
