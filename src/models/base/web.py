#!/usr/bin/env python3
import asyncio
import re
import socket
import threading
from io import BytesIO
from typing import List, Literal, Optional, Tuple, overload
from urllib.parse import quote

import requests
import urllib3.util.connection as urllib3_cn
from PIL import Image
from ping3 import ping

from ..config.manager import config
from ..signals import signal
from .utils import get_user_agent
from .web_sync import get_json_sync


def _allowed_gai_family():
    """
    https://github.com/shazow/urllib3/blob/master/urllib3/util/connection.py
    """
    family = socket.AF_INET
    return family


try:
    if config.ipv4_only:
        urllib3_cn.allowed_gai_family = _allowed_gai_family
except Exception:
    urllib3_cn.allowed_gai_family = _allowed_gai_family


def url_encode(url: str) -> str:
    new_url = ""
    for i in url:
        if i not in [":", "/", "&", "?", "=", "%"]:
            i = quote(i)
        new_url += i
    return new_url


@overload
async def check_url(url: str, length: Literal[False] = False, real_url: bool = False) -> Optional[str]: ...
@overload
async def check_url(url: str, length: Literal[True] = True, real_url: bool = False) -> Optional[int]: ...
async def check_url(url: str, length: bool = False, real_url: bool = False):
    """
    æ£€æµ‹ä¸‹è½½é“¾æ¥. å¤±è´¥æ—¶è¿”å› None.

    Args:
        url (str): è¦æ£€æµ‹çš„ URL
        length (bool, optional): æ˜¯å¦è¿”å›æ–‡ä»¶å¤§å°. Defaults to False.
        real_url (bool, optional): ç›´æ¥è¿”å›çœŸå® URL ä¸è¿›è¡Œåç»­æ£€æŸ¥. Defaults to False.
    """
    if not url:
        return

    if "http" not in url:
        signal.add_log(f"ğŸ”´ æ£€æµ‹é“¾æ¥å¤±è´¥: æ ¼å¼é”™è¯¯ {url}")
        return

    try:
        # ä½¿ç”¨ request æ–¹æ³•å‘é€ HEAD è¯·æ±‚
        response, error = await config.async_client.request("HEAD", url)

        # å¤„ç†è¯·æ±‚å¤±è´¥çš„æƒ…å†µ
        if response is None:
            signal.add_log(f"ğŸ”´ æ£€æµ‹é“¾æ¥å¤±è´¥: {error}")
            return

        # ä¸è¾“å‡ºè·å– dmmé¢„è§ˆè§†é¢‘(trailer) æœ€é«˜åˆ†è¾¨ç‡çš„æµ‹è¯•ç»“æœåˆ°æ—¥å¿—ä¸­
        if response.status_code == 404 and "_w.mp4" in url:
            return

        # è¿”å›é‡å®šå‘çš„url
        true_url = str(response.url)
        if real_url:
            return true_url

        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•
        if "login" in true_url:
            signal.add_log(f"ğŸ”´ æ£€æµ‹é“¾æ¥å¤±è´¥: éœ€ç™»å½• {true_url}")
            return

        # æ£€æŸ¥æ˜¯å¦å¸¦æœ‰å›¾ç‰‡ä¸å­˜åœ¨çš„å…³é”®è¯
        bad_url_keys = ["now_printing", "nowprinting", "noimage", "nopic", "media_violation"]
        for each_key in bad_url_keys:
            if each_key in true_url:
                signal.add_log(f"ğŸ”´ æ£€æµ‹é“¾æ¥å¤±è´¥: å›¾ç‰‡å·²è¢«ç½‘ç«™åˆ é™¤ {url}")
                return

        # è·å–æ–‡ä»¶å¤§å°
        content_length = response.headers.get("Content-Length")
        if not content_length:
            # å¦‚æœæ²¡æœ‰è·å–åˆ°æ–‡ä»¶å¤§å°ï¼Œå°è¯•ä¸‹è½½æ•°æ®
            content, error = await config.async_client.get_content(true_url)

            if content is not None and len(content) > 0:
                signal.add_log(f"âœ… æ£€æµ‹é“¾æ¥é€šè¿‡: é¢„ä¸‹è½½æˆåŠŸ {true_url}")
                return 10240 if length else true_url
            else:
                signal.add_log(f"ğŸ”´ æ£€æµ‹é“¾æ¥å¤±è´¥: æœªè¿”å›å¤§å°ä¸”é¢„ä¸‹è½½å¤±è´¥ {true_url}")
                return
        # å¦‚æœè¿”å›å†…å®¹çš„æ–‡ä»¶å¤§å° < 8kï¼Œè§†ä¸ºä¸å¯ç”¨
        elif int(content_length) < 8192:
            signal.add_log(f"ğŸ”´ æ£€æµ‹é“¾æ¥å¤±è´¥: è¿”å›å¤§å°({content_length}) < 8k {true_url}")
            return

        signal.add_log(f"âœ… æ£€æµ‹é“¾æ¥é€šè¿‡: è¿”å›å¤§å°({content_length}) {true_url}")
        return int(content_length) if length else true_url

    except Exception as e:
        signal.add_log(f"ğŸ”´ æ£€æµ‹é“¾æ¥å¤±è´¥: æœªçŸ¥å¼‚å¸¸ {e} {url}")
        return


async def get_avsox_domain() -> str:
    issue_url = "https://tellme.pw/avsox"
    response, error = await config.async_client.get_text(issue_url)
    domain = "https://avsox.click"
    if response is not None:
        res = re.findall(r'(https://[^"]+)', response)
        for s in res:
            if s and "https://avsox.com" not in s or "api.qrserver.com" not in s:
                return s
    return domain


async def get_amazon_data(req_url: str) -> Tuple[bool, str]:
    """
    è·å– Amazon æ•°æ®
    """
    headers = {
        "accept-encoding": "gzip, deflate, br",
        "Host": "www.amazon.co.jp",
        "User-Agent": get_user_agent(),
    }
    html_info, error = await config.async_client.get_text(req_url, encoding="Shift_JIS")
    if html_info is None:
        html_info, error = await config.async_client.get_text(req_url, headers=headers, encoding="Shift_JIS")
    if html_info is None:
        session_id = ""
        ubid_acbjp = ""
        if x := re.findall(r'sessionId: "([^"]+)', html_info or ""):
            session_id = x[0]
        if x := re.findall(r"ubid-acbjp=([^ ]+)", html_info or ""):
            ubid_acbjp = x[0]
        headers_o = {
            "cookie": f"session-id={session_id}; ubid_acbjp={ubid_acbjp}",
        }
        headers.update(headers_o)
        html_info, error = await config.async_client.get_text(req_url, headers=headers, encoding="Shift_JIS")
    if html_info is None:
        return False, error
    if "HTTP 503" in html_info:
        headers = {
            "Host": "www.amazon.co.jp",
            "User-Agent": get_user_agent(),
        }
        html_info, error = await config.async_client.get_text(req_url, headers=headers, encoding="Shift_JIS")
    if html_info is None:
        return False, error
    return True, html_info


async def get_imgsize(url) -> tuple[int, int]:
    response, err = await config.async_client.request("GET", url, stream=True)
    if response is None or response.status_code != 200:
        return 0, 0
    file_head = BytesIO()
    chunk_size = 1024 * 10
    for chunk in response.iter_content(chunk_size):
        file_head.write(chunk)
        response.close()
        try:

            def _get_size():
                return Image.open(file_head).size

            await asyncio.to_thread(_get_size)
        except Exception:
            return 0, 0
    return 0, 0


async def get_dmm_trailer(trailer_url):
    """
    å¼‚æ­¥ç‰ˆæœ¬çš„ get_dmm_trailer å‡½æ•°
    å¦‚æœé¢„è§ˆç‰‡åœ°å€ä¸º dmm ï¼Œå°è¯•è·å– dmm é¢„è§ˆç‰‡æœ€é«˜åˆ†è¾¨ç‡

    Args:
        trailer_url (str): é¢„è§ˆç‰‡åœ°å€

    Returns:
        str: æœ€é«˜åˆ†è¾¨ç‡çš„é¢„è§ˆç‰‡åœ°å€
    """
    # å¦‚æœä¸æ˜¯DMMåŸŸåæˆ–å·²ç»æ˜¯æœ€é«˜åˆ†è¾¨ç‡ï¼Œåˆ™ç›´æ¥è¿”å›
    if ".dmm.co" not in trailer_url or "_mhb_w.mp4" in trailer_url:
        return trailer_url

    # å°†ç›¸å¯¹URLè½¬æ¢ä¸ºç»å¯¹URL
    if trailer_url.startswith("//"):
        trailer_url = "https:" + trailer_url

    """
    DMMé¢„è§ˆç‰‡åˆ†è¾¨ç‡å¯¹åº”å…³ç³»:
    '_sm_w.mp4': 320*180, 3.8MB     # æœ€ä½åˆ†è¾¨ç‡
    '_dm_w.mp4': 560*316, 10.1MB    # ä¸­ç­‰åˆ†è¾¨ç‡
    '_dmb_w.mp4': 720*404, 14.6MB   # æ¬¡é«˜åˆ†è¾¨ç‡
    '_mhb_w.mp4': 720*404, 27.9MB   # æœ€é«˜åˆ†è¾¨ç‡
    
    ç¤ºä¾‹:
    https://cc3001.dmm.co.jp/litevideo/freepv/s/ssi/ssis00090/ssis00090_sm_w.mp4
    https://cc3001.dmm.co.jp/litevideo/freepv/s/ssi/ssis00090/ssis00090_dm_w.mp4
    https://cc3001.dmm.co.jp/litevideo/freepv/s/ssi/ssis00090/ssis00090_dmb_w.mp4
    https://cc3001.dmm.co.jp/litevideo/freepv/s/ssi/ssis00090/ssis00090_mhb_w.mp4
    """

    # è§£æURLè·å–åŸºç¡€éƒ¨åˆ†å’Œå½“å‰åˆ†è¾¨ç‡æ ‡è¯†
    pattern = r"(.+)(_[sd]mb?_w.mp4)"
    match = re.findall(pattern, trailer_url)
    if not match:
        return trailer_url

    # è§£æURLåŸºç¡€éƒ¨åˆ†å’Œåˆ†è¾¨ç‡æ ‡è¯†
    base_url, resolution_tag = match[0]

    # æ„å»ºå„ç§åˆ†è¾¨ç‡çš„URL
    resolutions = {
        "_mhb_w.mp4": base_url + "_mhb_w.mp4",  # æœ€é«˜åˆ†è¾¨ç‡
        "_dmb_w.mp4": base_url + "_dmb_w.mp4",  # æ¬¡é«˜åˆ†è¾¨ç‡
        "_dm_w.mp4": base_url + "_dm_w.mp4",  # ä¸­ç­‰åˆ†è¾¨ç‡
    }

    # æ ¹æ®å½“å‰åˆ†è¾¨ç‡é€‰æ‹©æ£€æŸ¥ç­–ç•¥
    check_list = []
    if resolution_tag == "_dmb_w.mp4":
        # å·²ç»æ˜¯æ¬¡é«˜åˆ†è¾¨ç‡ï¼Œåªéœ€æ£€æŸ¥æœ€é«˜åˆ†è¾¨ç‡
        check_list = ["_mhb_w.mp4"]
    elif resolution_tag == "_dm_w.mp4":
        # ä¸­ç­‰åˆ†è¾¨ç‡ï¼ŒæŒ‰ä¼˜å…ˆçº§æ£€æŸ¥æœ€é«˜å’Œæ¬¡é«˜åˆ†è¾¨ç‡
        check_list = ["_mhb_w.mp4", "_dmb_w.mp4"]
    elif resolution_tag == "_sm_w.mp4":
        # æœ€ä½åˆ†è¾¨ç‡ï¼ŒæŒ‰ä¼˜å…ˆçº§æ£€æŸ¥æ‰€æœ‰æ›´é«˜åˆ†è¾¨ç‡
        check_list = ["_mhb_w.mp4", "_dmb_w.mp4", "_dm_w.mp4"]

    # æŒ‰ä¼˜å…ˆçº§æ£€æŸ¥æ›´é«˜åˆ†è¾¨ç‡
    for res_key in check_list:
        if await check_url(resolutions[res_key]):
            return resolutions[res_key]

    # å¦‚æœæ‰€æœ‰æ£€æŸ¥éƒ½å¤±è´¥ï¼Œåˆ™è¿”å›åŸå§‹URL
    return trailer_url


def _ping_host_thread(host_address: str, result_list: List[Optional[int]], i: int) -> None:
    response = ping(host_address, timeout=1)
    result_list[i] = int(response * 1000) if response else 0


def ping_host(host_address: str) -> str:
    count = config.retry
    result_list: List[Optional[int]] = [None] * count
    thread_list: List[threading.Thread] = [None] * count  # type: ignore # todo
    for i in range(count):
        thread_list[i] = threading.Thread(target=_ping_host_thread, args=(host_address, result_list, i))
        thread_list[i].start()
    for i in range(count):
        thread_list[i].join()
    new_list = [each for each in result_list if each]
    return (
        f"  â± Ping {int(sum(new_list) / len(new_list))} ms ({len(new_list)}/{count})"
        if new_list
        else f"  ğŸ”´ Ping - ms (0/{count})"
    )


def check_version() -> Optional[int]:
    if config.update_check:
        url = "https://api.github.com/repos/sqzw-x/mdcx/releases/latest"
        res_json, error = get_json_sync(url)
        if res_json is not None:
            try:
                latest_version = res_json["tag_name"]
                latest_version = int(latest_version)
                return latest_version
            except Exception:
                signal.add_log(f"âŒ è·å–æœ€æ–°ç‰ˆæœ¬å¤±è´¥ï¼{res_json}")
    return None


def check_theporndb_api_token() -> str:
    tips = "âœ… è¿æ¥æ­£å¸¸! "
    headers = config.headers
    proxies = config.proxies
    timeout = config.timeout
    api_token = config.theporndb_api_token
    url = "https://api.theporndb.net/scenes/hash/8679fcbdd29fa735"
    headers = {
        "Authorization": f"Bearer {api_token}",
        "Content-Type": "application/json",
        "Accept": "application/json",
        "User-Agent": get_user_agent(),
    }
    if not api_token:
        tips = "âŒ æœªå¡«å†™ API Tokenï¼Œå½±å“æ¬§ç¾åˆ®å‰Šï¼å¯åœ¨ã€Œè®¾ç½®ã€-ã€Œç½‘ç»œã€æ·»åŠ ï¼"
    else:
        try:
            response = requests.get(url, headers=headers, proxies=proxies, timeout=timeout, verify=False)
            if response.status_code == 401 and "Unauthenticated" in str(response.text):
                tips = "âŒ API Token é”™è¯¯ï¼å½±å“æ¬§ç¾åˆ®å‰Šï¼è¯·åˆ°ã€Œè®¾ç½®ã€-ã€Œç½‘ç»œã€ä¸­ä¿®æ”¹ã€‚"
            elif response.status_code == 200:
                if response.json().get("data"):
                    tips = "âœ… è¿æ¥æ­£å¸¸ï¼"
                else:
                    tips = "âŒ è¿”å›æ•°æ®å¼‚å¸¸ï¼"
            else:
                tips = f"âŒ è¿æ¥å¤±è´¥ï¼è¯·æ£€æŸ¥ç½‘ç»œæˆ–ä»£ç†è®¾ç½®ï¼ {response.status_code} {response.text}"
        except Exception as e:
            tips = f"âŒ è¿æ¥å¤±è´¥!è¯·æ£€æŸ¥ç½‘ç»œæˆ–ä»£ç†è®¾ç½®ï¼ {e}"
    signal.show_log_text(tips.replace("âŒ", " âŒ ThePornDB").replace("âœ…", " âœ… ThePornDB"))
    return tips


async def _get_pic_by_google(pic_url):
    google_keyused = config.google_keyused
    google_keyword = config.google_keyword
    req_url = f"https://www.google.com/searchbyimage?sbisrc=2&image_url={pic_url}"
    # req_url = f'https://lens.google.com/uploadbyurl?url={pic_url}&hl=zh-CN&re=df&ep=gisbubu'
    response, error = await config.async_client.get_text(req_url)
    big_pic = True
    if response is None:
        return "", "", ""
    url_list = re.findall(r'a href="([^"]+isz:l[^"]+)">', response)
    url_list_middle = re.findall(r'a href="([^"]+isz:m[^"]+)">', response)
    if not url_list and url_list_middle:
        url_list = url_list_middle
        big_pic = False
    if url_list:
        req_url = "https://www.google.com" + url_list[0].replace("amp;", "")
        response, error = await config.async_client.get_text(req_url)
    if response is None:
        return "", "", ""
    url_list = re.findall(r'\["(http[^"]+)",(\d{3,4}),(\d{3,4})\],[^[]', response)
    # ä¼˜å…ˆä¸‹è½½æ”¾å‰é¢
    new_url_list = []
    for each_url in url_list.copy():
        if int(each_url[2]) < 800:
            url_list.remove(each_url)

    for each_key in google_keyused:
        for each_url in url_list.copy():
            if each_key in each_url[0]:
                new_url_list.append(each_url)
                url_list.remove(each_url)
    # åªä¸‹è½½å…³æ—¶ï¼Œè¿½åŠ å‰©ä½™åœ°å€
    if "goo_only" not in config.download_hd_pics:
        new_url_list += url_list
    # è§£æåœ°å€
    for each in new_url_list:
        temp_url = each[0]
        for temp_keyword in google_keyword:
            if temp_keyword in temp_url:
                break
        else:
            h = int(each[1])
            w = int(each[2])
            if w > h and w / h < 1.4:  # thumb è¢«æ‹‰é«˜æ—¶è·³è¿‡
                continue

            p_url = temp_url.encode("utf-8").decode("unicode_escape")  # urlä¸­çš„Unicodeå­—ç¬¦è½¬ä¹‰ï¼Œä¸è½¬ä¹‰ï¼Œurlè¯·æ±‚ä¼šå¤±è´¥
            if "m.media-amazon.com" in p_url:
                p_url = re.sub(r"\._[_]?AC_[^\.]+\.", ".", p_url)
                pic_size = await get_imgsize(p_url)
                if pic_size[0]:
                    return p_url, pic_size, big_pic
            else:
                url = await check_url(p_url)
                if url:
                    pic_size = (w, h)
                    return url, pic_size, big_pic
    return "", "", ""


async def get_big_pic_by_google(pic_url, poster=False):
    url, pic_size, big_pic = await _get_pic_by_google(pic_url)
    if not poster:
        if big_pic or (
            pic_size and int(pic_size[0]) > 800 and int(pic_size[1]) > 539
        ):  # cover æœ‰å¤§å›¾æ—¶æˆ–è€…å›¾ç‰‡é«˜åº¦ > 800 æ—¶ä½¿ç”¨è¯¥å›¾ç‰‡
            return url, pic_size
        return "", ""
    if url and int(pic_size[1]) < 1000:  # posterï¼Œå›¾ç‰‡é«˜åº¦å°äº 1500ï¼Œé‡æ–°æœç´¢ä¸€æ¬¡
        url, pic_size, big_pic = await _get_pic_by_google(url)
    if pic_size and (
        big_pic or "blogger.googleusercontent.com" in url or int(pic_size[1]) > 560
    ):  # posterï¼Œå¤§å›¾æˆ–é«˜åº¦ > 560 æ—¶ï¼Œä½¿ç”¨è¯¥å›¾ç‰‡
        return url, pic_size
    else:
        return "", ""
