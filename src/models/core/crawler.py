"""
çˆ¬è™«æ§åˆ¶, è°ƒç”¨ models.crawlers ä¸­å„ä¸ªç½‘ç«™çˆ¬è™«
"""

import json
import re
from typing import Any, Callable

import langid

from models.base.number import get_number_letters, is_uncensored
from models.config.config import config
from models.core.flags import Flags
from models.core.json_data import JsonData, LogBuffer
from models.crawlers import (
    airav,
    airav_cc,
    avsex,
    avsox,
    cableav,
    cnmdb,
    dahlia,
    dmm,
    faleno,
    fantastica,
    fc2,
    fc2club,
    fc2hub,
    fc2ppvdb,
    freejavbt,
    getchu,
    getchu_dmm,
    giga,
    hdouban,
    hscangku,
    iqqtv_new,
    jav321,
    javbus,
    javday,
    javdb,
    javlibrary_new,
    kin8,
    love6,
    lulubar,
    madouqu,
    mdtv,
    mgstage,
    mmtv,
    mywife,
    official,
    prestige,
    theporndb,
    xcity,
)
from models.entity.enums import FileMode


def _get_new_website_list(
    field_website_list: list[str],
    number_website_list: list[str],
    file_number: str,
    short_number: str,
    field: str,
    all: bool = False,
) -> list[str]:
    whole_fields = config.whole_fields  # ç»§ç»­è¡¥å…¨çš„å­—æ®µ
    field_website_list = [i for i in field_website_list if i.strip()]  # å»ç©º
    number_website_list = [i for i in number_website_list if i.strip()]  # å»ç©º
    same_list = [i for i in field_website_list if i in number_website_list]  # å–äº¤é›†
    if (
        field in whole_fields or field == "title" or all
    ):  # å–å‰©ä½™æœªç›¸äº¤ç½‘ç«™ï¼Œ trailer ä¸å–æœªç›¸äº¤ç½‘ç«™ï¼Œtitle é»˜è®¤å–æœªç›¸äº¤ç½‘ç«™
        if field != "trailer":
            diff_list = [i for i in number_website_list if i not in field_website_list]
            same_list.extend(diff_list)
    dic_escape = {
        "title": config.title_website_exclude.split(","),
        "outline": config.outline_website_exclude.split(","),
        "actor": config.actor_website_exclude.split(","),
        "thumb": config.thumb_website_exclude.split(","),
        "poster": config.poster_website_exclude.split(","),
        "extrafanart": config.extrafanart_website_exclude.split(","),
        "trailer": config.trailer_website_exclude.split(","),
        "tag": config.tag_website_exclude.split(","),
        "release": config.release_website_exclude.split(","),
        "runtime": config.runtime_website_exclude.split(","),
        "score": config.score_website_exclude.split(","),
        "director": config.director_website_exclude.split(","),
        "series": config.series_website_exclude.split(","),
        "studio": config.studio_website_exclude.split(","),
        "publisher": config.publisher_website_exclude.split(","),
    }  # æ ¹æ®å­—æ®µæ’é™¤çš„ç½‘ç«™

    escape_list = dic_escape.get(field)
    if escape_list:
        same_list = [i for i in same_list if i not in escape_list]  # æ ¹æ®å­—æ®µæ’é™¤ä¸€äº›ä¸å«è¿™äº›å­—æ®µçš„ç½‘ç«™

    # mgstage ç´ äººç•ªå·æ£€æŸ¥
    if short_number:
        not_frist_field_list = ["title", "actor"]  # è¿™äº›å­—æ®µä»¥å¤–ï¼Œç´ äººæŠŠ mgstage æ”¾åœ¨ç¬¬ä¸€ä½
        if field not in not_frist_field_list and "mgstage" in same_list:
            same_list.remove("mgstage")
            same_list.insert(0, "mgstage")

    # faleno.jp ç•ªå·æ£€æŸ¥ dldss177 dhla009
    elif re.findall(r"F[A-Z]{2}SS", file_number):
        same_list = _deal_some_list(field, "faleno", same_list)

    # dahlia-av.jp ç•ªå·æ£€æŸ¥
    elif file_number.startswith("DLDSS") or file_number.startswith("DHLA"):
        same_list = _deal_some_list(field, "dahlia", same_list)

    # fantastica ç•ªå·æ£€æŸ¥ FAVIã€FAAPã€FAPLã€FAKGã€FAHOã€FAVAã€FAKYã€FAMIã€FAITã€FAKAã€FAMOã€FASOã€FAIHã€FASHã€FAKSã€FAAN
    elif (
        re.search(r"FA[A-Z]{2}-?\d+", file_number.upper())
        or file_number.upper().startswith("CLASS")
        or file_number.upper().startswith("FADRV")
        or file_number.upper().startswith("FAPRO")
        or file_number.upper().startswith("FAKWM")
        or file_number.upper().startswith("PDS")
    ):
        same_list = _deal_some_list(field, "fantastica", same_list)

    return same_list


def _deal_some_list(field: str, website: str, same_list: list[str]) -> list[str]:
    if website not in same_list:
        same_list.append(website)
    if field in ["title", "outline", "thumb", "poster", "trailer", "extrafanart"]:
        same_list.remove(website)
        same_list.insert(0, website)
    elif field in ["tag", "score", "director", "series"]:
        same_list.remove(website)
    return same_list


def _call_crawler(
    json_data: JsonData,
    website: str,
    language: str,
    file_number: str,
    short_number: str,
    mosaic: str,
    org_language: str,
) -> dict[str, Any]:
    """
    è·å–æŸä¸ªç½‘ç«™æ•°æ®
    """
    appoint_number = json_data["appoint_number"]
    appoint_url = json_data["appoint_url"]
    file_path = json_data["file_path"]

    # 259LUXU-1111ï¼Œ mgstage å’Œ avsex ä¹‹å¤–ä½¿ç”¨ LUXU-1111ï¼ˆç´ äººç•ªå·æ—¶ï¼Œshort_numberæœ‰å€¼ï¼Œä¸å¸¦å‰ç¼€æ•°å­—ï¼›åä¹‹ï¼Œshort_numberä¸ºç©º)
    if short_number and website != "mgstage" and website != "avsex":
        file_number = short_number
    _: dict[str, Callable] = {
        "official": official.main,
        "iqqtv": iqqtv_new.main,
        "avsex": avsex.main,
        "airav_cc": airav_cc.main,
        "airav": airav.main,
        "freejavbt": freejavbt.main,
        "javbus": javbus.main,
        "javdb": javdb.main,
        "jav321": jav321.main,
        "dmm": dmm.main,
        "javlibrary": javlibrary_new.main,
        "xcity": xcity.main,
        "avsox": avsox.main,
        "mgstage": mgstage.main,
        "7mmtv": mmtv.main,
        "fc2": fc2.main,
        "fc2hub": fc2hub.main,
        "fc2club": fc2club.main,
        "fc2ppvdb": fc2ppvdb.main,
        "mdtv": mdtv.main,
        "madouqu": madouqu.main,
        "hscangku": hscangku.main,
        "cableav": cableav.main,
        "getchu": getchu.main,
        "getchu_dmm": getchu_dmm.main,
        "mywife": mywife.main,
        "giga": giga.main,
        "hdouban": hdouban.main,
        "lulubar": lulubar.main,
        "love6": love6.main,
        "cnmdb": cnmdb.main,
        "faleno": faleno.main,
        "fantastica": fantastica.main,
        "theporndb": theporndb.main,
        "dahlia": dahlia.main,
        "prestige": prestige.main,
        "kin8": kin8.main,
        "javday": javday.main,
    }
    if website == "official":
        res = json.loads(official.main(file_number, appoint_url, language))
    elif website == "iqqtv":
        res = json.loads(iqqtv_new.main(file_number, appoint_url, language))
    elif website == "avsex":
        res = json.loads(avsex.main(file_number, appoint_url, language))
    elif website == "airav_cc":
        res = json.loads(airav_cc.main(file_number, appoint_url, language))
    elif website == "airav":
        res = json.loads(airav.main(file_number, appoint_url, language))
    elif website == "freejavbt":
        res = json.loads(freejavbt.main(file_number, appoint_url, language))
    elif website == "javbus":
        res = json.loads(javbus.main(file_number, appoint_url, language, mosaic))
    elif website == "javdb":
        res = json.loads(javdb.main(file_number, appoint_url, language, org_language))
    elif website == "jav321":
        res = json.loads(jav321.main(file_number, appoint_url, language))
    elif website == "dmm":
        res = json.loads(dmm.main(file_number, appoint_url, language, file_path))
    elif website == "javlibrary":
        res = json.loads(javlibrary_new.main(file_number, appoint_url, language))
    elif website == "xcity":
        res = json.loads(xcity.main(file_number, appoint_url, language))
    elif website == "avsox":
        res = json.loads(avsox.main(file_number, appoint_url, language))
    elif website == "mgstage":
        res = json.loads(mgstage.main(file_number, appoint_url, language, short_number))
    elif website == "7mmtv":
        res = json.loads(mmtv.main(file_number, appoint_url, language, file_path))
    elif website == "fc2":
        res = json.loads(fc2.main(file_number, appoint_url, language))
    elif website == "fc2hub":
        res = json.loads(fc2hub.main(file_number, appoint_url, language))
    elif website == "fc2club":
        res = json.loads(fc2club.main(file_number, appoint_url, language))
    elif website == "fc2ppvdb":
        res = json.loads(fc2ppvdb.main(file_number, appoint_url, language))
    elif website == "mdtv":
        res = json.loads(mdtv.main(file_number, appoint_url, language, file_path, appoint_number))
    elif website == "madouqu":
        res = json.loads(madouqu.main(file_number, appoint_url, language, file_path, appoint_number))
    elif website == "hscangku":
        res = json.loads(hscangku.main(file_number, appoint_url, language, file_path, appoint_number))
    elif website == "cableav":
        res = json.loads(cableav.main(file_number, appoint_url, language, file_path, appoint_number))
    elif website == "getchu":
        res = json.loads(getchu.main(file_number, appoint_url, language))
    elif website == "getchu_dmm":
        res = json.loads(getchu_dmm.main(file_number, appoint_url, language))
    elif website == "mywife":
        res = json.loads(mywife.main(file_number, appoint_url, language))
    elif website == "giga":
        res = json.loads(giga.main(file_number, appoint_url, language))
    elif website == "hdouban":
        res = json.loads(hdouban.main(file_number, appoint_url, language, file_path, appoint_number, mosaic))
    elif website == "lulubar":
        res = json.loads(lulubar.main(file_number, appoint_url, language))
    elif website == "love6":
        res = json.loads(love6.main(file_number, appoint_url, language))
    elif website == "cnmdb":
        res = json.loads(cnmdb.main(file_number, appoint_url, language, file_path, appoint_number))
    elif website == "faleno":
        res = json.loads(faleno.main(file_number, appoint_url, language))
    elif website == "fantastica":
        res = json.loads(fantastica.main(file_number, appoint_url, language))
    elif website == "theporndb":
        res = json.loads(theporndb.main(file_number, appoint_url, language, file_path))
    elif website == "dahlia":
        res = json.loads(dahlia.main(file_number, appoint_url, language))
    elif website == "prestige":
        res = json.loads(prestige.main(file_number, appoint_url, language))
    elif website == "kin8":
        res = json.loads(kin8.main(file_number, appoint_url, language))
    elif website == "javday":
        res = json.loads(javday.main(file_number, appoint_url, language))
    else:
        res = json.loads(javdb.main(file_number, appoint_url, language))

    return res


def _decide_websites(
    json_data: JsonData,
    number_website_list: list[str],
) -> JsonData:
    """
    è·å–ä¸€ç»„ç½‘ç«™çš„æ•°æ®ï¼šæŒ‰ç…§è®¾ç½®çš„ç½‘ç«™ç»„ï¼Œè¯·æ±‚å„å­—æ®µæ•°æ®ï¼Œå¹¶è¿”å›æœ€ç»ˆçš„æ•°æ®
    """
    file_number = json_data["number"]
    short_number = json_data["short_number"]
    scrape_like = config.scrape_like
    none_fields = config.none_fields  # ä¸åˆ®å‰Šçš„å­—æ®µ

    # è·å–ä½¿ç”¨çš„ç½‘ç«™
    title_jp_website_list = config.title_website.split(",")
    title_zh_website_list = config.title_zh_website.split(",")
    outline_jp_website_list = config.outline_website.split(",")
    outline_zh_website_list = config.outline_zh_website.split(",")
    actor_website_list = config.actor_website.split(",")
    thumb_website_list = config.thumb_website.split(",")
    poster_website_list = config.poster_website.split(",")
    extrafanart_website_list = config.extrafanart_website.split(",")
    trailer_website_list = config.trailer_website.split(",")
    tag_website_list = config.tag_website.split(",")
    release_website_list = config.release_website.split(",")
    runtime_website_list = config.runtime_website.split(",")
    score_website_list = config.score_website.split(",")
    director_website_list = config.director_website.split(",")
    series_website_list = config.series_website.split(",")
    studio_website_list = config.studio_website.split(",")
    publisher_website_list = config.publisher_website.split(",")
    wanted_website_list = config.wanted_website.split(",")
    title_jp_website_new_list = _get_new_website_list(
        title_jp_website_list, number_website_list, file_number, short_number, "title"
    )
    title_zh_website_new_list = _get_new_website_list(
        title_zh_website_list, number_website_list, file_number, short_number, "title_zh"
    )
    outline_jp_website_new_list = _get_new_website_list(
        outline_jp_website_list, number_website_list, file_number, short_number, "outline"
    )
    outline_zh_website_new_list = _get_new_website_list(
        outline_zh_website_list, number_website_list, file_number, short_number, "outline_zh"
    )
    actor_website_new_list = _get_new_website_list(
        actor_website_list, number_website_list, file_number, short_number, "actor"
    )
    thumb_website_new_list = _get_new_website_list(
        thumb_website_list, number_website_list, file_number, short_number, "thumb"
    )
    poster_website_new_list = _get_new_website_list(
        poster_website_list, number_website_list, file_number, short_number, "poster"
    )
    extrafanart_website_new_list = _get_new_website_list(
        extrafanart_website_list, number_website_list, file_number, short_number, "extrafanart"
    )
    trailer_website_new_list = _get_new_website_list(
        trailer_website_list, number_website_list, file_number, short_number, "trailer"
    )
    tag_website_new_list = _get_new_website_list(
        tag_website_list, number_website_list, file_number, short_number, "tag"
    )
    release_website_new_list = _get_new_website_list(
        release_website_list, number_website_list, file_number, short_number, "release"
    )
    runtime_website_new_list = _get_new_website_list(
        runtime_website_list, number_website_list, file_number, short_number, "runtime"
    )
    score_website_new_list = _get_new_website_list(
        score_website_list, number_website_list, file_number, short_number, "score"
    )
    director_website_new_list = _get_new_website_list(
        director_website_list, number_website_list, file_number, short_number, "director"
    )
    series_website_new_list = _get_new_website_list(
        series_website_list, number_website_list, file_number, short_number, "series"
    )
    studio_website_new_list = _get_new_website_list(
        studio_website_list, number_website_list, file_number, short_number, "studio"
    )
    publisher_website_new_list = _get_new_website_list(
        publisher_website_list, number_website_list, file_number, short_number, "publisher"
    )
    wanted_website_new_list = _get_new_website_list(
        wanted_website_list, number_website_list, file_number, short_number, "wanted"
    )

    # åˆå§‹åŒ–å˜é‡
    all_json_data = {}

    # ç”Ÿæˆå„å­—æ®µåŠè¯·æ±‚ç½‘ç«™åˆ—è¡¨ï¼Œå¹¶è¯·æ±‚æ•°æ®
    if scrape_like == "speed":
        request_field_list = [("title", "æ ‡é¢˜", "title_language", number_website_list)]
    else:
        if "official" in config.website_set:
            title_jp_website_new_list.insert(0, "official")
        request_field_list = [
            ("title", "æ ‡é¢˜", "title_language", title_jp_website_new_list),
            ("title_zh", "ä¸­æ–‡æ ‡é¢˜", "title_language", title_zh_website_new_list),
            ("outline", "ç®€ä»‹", "outline_language", outline_jp_website_new_list),
            ("outline_zh", "ä¸­æ–‡ç®€ä»‹", "outline_language", outline_zh_website_new_list),
            ("actor", "æ¼”å‘˜", "actor_language", actor_website_new_list),
            ("cover", "èƒŒæ™¯å›¾", "title_language", thumb_website_new_list),
            ("poster", "å°é¢å›¾", "title_language", poster_website_new_list),
            ("extrafanart", "å‰§ç…§", "title_language", extrafanart_website_new_list),
            ("tag", "æ ‡ç­¾", "tag_language", tag_website_new_list),
            ("release", "å‘è¡Œæ—¥æœŸ", "title_language", release_website_new_list),
            ("runtime", "æ—¶é•¿", "title_language", runtime_website_new_list),
            ("score", "è¯„åˆ†", "title_language", score_website_new_list),
            ("director", "å¯¼æ¼”", "director_language", director_website_new_list),
            ("series", "ç³»åˆ—", "series_language", series_website_new_list),
            ("studio", "ç‰‡å•†", "studio_language", studio_website_new_list),
            ("publisher", "å‘è¡Œå•†", "publisher_language", publisher_website_new_list),
            ("trailer", "é¢„å‘Šç‰‡", "title_language", trailer_website_new_list),
            ("wanted", "æƒ³çœ‹äººæ•°", "title_language", wanted_website_new_list),
        ]
        if config.outline_language == "jp":
            request_field_list.pop(3)
        if config.title_language == "jp":
            request_field_list.pop(1)
        if not wanted_website_new_list:
            request_field_list.pop()

    for field_name, field_cnname, field_language, website_list in request_field_list:
        if field_name in none_fields:
            continue
        _call_crawlers(
            all_json_data,
            json_data,
            website_list,
            field_name,
            field_cnname,
            field_language,
            config,
            file_number,
            short_number,
            json_data["mosaic"],
        )
        if field_name == "title" and not json_data["title"]:
            return json_data

    # å¤„ç†å­—æ®µå­—æ®µï¼šä»å·²è¯·æ±‚çš„ç½‘ç«™ä¸­ï¼ŒæŒ‰å­—æ®µç½‘ç«™ä¼˜å…ˆçº§å–å€¼
    title_website_list = title_jp_website_list
    outline_website_list = outline_jp_website_list
    number_website_list = [i for i in number_website_list if i in all_json_data.keys()]
    new_number_website_list = number_website_list
    if "official" in all_json_data.keys() and all_json_data["official"]["jp"]["title"]:
        official_website_name = all_json_data["official"]["jp"]["source"]
        new_number_website_list = [official_website_name] + number_website_list
        title_jp_website_list = [official_website_name] + title_jp_website_list
        outline_jp_website_list = [official_website_name] + outline_jp_website_list
    if config.title_language != "jp":
        title_website_list = title_zh_website_list + title_jp_website_list
    if config.outline_language != "jp":
        outline_website_list = outline_zh_website_list + outline_jp_website_list
    title_website_new_list = _get_new_website_list(
        title_website_list, new_number_website_list, file_number, short_number, "title", all=True
    )
    title_jp_website_new_list = _get_new_website_list(
        title_jp_website_list, new_number_website_list, file_number, short_number, "title", all=True
    )
    outline_website_new_list = _get_new_website_list(
        outline_website_list, new_number_website_list, file_number, short_number, "outline", all=True
    )
    outline_jp_website_new_list = _get_new_website_list(
        outline_jp_website_list, new_number_website_list, file_number, short_number, "outline", all=True
    )
    actor_website_new_list = _get_new_website_list(
        actor_website_list, number_website_list, file_number, short_number, "actor", all=True
    )
    thumb_website_new_list = _get_new_website_list(
        thumb_website_list, number_website_list, file_number, short_number, "thumb", all=True
    )
    poster_website_new_list = _get_new_website_list(
        poster_website_list, number_website_list, file_number, short_number, "poster", all=True
    )
    extrafanart_website_new_list = _get_new_website_list(
        extrafanart_website_list, number_website_list, file_number, short_number, "extrafanart", all=True
    )
    tag_website_new_list = _get_new_website_list(
        tag_website_list, number_website_list, file_number, short_number, "tag", all=True
    )
    release_website_new_list = _get_new_website_list(
        release_website_list, number_website_list, file_number, short_number, "release", all=True
    )
    runtime_website_new_list = _get_new_website_list(
        runtime_website_list, number_website_list, file_number, short_number, "runtime", all=True
    )
    score_website_new_list = _get_new_website_list(
        score_website_list, number_website_list, file_number, short_number, "score", all=True
    )
    director_website_new_list = _get_new_website_list(
        director_website_list, number_website_list, file_number, short_number, "director", all=True
    )
    series_website_new_list = _get_new_website_list(
        series_website_list, number_website_list, file_number, short_number, "series", all=True
    )
    studio_website_new_list = _get_new_website_list(
        studio_website_list, number_website_list, file_number, short_number, "studio", all=True
    )
    publisher_website_new_list = _get_new_website_list(
        publisher_website_list, number_website_list, file_number, short_number, "publisher", all=True
    )
    trailer_website_new_list = _get_new_website_list(
        trailer_website_list, number_website_list, file_number, short_number, "trailer", all=True
    )
    wanted_website_new_list = _get_new_website_list(
        wanted_website_list, number_website_list, file_number, short_number, "wanted"
    )
    deal_field_list = [
        ("title", "æ ‡é¢˜", "title_language", title_website_new_list),
        ("originaltitle", "åŸæ ‡é¢˜", "outline_language", title_jp_website_new_list),
        ("outline", "ç®€ä»‹", "outline_language", outline_website_new_list),
        ("originalplot", "åŸç®€ä»‹", "outline_language", outline_jp_website_new_list),
        ("actor", "æ¼”å‘˜", "actor_language", actor_website_new_list),
        ("cover", "èƒŒæ™¯å›¾", "title_language", thumb_website_new_list),
        ("poster", "å°é¢å›¾", "title_language", poster_website_new_list),
        ("extrafanart", "å‰§ç…§", "title_language", extrafanart_website_new_list),
        ("tag", "æ ‡ç­¾", "tag_language", tag_website_new_list),
        ("release", "å‘è¡Œæ—¥æœŸ", "title_language", release_website_new_list),
        ("runtime", "æ—¶é•¿", "title_language", runtime_website_new_list),
        ("score", "è¯„åˆ†", "title_language", score_website_new_list),
        ("director", "å¯¼æ¼”", "director_language", director_website_new_list),
        ("series", "ç³»åˆ—", "series_language", series_website_new_list),
        ("studio", "ç‰‡å•†", "studio_language", studio_website_new_list),
        ("publisher", "å‘è¡Œå•†", "publisher_language", publisher_website_new_list),
        ("trailer", "é¢„å‘Šç‰‡", "title_language", trailer_website_new_list),
        ("wanted", "æƒ³çœ‹äººæ•°", "title_language", wanted_website_list),
    ]
    if not wanted_website_new_list or (scrape_like == "speed" and json_data["source"] not in wanted_website_new_list):
        deal_field_list.pop()

    for field_name, field_cnname, field_language, website_list in deal_field_list:
        _deal_each_field(all_json_data, json_data, website_list, field_name, field_cnname, field_language, config)

    # æŠŠå·²åˆ®å‰ŠæˆåŠŸç½‘ç«™çš„ cover url æŒ‰ç…§ cover ç½‘ç«™ä¼˜å…ˆçº§ï¼Œä¿å­˜ä¸ºä¸€ä¸ªåˆ—è¡¨ï¼Œç¬¬ä¸€ä¸ªå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œå¯ä»¥ä½¿ç”¨å…¶ä»–å›¾ç‰‡ä¸‹è½½
    cover_list = []
    for each_website in thumb_website_new_list:
        if each_website in all_json_data.keys() and all_json_data[each_website]["jp"]["title"]:
            temp_url = all_json_data[each_website]["jp"]["cover"]
            if temp_url not in cover_list:
                cover_list.append([each_website, temp_url])
    if not cover_list:
        json_data["cover"] = ""  # GBBH-1041 èƒŒæ™¯å›¾å›¾æŒ‚äº†
    json_data["cover_list"] = cover_list

    # æŠŠå·²åˆ®å‰ŠæˆåŠŸç½‘ç«™çš„ actorï¼Œä¿å­˜ä¸ºä¸€ä¸ªåˆ—è¡¨ï¼Œç”¨äº Amazon æœå›¾ï¼Œå› ä¸ºæœ‰çš„ç½‘ç«™ actor ä¸å¯¹ï¼Œæ¯”å¦‚ MOPP-023 javbusé”™çš„
    actor_amazon_list = []
    actor_amazon_list_cn = []
    actor_amazon_list_tw = []
    actor_new_website = []
    [
        actor_new_website.append(i)
        for i in title_jp_website_new_list + title_website_new_list + actor_website_new_list
        if i not in actor_new_website
    ]
    for each_website in actor_new_website:
        if each_website in all_json_data.keys() and all_json_data[each_website]["jp"]["title"]:
            temp_actor = all_json_data[each_website]["jp"]["actor"]
            if temp_actor:
                actor_amazon_list.extend(temp_actor.split(","))
                if all_json_data[each_website]["zh_cn"]["title"]:
                    actor_amazon_list_cn.extend(all_json_data[each_website]["zh_cn"]["actor"].split(","))
                if all_json_data[each_website]["zh_tw"]["title"]:
                    actor_amazon_list_tw.extend(all_json_data[each_website]["zh_tw"]["actor"].split(","))
    actor_amazon_list = actor_amazon_list + actor_amazon_list_cn + actor_amazon_list_tw
    actor_amazon = []
    [actor_amazon.append(i.strip()) for i in actor_amazon_list if i.strip() and i.strip() not in actor_amazon]
    if "ç´ äºº" in actor_amazon:
        actor_amazon.remove("ç´ äºº")
    json_data["actor_amazon"] = actor_amazon

    # å¤„ç† year
    release = json_data["release"]
    if release and (r := re.search(r"\d{4}", release)):
        json_data["year"] = r.group()

    # å¤„ç† numberï¼šç´ äººå½±ç‰‡æ—¶ä½¿ç”¨æœ‰æ•°å­—å‰ç¼€çš„number
    if short_number:
        json_data["number"] = file_number

    json_data["fields_info"] = f"\n ğŸŒ [website] {LogBuffer.req().get().strip('-> ')}{json_data['fields_info']}"
    if "javdb" in all_json_data and "javdbid" in all_json_data["javdb"]["jp"]:
        json_data["javdbid"] = all_json_data["javdb"]["jp"]["javdbid"]
    else:
        json_data["javdbid"] = ""
    return json_data


def _deal_each_field(
    all_json_data: dict[str, dict[str, Any]],
    json_data: JsonData,
    website_list: list[str],
    field_name: str,
    field_cnname: str,
    field_language: str,
    config: Any,
) -> None:
    """
    æŒ‰ç…§è®¾ç½®çš„ç½‘ç«™é¡ºåºå¤„ç†å­—æ®µ
    """
    if config.scrape_like == "speed":
        website_list = [json_data["source"]]

    elif "official" in config.website_set:
        if all_json_data["official"]["jp"]["title"]:
            if field_name not in ["title", "originaltitle", "outline", "originalplot", "wanted", "score"]:
                website_list.insert(0, all_json_data["official"]["jp"]["source"])

    if not website_list:
        return

    backup_data = ""
    LogBuffer.info().write(
        f"\n\n    ğŸ™‹ğŸ»â€ {field_cnname} \n    ====================================\n    ğŸŒ æ¥æºä¼˜å…ˆçº§ï¼š{' -> '.join(website_list)}"
    )
    backup_website = ""
    title_language = getattr(config, field_language, "jp")
    for website in website_list:
        if website not in ["airav_cc", "iqqtv", "airav", "avsex", "javlibrary", "mdtv", "madouqu", "lulubar"]:
            title_language = "jp"
        elif (
            field_name == "originaltitle"
            or field_name == "originalplot"
            or field_name == "trailer"
            or field_name == "wanted"
        ):
            title_language = "jp"
        try:
            web_data_json = all_json_data[website][title_language]
        except:
            continue

        if web_data_json["title"] and web_data_json[field_name]:
            if not len(backup_data):
                backup_data = web_data_json[field_name]
                backup_website = website

            if config.scrape_like != "speed":
                if field_name in ["title", "outline", "originaltitle", "originalplot"]:
                    if website in ["airav_cc", "iqqtv", "airav", "avsex", "javlibrary", "lulubar"]:
                        if langid.classify(web_data_json[field_name])[0] != "ja":
                            if title_language == "jp":
                                LogBuffer.info().write(f"\n    ğŸ”´ {website} (å¤±è´¥ï¼Œæ£€æµ‹ä¸ºéæ—¥æ–‡ï¼Œè·³è¿‡ï¼)")
                                continue
                        elif title_language != "jp":
                            LogBuffer.info().write(f"\n    ğŸ”´ {website} (å¤±è´¥ï¼Œæ£€æµ‹ä¸ºæ—¥æ–‡ï¼Œè·³è¿‡ï¼)")
                            continue
            if field_name == "poster":
                json_data["poster_from"] = website
                json_data["image_download"] = web_data_json["image_download"]
            elif field_name == "cover":
                json_data["cover_from"] = website
            elif field_name == "extrafanart":
                json_data["extrafanart_from"] = website
            elif field_name == "trailer":
                json_data["trailer_from"] = website
            elif field_name == "outline":
                json_data["outline_from"] = website
            elif field_name == "actor":
                json_data["all_actor"] = (
                    json_data["all_actor"] if json_data.get("all_actor") else web_data_json["actor"]
                )
                json_data["all_actor_photo"] = (
                    json_data["all_actor_photo"] if json_data.get("all_actor_photo") else web_data_json["actor_photo"]
                )
            elif field_name == "originaltitle":
                if web_data_json["actor"]:
                    json_data["amazon_orginaltitle_actor"] = web_data_json["actor"].split(",")[0]
            json_data[field_name] = web_data_json[field_name]
            json_data["fields_info"] += "\n     " + "%-13s" % field_name + f": {website} ({title_language})"
            LogBuffer.info().write(f"\n    ğŸŸ¢ {website} (æˆåŠŸ)\n     â†³ {json_data[field_name]}")
            break
        else:
            LogBuffer.info().write(f"\n    ğŸ”´ {website} (å¤±è´¥)")
    else:
        if len(backup_data):
            json_data[field_name] = backup_data
            json_data["fields_info"] += "\n     " + f"{field_name:<13}" + f": {backup_website} ({title_language})"
            LogBuffer.info().write(f"\n    ğŸŸ¢ {backup_website} (ä½¿ç”¨å¤‡ç”¨æ•°æ®)\n     â†³ {backup_data}")
        else:
            json_data["fields_info"] += "\n     " + f"{field_name:<13}" + f": {'-----'} ({'not found'})"


def _call_crawlers(
    all_json_data: dict[str, dict[str, Any]],
    json_data: JsonData,
    website_list: list[str],
    field_name: str,
    field_cnname: str,
    field_language: str,
    config: Any,
    file_number: str,
    short_number: str,
    mosaic: str,
) -> None:  # 4
    """
    æŒ‰ç…§è®¾ç½®çš„ç½‘ç«™é¡ºåºè·å–å„ä¸ªå­—æ®µä¿¡æ¯
    """
    if "official" in config.website_set:
        if field_name not in ["title", "title_zh", "outline_zh", "wanted", "score"]:
            website_list.insert(0, "official")

    backup_jsondata = {}
    backup_website = ""
    for website in website_list:
        if (website in ["avsox", "mdtv"] and mosaic in ["æœ‰ç ", "æ— ç ç ´è§£", "æµå‡º", "é‡Œç•ª", "åŠ¨æ¼«"]) or (
            website == "mdtv" and mosaic == "æ— ç "
        ):
            if field_name != "title":
                continue
        if field_name in ["title_zh", "outline_zh"]:
            title_language = "zh_cn"
            field_name = field_name.replace("_zh", "")
        elif field_name in ["originaltitle", "originalplot", "trailer", "wanted"]:
            title_language = "jp"
        elif website not in ["airav_cc", "iqqtv", "airav", "avsex", "javlibrary", "mdtv", "madouqu", "lulubar"]:
            title_language = "jp"
        else:
            title_language = getattr(config, field_language)

        try:
            web_data_json = all_json_data[website][title_language]
        except:
            web_data = _call_crawler(
                json_data, website, title_language, file_number, short_number, mosaic, config.title_language
            )
            all_json_data.update(web_data)
            web_data_json: dict = all_json_data.get(website, {}).get(title_language, {})

        if field_cnname == "æ ‡é¢˜":
            json_data.update(web_data_json)
        if web_data_json["title"] and web_data_json[field_name]:
            if not len(backup_jsondata):
                backup_jsondata.update(web_data_json)
                backup_website = website
            if field_cnname == "æ ‡é¢˜":
                json_data["outline_from"] = website
                json_data["poster_from"] = website
                json_data["cover_from"] = website
                json_data["extrafanart_from"] = website
                json_data["trailer_from"] = website
            if config.scrape_like != "speed":
                if website in ["airav_cc", "iqqtv", "airav", "avsex", "javlibrary", "lulubar"]:
                    if field_name in ["title", "outline", "originaltitle", "originalplot"]:
                        if langid.classify(web_data_json[field_name])[0] != "ja":
                            if title_language == "jp":
                                LogBuffer.info().write(
                                    f"\n    ğŸ”´ {field_cnname} æ£€æµ‹ä¸ºéæ—¥æ–‡ï¼Œè·³è¿‡ï¼({website})\n     â†³ {web_data_json[field_name]}"
                                )
                                continue
                        elif title_language != "jp":
                            LogBuffer.info().write(
                                f"\n    ğŸ”´ {field_cnname} æ£€æµ‹ä¸ºæ—¥æ–‡ï¼Œè·³è¿‡ï¼({website})\n     â†³ {web_data_json[field_name]}"
                            )
                            continue
                elif website == "official":
                    website = all_json_data["official"]["jp"]["source"]
            LogBuffer.info().write(
                f"\n    ğŸŸ¢ {field_cnname} è·å–æˆåŠŸï¼({website})\n     â†³ {web_data_json[field_name]} "
            )
            break
    else:
        if len(backup_jsondata):
            LogBuffer.info().write(
                f"\n    ğŸŸ¢ {field_cnname} ä½¿ç”¨å¤‡ç”¨æ•°æ®ï¼({backup_website})\n     â†³ {backup_jsondata[field_name]} "
            )
            if field_cnname == "æ ‡é¢˜":
                json_data.update(backup_jsondata)
        else:
            LogBuffer.info().write(f"\n    ğŸ”´ {field_cnname} è·å–å¤±è´¥ï¼")


def _call_specific_crawler(json_data: JsonData, website: str) -> JsonData:
    file_number = json_data["number"]
    short_number = json_data["short_number"]
    mosaic = json_data["mosaic"]
    json_data["fields_info"] = ""

    title_language = config.title_language
    org_language = title_language
    outline_language = config.outline_language
    actor_language = config.actor_language
    tag_language = config.tag_language
    series_language = config.series_language
    studio_language = config.studio_language
    publisher_language = config.publisher_language
    director_language = config.director_language
    if website not in ["airav_cc", "iqqtv", "airav", "avsex", "javlibrary", "mdtv", "madouqu", "lulubar"]:
        title_language = "jp"
        outline_language = "jp"
        actor_language = "jp"
        tag_language = "jp"
        series_language = "jp"
        studio_language = "jp"
        publisher_language = "jp"
        director_language = "jp"
    elif website == "mdtv":
        title_language = "zh_cn"
        outline_language = "zh_cn"
        actor_language = "zh_cn"
        tag_language = "zh_cn"
        series_language = "zh_cn"
        studio_language = "zh_cn"
        publisher_language = "zh_cn"
        director_language = "zh_cn"
    web_data = _call_crawler(json_data, website, title_language, file_number, short_number, mosaic, org_language)
    web_data_json = web_data.get(website, {}).get(title_language)
    json_data.update(web_data_json)
    if not json_data["title"]:
        return json_data
    if outline_language != title_language:
        web_data_json = web_data[website][outline_language]
        if web_data_json["outline"]:
            json_data["outline"] = web_data_json["outline"]
    if actor_language != title_language:
        web_data_json = web_data[website][actor_language]
        if web_data_json["actor"]:
            json_data["actor"] = web_data_json["actor"]
    if tag_language != title_language:
        web_data_json = web_data[website][tag_language]
        if web_data_json["tag"]:
            json_data["tag"] = web_data_json["tag"]
    if series_language != title_language:
        web_data_json = web_data[website][series_language]
        if web_data_json["series"]:
            json_data["series"] = web_data_json["series"]
    if studio_language != title_language:
        web_data_json = web_data[website][studio_language]
        if web_data_json["studio"]:
            json_data["studio"] = web_data_json["studio"]
    if publisher_language != title_language:
        web_data_json = web_data[website][publisher_language]
        if web_data_json["publisher"]:
            json_data["publisher"] = web_data_json["publisher"]
    if director_language != title_language:
        web_data_json = web_data[website][director_language]
        if web_data_json["director"]:
            json_data["director"] = web_data_json["director"]
    if json_data["cover"]:
        json_data["cover_list"] = [(website, json_data["cover"])]

    # åŠ å…¥æ¥æºä¿¡æ¯
    json_data["outline_from"] = website
    json_data["poster_from"] = website
    json_data["cover_from"] = website
    json_data["extrafanart_from"] = website
    json_data["trailer_from"] = website
    json_data["fields_info"] = f"\n ğŸŒ [website] {LogBuffer.req().get().strip('-> ')}"

    if short_number:
        json_data["number"] = file_number

    temp_actor = (
        web_data[website]["jp"]["actor"]
        + ","
        + web_data[website]["zh_cn"]["actor"]
        + ","
        + web_data[website]["zh_tw"]["actor"]
    )
    json_data["actor_amazon"] = []
    [json_data["actor_amazon"].append(i) for i in temp_actor.split(",") if i and i not in json_data["actor_amazon"]]
    json_data["all_actor"] = json_data["all_actor"] if json_data.get("all_actor") else web_data_json["actor"]
    json_data["all_actor_photo"] = (
        json_data["all_actor_photo"] if json_data.get("all_actor_photo") else web_data_json["actor_photo"]
    )

    return json_data


def _crawl(json_data: JsonData, website_name: str) -> JsonData:  # ä»JSONè¿”å›å…ƒæ•°æ®
    file_number = json_data["number"]
    file_path = json_data["file_path"]
    short_number = json_data["short_number"]
    appoint_number = json_data["appoint_number"]
    appoint_url = json_data["appoint_url"]
    has_sub = json_data["has_sub"]
    c_word = json_data["c_word"]
    leak = json_data["leak"]
    wuma = json_data["wuma"]
    youma = json_data["youma"]
    cd_part = json_data["cd_part"]
    destroyed = json_data["destroyed"]
    mosaic = json_data["mosaic"]
    version = json_data["version"]
    json_data["title"] = ""
    json_data["fields_info"] = ""
    json_data["all_actor"] = ""
    json_data["all_actor_photo"] = {}
    # ================================================ç½‘ç«™è§„åˆ™æ·»åŠ å¼€å§‹================================================

    if website_name == "all":  # ä»å…¨éƒ¨ç½‘ç«™åˆ®å‰Š
        # =======================================================================å…ˆåˆ¤æ–­æ˜¯ä¸æ˜¯å›½äº§ï¼Œé¿å…æµªè´¹æ—¶é—´
        if (
            mosaic == "å›½äº§"
            or mosaic == "åœ‹ç”¢"
            or (re.search(r"([^A-Z]|^)MD[A-Z-]*\d{4,}", file_number) and "MDVR" not in file_number)
            or re.search(r"MKY-[A-Z]+-\d{3,}", file_number)
        ):
            json_data["mosaic"] = "å›½äº§"
            website_list = config.website_guochan.split(",")
            json_data = _decide_websites(json_data, website_list)

        # =======================================================================kin8
        elif file_number.startswith("KIN8"):
            website_name = "kin8"
            json_data = _call_specific_crawler(json_data, website_name)

        # =======================================================================åŒäºº
        elif file_number.startswith("DLID"):
            website_name = "getchu"
            json_data = _call_specific_crawler(json_data, website_name)

        # =======================================================================é‡Œç•ª
        elif "getchu" in file_path.lower() or "é‡Œç•ª" in file_path or "è£ç•ª" in file_path:
            website_name = "getchu_dmm"
            json_data = _call_specific_crawler(json_data, website_name)

        # =======================================================================Mywife No.1111
        elif "mywife" in file_path.lower():
            website_name = "mywife"
            json_data = _call_specific_crawler(json_data, website_name)

        # =======================================================================FC2-111111
        elif "FC2" in file_number.upper():
            file_number_1 = re.search(r"\d{5,}", file_number)
            if file_number_1:
                file_number_1.group()
                website_list = config.website_fc2.split(",")
                json_data = _decide_websites(json_data, website_list)
            else:
                LogBuffer.error().write(f"æœªè¯†åˆ«åˆ°FC2ç•ªå·ï¼š{file_number}")

        # =======================================================================sexart.15.06.14
        elif re.search(r"[^.]+\.\d{2}\.\d{2}\.\d{2}", file_number) or (
            "æ¬§ç¾" in file_path and "ä¸œæ¬§ç¾" not in file_path
        ):
            website_list = config.website_oumei.split(",")
            json_data = _decide_websites(json_data, website_list)

        # =======================================================================æ— ç æŠ“å–:111111-111,n1111,HEYZO-1111,SMD-115
        elif mosaic == "æ— ç " or mosaic == "ç„¡ç¢¼":
            website_list = config.website_wuma.split(",")
            json_data = _decide_websites(json_data, website_list)

        # =======================================================================259LUXU-1111
        elif short_number or "SIRO" in file_number.upper():
            website_list = config.website_suren.split(",")
            json_data = _decide_websites(json_data, website_list)

        # =======================================================================ssni00321
        elif re.match(r"\D{2,}00\d{3,}", file_number) and "-" not in file_number and "_" not in file_number:
            website_list = ["dmm"]
            json_data = _decide_websites(json_data, website_list)

        # =======================================================================å‰©ä¸‹çš„ï¼ˆå«åŒ¹é…ä¸äº†ï¼‰çš„æŒ‰æœ‰ç æ¥åˆ®å‰Š
        else:
            website_list = config.website_youma.split(",")
            json_data = _decide_websites(json_data, website_list)
    else:
        json_data = _call_specific_crawler(json_data, website_name)

    # ================================================ç½‘ç«™è¯·æ±‚ç»“æŸ================================================
    # ======================================è¶…æ—¶æˆ–æœªæ‰¾åˆ°è¿”å›
    if json_data["title"] == "":
        return json_data

    number = json_data["number"]
    if appoint_number:
        number = appoint_number

    # é©¬èµ›å…‹
    if leak:
        json_data["mosaic"] = "æ— ç æµå‡º"
    elif destroyed:
        json_data["mosaic"] = "æ— ç ç ´è§£"
    elif wuma:
        json_data["mosaic"] = "æ— ç "
    elif youma:
        json_data["mosaic"] = "æœ‰ç "
    elif mosaic:
        json_data["mosaic"] = mosaic
    if not json_data.get("mosaic"):
        if is_uncensored(number):
            json_data["mosaic"] = "æ— ç "
        else:
            json_data["mosaic"] = "æœ‰ç "
    print(number, cd_part, json_data["mosaic"], LogBuffer.req().get().strip("-> "))

    # è½¦ç‰Œå­—æ¯
    letters = get_number_letters(number)

    # åŸæ ‡é¢˜ï¼Œç”¨äºamazonæœç´¢
    originaltitle = json_data.get("originaltitle") if json_data.get("originaltitle") else ""
    json_data["originaltitle_amazon"] = originaltitle
    for each in json_data["actor_amazon"]:  # å»é™¤æ¼”å‘˜åï¼Œé¿å…æœç´¢ä¸åˆ°
        try:
            end_actor = re.compile(rf" {each}$")
            json_data["originaltitle_amazon"] = re.sub(end_actor, "", json_data["originaltitle_amazon"])
        except:
            pass

    # VR æ—¶ä¸‹è½½å°å°é¢
    if "VR" in number:
        json_data["image_download"] = True

    # è¿”å›å¤„ç†åçš„json_data
    json_data["number"] = number
    json_data["letters"] = letters
    json_data["has_sub"] = has_sub
    json_data["c_word"] = c_word
    json_data["leak"] = leak
    json_data["wuma"] = wuma
    json_data["youma"] = youma
    json_data["cd_part"] = cd_part
    json_data["destroyed"] = destroyed
    json_data["version"] = version
    json_data["file_path"] = file_path
    json_data["appoint_number"] = appoint_number
    json_data["appoint_url"] = appoint_url

    return json_data


def _get_website_name(json_data: JsonData, file_mode: FileMode) -> str:
    # è·å–åˆ®å‰Šç½‘ç«™
    website_name = "all"
    if file_mode == FileMode.Single:  # åˆ®å‰Šå•æ–‡ä»¶ï¼ˆå·¥å…·é¡µé¢ï¼‰
        website_name = Flags.website_name
    elif file_mode == FileMode.Again:  # é‡æ–°åˆ®å‰Š
        website_temp = json_data["website_name"]
        if website_temp:
            website_name = website_temp
    elif config.scrape_like == "single":
        website_name = config.website_single

    return website_name


def crawl(json_data: JsonData, file_mode: FileMode) -> JsonData:
    # ä»æŒ‡å®šç½‘ç«™è·å–json_data
    website_name = _get_website_name(json_data, file_mode)
    json_data = _crawl(json_data, website_name)
    return _deal_json_data(json_data)


def _deal_json_data(json_data: JsonData) -> JsonData:
    # æ ‡é¢˜ä¸ºç©ºè¿”å›
    title = json_data["title"]
    if not title:
        return json_data

    # æ¼”å‘˜
    json_data["actor"] = (
        str(json_data["actor"])
        .strip(" [ ]")
        .replace("'", "")
        .replace(", ", ",")
        .replace("<", "(")
        .replace(">", ")")
        .strip(",")
    )  # åˆ—è¡¨è½¬å­—ç¬¦ä¸²ï¼ˆé¿å…ä¸ªåˆ«ç½‘ç«™åˆ®å‰Šè¿”å›çš„æ˜¯åˆ—è¡¨ï¼‰

    # æ ‡ç­¾
    tag = (
        str(json_data["tag"]).strip(" [ ]").replace("'", "").replace(", ", ",")
    )  # åˆ—è¡¨è½¬å­—ç¬¦ä¸²ï¼ˆé¿å…ä¸ªåˆ«ç½‘ç«™åˆ®å‰Šè¿”å›çš„æ˜¯åˆ—è¡¨ï¼‰
    tag = re.sub(r",\d+[kKpP],", ",", tag)
    tag_rep_word = [",HDé«˜ç”»è´¨", ",HDé«˜ç•«è³ª", ",é«˜ç”»è´¨", ",é«˜ç•«è³ª"]
    for each in tag_rep_word:
        if tag.endswith(each):
            tag = tag.replace(each, "")
        tag = tag.replace(each + ",", ",")
    json_data["tag"] = tag

    # posterå›¾
    if not json_data.get("poster"):
        json_data["poster"] = ""

    # å‘è¡Œæ—¥æœŸ
    release = json_data["release"]
    if release:
        release = release.replace("/", "-").strip(". ")
        if len(release) < 10:
            release_list = re.findall(r"(\d{4})-(\d{1,2})-(\d{1,2})", release)
            if release_list:
                r_year, r_month, r_day = release_list[0]
                r_month = "0" + r_month if len(r_month) == 1 else r_month
                r_day = "0" + r_day if len(r_day) == 1 else r_day
                release = r_year + "-" + r_month + "-" + r_day
    json_data["release"] = release

    # è¯„åˆ†
    if json_data["score"]:
        json_data["score"] = "%.1f" % float(json_data.get("score"))

    # publisher
    if not json_data.get("publisher"):
        json_data["publisher"] = json_data["studio"]

    # å­—ç¬¦è½¬ä¹‰ï¼Œé¿å…æ˜¾ç¤ºé—®é¢˜
    key_word = [
        "title",
        "originaltitle",
        "number",
        "outline",
        "originalplot",
        "actor",
        "tag",
        "series",
        "director",
        "studio",
        "publisher",
    ]
    rep_word = {
        "&amp;": "&",
        "&lt;": "<",
        "&gt;": ">",
        "&apos;": "'",
        "&quot;": '"',
        "&lsquo;": "ã€Œ",
        "&rsquo;": "ã€",
        "&hellip;": "â€¦",
        "<br/>": "",
        "ãƒ»": "Â·",
        "â€œ": "ã€Œ",
        "â€": "ã€",
        "...": "â€¦",
        "\xa0": "",
        "\u3000": "",
        "\u2800": "",
    }
    for each in key_word:
        for key, value in rep_word.items():
            json_data[each] = json_data[each].replace(key, value)

    # å‘½åè§„åˆ™
    naming_media = config.naming_media
    naming_file = config.naming_file
    folder_name = config.folder_name
    json_data["naming_media"] = naming_media
    json_data["naming_file"] = naming_file
    json_data["folder_name"] = folder_name
    return json_data
