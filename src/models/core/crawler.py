"""
çˆ¬è™«æ§åˆ¶, è°ƒç”¨ models.crawlers ä¸­å„ä¸ªç½‘ç«™çˆ¬è™«
"""
import json
import re

import langid

from models.base.number import get_number_letters, is_uncensored
from models.config.config import config
from models.core.flags import Flags
from models.crawlers import airav_cc_new, airav_new, avsex, avsox, cnmdb, dahlia, dmm, faleno, fantastica, fc2, fc2club, \
    fc2hub, freejavbt, getchu, getchu_dmm, giga, hdouban, iqqtv_new, jav321, javbus, javdb, javlibrary_new, kin8, love6, \
    lulubar, madouqu, mdtv, mgstage, mmtv, mywife, official, prestige, theporndb, xcity
from models.entity.enums import FileMode


def _get_new_website_list(field_website_list, number_website_list, file_number, short_number, field, all=False):
    whole_fields = config.whole_fields  # ç»§ç»­è¡¥å…¨çš„å­—æ®µ
    field_website_list = [i for i in field_website_list if i.strip()]  # å»ç©º
    number_website_list = [i for i in number_website_list if i.strip()]  # å»ç©º
    same_list = [i for i in field_website_list if i in number_website_list]  # å–äº¤é›†
    if field in whole_fields or field == 'title' or all:  # å–å‰©ä½™æœªç›¸äº¤ç½‘ç«™ï¼Œ trailer ä¸å–æœªç›¸äº¤ç½‘ç«™ï¼Œtitle é»˜è®¤å–æœªç›¸äº¤ç½‘ç«™
        if field != 'trailer':
            diff_list = [i for i in number_website_list if i not in field_website_list]
            same_list.extend(diff_list)
    dic_escape = {
        'title': config.title_website_exclude.split(','),
        'outline': config.outline_website_exclude.split(','),
        'actor': config.actor_website_exclude.split(','),
        'thumb': config.thumb_website_exclude.split(','),
        'poster': config.poster_website_exclude.split(','),
        'extrafanart': config.extrafanart_website_exclude.split(','),
        'trailer': config.trailer_website_exclude.split(','),
        'tag': config.tag_website_exclude.split(','),
        'release': config.release_website_exclude.split(','),
        'runtime': config.runtime_website_exclude.split(','),
        'score': config.score_website_exclude.split(','),
        'director': config.director_website_exclude.split(','),
        'series': config.series_website_exclude.split(','),
        'studio': config.studio_website_exclude.split(','),
        'publisher': config.publisher_website_exclude.split(','),
    }  # æ ¹æ®å­—æ®µæ’é™¤çš„ç½‘ç«™

    escape_list = dic_escape.get(field)
    if escape_list:
        same_list = [i for i in same_list if i not in escape_list]  # æ ¹æ®å­—æ®µæ’é™¤ä¸€äº›ä¸å«è¿™äº›å­—æ®µçš„ç½‘ç«™

    # mgstage ç´ äººç•ªå·æ£€æŸ¥
    if short_number:
        not_frist_field_list = ['title', 'actor']  # è¿™äº›å­—æ®µä»¥å¤–ï¼Œç´ äººæŠŠ mgstage æ”¾åœ¨ç¬¬ä¸€ä½
        if field not in not_frist_field_list and 'mgstage' in same_list:
            same_list.remove('mgstage')
            same_list.insert(0, 'mgstage')

    # faleno.jp ç•ªå·æ£€æŸ¥ dldss177 dhla009
    elif re.findall(r'F[A-Z]{2}SS', file_number):
        same_list = _deal_some_list(field, 'faleno', same_list)

    # dahlia-av.jp ç•ªå·æ£€æŸ¥
    elif file_number.startswith('DLDSS') or file_number.startswith('DHLA'):
        same_list = _deal_some_list(field, 'dahlia', same_list)

    # fantastica ç•ªå·æ£€æŸ¥ FAVIã€FAAPã€FAPLã€FAKGã€FAHOã€FAVAã€FAKYã€FAMIã€FAITã€FAKAã€FAMOã€FASOã€FAIHã€FASHã€FAKSã€FAAN
    elif re.search(r'FA[A-Z]{2}-?\d+', file_number.upper()) or file_number.upper().startswith(
            'CLASS') or file_number.upper().startswith('FADRV') or file_number.upper().startswith(
        'FAPRO') or file_number.upper().startswith('FAKWM') or file_number.upper().startswith('PDS'):
        same_list = _deal_some_list(field, 'fantastica', same_list)

    return same_list


def _deal_some_list(field, website, same_list):
    if website not in same_list:
        same_list.append(website)
    if field in ['title', 'outline', 'thumb', 'poster', 'trailer', 'extrafanart']:
        same_list.remove(website)
        same_list.insert(0, website)
    elif field in ['tag', 'score', 'director', 'series']:
        same_list.remove(website)
    return same_list


def _call_crawler(json_data, website, language, file_number, short_number, mosaic, org_language):
    """
    è·å–æŸä¸ªç½‘ç«™æ•°æ®
    """
    appoint_number = json_data['appoint_number']
    appoint_url = json_data['appoint_url']
    log_info = json_data['log_info']
    req_web = json_data['req_web']
    file_path = json_data['file_path']

    # 259LUXU-1111ï¼Œ mgstage å’Œ avsex ä¹‹å¤–ä½¿ç”¨ LUXU-1111ï¼ˆç´ äººç•ªå·æ—¶ï¼Œshort_numberæœ‰å€¼ï¼Œä¸å¸¦å‰ç¼€æ•°å­—ï¼›åä¹‹ï¼Œshort_numberä¸ºç©º)
    if short_number and website != 'mgstage' and website != 'avsex':
        file_number = short_number

    if website == 'official':
        json_data = json.loads(official.main(file_number, appoint_url, log_info, req_web, language))
    elif website == 'iqqtv':
        json_data = json.loads(iqqtv_new.main(file_number, appoint_url, log_info, req_web, language))
    elif website == 'avsex':
        json_data = json.loads(avsex.main(file_number, appoint_url, log_info, req_web, language))
    elif website == 'airav_cc':
        json_data = json.loads(airav_cc_new.main(file_number, appoint_url, log_info, req_web, language))
    elif website == 'airav':
        json_data = json.loads(airav_new.main(file_number, appoint_url, log_info, req_web, language))
    elif website == 'freejavbt':
        json_data = json.loads(freejavbt.main(file_number, appoint_url, log_info, req_web, language))
    elif website == 'javbus':
        json_data = json.loads(javbus.main(file_number, appoint_url, log_info, req_web, language, mosaic))
    elif website == 'javdb':
        json_data = json.loads(javdb.main(file_number, appoint_url, log_info, req_web, language, org_language))
    elif website == 'jav321':
        json_data = json.loads(jav321.main(file_number, appoint_url, log_info, req_web, language))
    elif website == 'dmm':
        json_data = json.loads(dmm.main(file_number, appoint_url, log_info, req_web, language, file_path))
    elif website == 'javlibrary':
        json_data = json.loads(javlibrary_new.main(file_number, appoint_url, log_info, req_web, language))
    elif website == 'xcity':
        json_data = json.loads(xcity.main(file_number, appoint_url, log_info, req_web, language))
    elif website == 'avsox':
        json_data = json.loads(avsox.main(file_number, appoint_url, log_info, req_web, language))
    elif website == 'mgstage':
        json_data = json.loads(mgstage.main(file_number, appoint_url, log_info, req_web, language, short_number))
    elif website == '7mmtv':
        json_data = json.loads(mmtv.main(file_number, appoint_url, log_info, req_web, language))
    elif website == 'fc2':
        json_data = json.loads(fc2.main(file_number, appoint_url, log_info, req_web, language))
    elif website == 'fc2hub':
        json_data = json.loads(fc2hub.main(file_number, appoint_url, log_info, req_web, language))
    elif website == 'fc2club':
        json_data = json.loads(fc2club.main(file_number, appoint_url, log_info, req_web, language))
    elif website == 'mdtv':
        json_data = json.loads(
            mdtv.main(file_number, appoint_url, log_info, req_web, language, file_path, appoint_number))
    elif website == 'madouqu':
        json_data = json.loads(
            madouqu.main(file_number, appoint_url, log_info, req_web, language, file_path, appoint_number))
    elif website == 'getchu':
        json_data = json.loads(getchu.main(file_number, appoint_url, log_info, req_web, language))
    elif website == 'getchu_dmm':
        json_data = json.loads(getchu_dmm.main(file_number, appoint_url, log_info, req_web, language))
    elif website == 'mywife':
        json_data = json.loads(mywife.main(file_number, appoint_url, log_info, req_web, language))
    elif website == 'giga':
        json_data = json.loads(giga.main(file_number, appoint_url, log_info, req_web, language))
    elif website == 'hdouban':
        json_data = json.loads(
            hdouban.main(file_number, appoint_url, log_info, req_web, language, file_path, appoint_number, mosaic))
    elif website == 'lulubar':
        json_data = json.loads(lulubar.main(file_number, appoint_url, log_info, req_web, language))
    elif website == 'love6':
        json_data = json.loads(love6.main(file_number, appoint_url, log_info, req_web, language))
    elif website == 'cnmdb':
        json_data = json.loads(
            cnmdb.main(file_number, appoint_url, log_info, req_web, language, file_path, appoint_number))
    elif website == 'faleno':
        json_data = json.loads(faleno.main(file_number, appoint_url, log_info, req_web, language))
    elif website == 'fantastica':
        json_data = json.loads(fantastica.main(file_number, appoint_url, log_info, req_web, language))
    elif website == 'theporndb':
        json_data = json.loads(theporndb.main(file_number, appoint_url, log_info, req_web, language, file_path))
    elif website == 'dahlia':
        json_data = json.loads(dahlia.main(file_number, appoint_url, log_info, req_web, language))
    elif website == 'prestige':
        json_data = json.loads(prestige.main(file_number, appoint_url, log_info, req_web, language))
    elif website == 'kin8':
        json_data = json.loads(kin8.main(file_number, appoint_url, log_info, req_web, language))
    else:
        json_data = json.loads(javdb.main(file_number, appoint_url, log_info, req_web, language))

    return json_data


def _decide_websites(json_data, number_website_list):
    """
    è·å–ä¸€ç»„ç½‘ç«™çš„æ•°æ®ï¼šæŒ‰ç…§è®¾ç½®çš„ç½‘ç«™ç»„ï¼Œè¯·æ±‚å„å­—æ®µæ•°æ®ï¼Œå¹¶è¿”å›æœ€ç»ˆçš„æ•°æ®
    """
    file_number = json_data['number']
    short_number = json_data['short_number']
    scrape_like = config.scrape_like
    none_fields = config.none_fields  # ä¸åˆ®å‰Šçš„å­—æ®µ

    # è·å–ä½¿ç”¨çš„ç½‘ç«™
    title_jp_website_list = config.title_website.split(',')
    title_zh_website_list = config.title_zh_website.split(',')
    outline_jp_website_list = config.outline_website.split(',')
    outline_zh_website_list = config.outline_zh_website.split(',')
    actor_website_list = config.actor_website.split(',')
    thumb_website_list = config.thumb_website.split(',')
    poster_website_list = config.poster_website.split(',')
    extrafanart_website_list = config.extrafanart_website.split(',')
    trailer_website_list = config.trailer_website.split(',')
    tag_website_list = config.tag_website.split(',')
    release_website_list = config.release_website.split(',')
    runtime_website_list = config.runtime_website.split(',')
    score_website_list = config.score_website.split(',')
    director_website_list = config.director_website.split(',')
    series_website_list = config.series_website.split(',')
    studio_website_list = config.studio_website.split(',')
    publisher_website_list = config.publisher_website.split(',')
    wanted_website_list = config.wanted_website.split(',')
    title_jp_website_new_list = _get_new_website_list(title_jp_website_list, number_website_list, file_number,
                                                      short_number, 'title')
    title_zh_website_new_list = _get_new_website_list(title_zh_website_list, number_website_list, file_number,
                                                      short_number, 'title_zh')
    outline_jp_website_new_list = _get_new_website_list(outline_jp_website_list, number_website_list, file_number,
                                                        short_number, 'outline')
    outline_zh_website_new_list = _get_new_website_list(outline_zh_website_list, number_website_list, file_number,
                                                        short_number, 'outline_zh')
    actor_website_new_list = _get_new_website_list(actor_website_list, number_website_list, file_number, short_number,
                                                   'actor')
    thumb_website_new_list = _get_new_website_list(thumb_website_list, number_website_list, file_number, short_number,
                                                   'thumb')
    poster_website_new_list = _get_new_website_list(poster_website_list, number_website_list, file_number, short_number,
                                                    'poster')
    extrafanart_website_new_list = _get_new_website_list(extrafanart_website_list, number_website_list, file_number,
                                                         short_number, 'extrafanart')
    trailer_website_new_list = _get_new_website_list(trailer_website_list, number_website_list, file_number,
                                                     short_number, 'trailer')
    tag_website_new_list = _get_new_website_list(tag_website_list, number_website_list, file_number, short_number,
                                                 'tag')
    release_website_new_list = _get_new_website_list(release_website_list, number_website_list, file_number,
                                                     short_number, 'release')
    runtime_website_new_list = _get_new_website_list(runtime_website_list, number_website_list, file_number,
                                                     short_number, 'runtime')
    score_website_new_list = _get_new_website_list(score_website_list, number_website_list, file_number, short_number,
                                                   'score')
    director_website_new_list = _get_new_website_list(director_website_list, number_website_list, file_number,
                                                      short_number, 'director')
    series_website_new_list = _get_new_website_list(series_website_list, number_website_list, file_number, short_number,
                                                    'series')
    studio_website_new_list = _get_new_website_list(studio_website_list, number_website_list, file_number, short_number,
                                                    'studio')
    publisher_website_new_list = _get_new_website_list(publisher_website_list, number_website_list, file_number,
                                                       short_number, 'publisher')
    wanted_website_new_list = _get_new_website_list(wanted_website_list, number_website_list, file_number, short_number,
                                                    'wanted')

    # åˆå§‹åŒ–å˜é‡
    all_json_data = {}

    # ç”Ÿæˆå„å­—æ®µåŠè¯·æ±‚ç½‘ç«™åˆ—è¡¨ï¼Œå¹¶è¯·æ±‚æ•°æ®
    if scrape_like == 'speed':
        request_field_list = [['title', 'æ ‡é¢˜', 'title_language', number_website_list]]
    else:
        if 'official' in config.website_set:
            title_jp_website_new_list.insert(0, 'official')
        request_field_list = [
            ['title', 'æ ‡é¢˜', 'title_language', title_jp_website_new_list],
            ['title_zh', 'ä¸­æ–‡æ ‡é¢˜', 'title_language', title_zh_website_new_list],
            ['outline', 'ç®€ä»‹', 'outline_language', outline_jp_website_new_list],
            ['outline_zh', 'ä¸­æ–‡ç®€ä»‹', 'outline_language', outline_zh_website_new_list],
            ['actor', 'æ¼”å‘˜', 'actor_language', actor_website_new_list],
            ['cover', 'èƒŒæ™¯å›¾', 'title_language', thumb_website_new_list],
            ['poster', 'å°é¢å›¾', 'title_language', poster_website_new_list],
            ['extrafanart', 'å‰§ç…§', 'title_language', extrafanart_website_new_list],
            ['tag', 'æ ‡ç­¾', 'tag_language', tag_website_new_list],
            ['release', 'å‘è¡Œæ—¥æœŸ', 'title_language', release_website_new_list],
            ['runtime', 'æ—¶é•¿', 'title_language', runtime_website_new_list],
            ['score', 'è¯„åˆ†', 'title_language', score_website_new_list],
            ['director', 'å¯¼æ¼”', 'director_language', director_website_new_list],
            ['series', 'ç³»åˆ—', 'series_language', series_website_new_list],
            ['studio', 'ç‰‡å•†', 'studio_language', studio_website_new_list],
            ['publisher', 'å‘è¡Œå•†', 'publisher_language', publisher_website_new_list],
            ['trailer', 'é¢„å‘Šç‰‡', 'title_language', trailer_website_new_list],
            ['wanted', 'æƒ³çœ‹äººæ•°', 'title_language', wanted_website_new_list],
        ]
        if config.outline_language == 'jp':
            request_field_list.pop(3)
        if config.title_language == 'jp':
            request_field_list.pop(1)
        if not wanted_website_new_list:
            request_field_list.pop()

    for each_f in request_field_list:
        field_name, field_cnname, field_language, website_list = each_f
        if field_name in none_fields:
            continue
        _call_crawlers(all_json_data, json_data, website_list, field_name, field_cnname, field_language, config,
                       file_number, short_number, json_data['mosaic'])
        if field_name == 'title' and not json_data['title']:
            return json_data

    # å¤„ç†å­—æ®µå­—æ®µï¼šä»å·²è¯·æ±‚çš„ç½‘ç«™ä¸­ï¼ŒæŒ‰å­—æ®µç½‘ç«™ä¼˜å…ˆçº§å–å€¼
    title_website_list = title_jp_website_list
    outline_website_list = outline_jp_website_list
    number_website_list = [i for i in number_website_list if i in all_json_data.keys()]
    new_number_website_list = number_website_list
    if 'official' in all_json_data.keys() and all_json_data['official']['jp']['title']:
        official_website_name = all_json_data['official']['jp']['source']
        new_number_website_list = [official_website_name] + number_website_list
        title_jp_website_list = [official_website_name] + title_jp_website_list
        outline_jp_website_list = [official_website_name] + outline_jp_website_list
    if config.title_language != 'jp':
        title_website_list = title_zh_website_list + title_jp_website_list
    if config.outline_language != 'jp':
        outline_website_list = outline_zh_website_list + outline_jp_website_list
    title_website_new_list = _get_new_website_list(title_website_list, new_number_website_list, file_number,
                                                   short_number, 'title', all=True)
    title_jp_website_new_list = _get_new_website_list(title_jp_website_list, new_number_website_list, file_number,
                                                      short_number, 'title', all=True)
    outline_website_new_list = _get_new_website_list(outline_website_list, new_number_website_list, file_number,
                                                     short_number, 'outline', all=True)
    outline_jp_website_new_list = _get_new_website_list(outline_jp_website_list, new_number_website_list, file_number,
                                                        short_number, 'outline', all=True)
    actor_website_new_list = _get_new_website_list(actor_website_list, number_website_list, file_number, short_number,
                                                   'actor', all=True)
    thumb_website_new_list = _get_new_website_list(thumb_website_list, number_website_list, file_number, short_number,
                                                   'thumb', all=True)
    poster_website_new_list = _get_new_website_list(poster_website_list, number_website_list, file_number, short_number,
                                                    'poster', all=True)
    extrafanart_website_new_list = _get_new_website_list(extrafanart_website_list, number_website_list, file_number,
                                                         short_number, 'extrafanart', all=True)
    tag_website_new_list = _get_new_website_list(tag_website_list, number_website_list, file_number, short_number,
                                                 'tag', all=True)
    release_website_new_list = _get_new_website_list(release_website_list, number_website_list, file_number,
                                                     short_number, 'release', all=True)
    runtime_website_new_list = _get_new_website_list(runtime_website_list, number_website_list, file_number,
                                                     short_number, 'runtime', all=True)
    score_website_new_list = _get_new_website_list(score_website_list, number_website_list, file_number, short_number,
                                                   'score', all=True)
    director_website_new_list = _get_new_website_list(director_website_list, number_website_list, file_number,
                                                      short_number, 'director', all=True)
    series_website_new_list = _get_new_website_list(series_website_list, number_website_list, file_number, short_number,
                                                    'series', all=True)
    studio_website_new_list = _get_new_website_list(studio_website_list, number_website_list, file_number, short_number,
                                                    'studio', all=True)
    publisher_website_new_list = _get_new_website_list(publisher_website_list, number_website_list, file_number,
                                                       short_number, 'publisher', all=True)
    trailer_website_new_list = _get_new_website_list(trailer_website_list, number_website_list, file_number,
                                                     short_number, 'trailer', all=True)
    wanted_website_new_list = _get_new_website_list(wanted_website_list, number_website_list, file_number, short_number,
                                                    'wanted')
    deal_field_list = [
        ['title', 'æ ‡é¢˜', 'title_language', title_website_new_list],
        ['originaltitle', 'åŸæ ‡é¢˜', 'outline_language', title_jp_website_new_list],
        ['outline', 'ç®€ä»‹', 'outline_language', outline_website_new_list],
        ['originalplot', 'åŸç®€ä»‹', 'outline_language', outline_jp_website_new_list],
        ['actor', 'æ¼”å‘˜', 'actor_language', actor_website_new_list],
        ['cover', 'èƒŒæ™¯å›¾', 'title_language', thumb_website_new_list],
        ['poster', 'å°é¢å›¾', 'title_language', poster_website_new_list],
        ['extrafanart', 'å‰§ç…§', 'title_language', extrafanart_website_new_list],
        ['tag', 'æ ‡ç­¾', 'tag_language', tag_website_new_list],
        ['release', 'å‘è¡Œæ—¥æœŸ', 'title_language', release_website_new_list],
        ['runtime', 'æ—¶é•¿', 'title_language', runtime_website_new_list],
        ['score', 'è¯„åˆ†', 'title_language', score_website_new_list],
        ['director', 'å¯¼æ¼”', 'director_language', director_website_new_list],
        ['series', 'ç³»åˆ—', 'series_language', series_website_new_list],
        ['studio', 'ç‰‡å•†', 'studio_language', studio_website_new_list],
        ['publisher', 'å‘è¡Œå•†', 'publisher_language', publisher_website_new_list],
        ['trailer', 'é¢„å‘Šç‰‡', 'title_language', trailer_website_new_list],
        ['wanted', 'æƒ³çœ‹äººæ•°', 'title_language', wanted_website_list],
    ]
    if not wanted_website_new_list or (scrape_like == 'speed' and json_data['source'] not in wanted_website_new_list):
        deal_field_list.pop()

    for each_f in deal_field_list:
        field_name, field_cnname, field_language, website_list = each_f
        _deal_each_field(all_json_data, json_data, website_list, field_name, field_cnname, field_language, config)

    # æŠŠå·²åˆ®å‰ŠæˆåŠŸç½‘ç«™çš„ cover url æŒ‰ç…§ cover ç½‘ç«™ä¼˜å…ˆçº§ï¼Œä¿å­˜ä¸ºä¸€ä¸ªåˆ—è¡¨ï¼Œç¬¬ä¸€ä¸ªå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œå¯ä»¥ä½¿ç”¨å…¶ä»–å›¾ç‰‡ä¸‹è½½
    cover_list = []
    for each_website in thumb_website_new_list:
        if each_website in all_json_data.keys() and all_json_data[each_website]['jp']['title']:
            temp_url = all_json_data[each_website]['jp']['cover']
            if temp_url not in cover_list:
                cover_list.append([each_website, temp_url])
    if not cover_list:
        json_data['cover'] = ''  # GBBH-1041 èƒŒæ™¯å›¾å›¾æŒ‚äº†
    json_data['cover_list'] = cover_list

    # æŠŠå·²åˆ®å‰ŠæˆåŠŸç½‘ç«™çš„ actorï¼Œä¿å­˜ä¸ºä¸€ä¸ªåˆ—è¡¨ï¼Œç”¨äº Amazon æœå›¾ï¼Œå› ä¸ºæœ‰çš„ç½‘ç«™ actor ä¸å¯¹ï¼Œæ¯”å¦‚ MOPP-023 javbusé”™çš„
    actor_amazon_list = []
    actor_amazon_list_cn = []
    actor_amazon_list_tw = []
    actor_new_website = []
    [actor_new_website.append(i) for i in title_jp_website_new_list + title_website_new_list + actor_website_new_list if
     i not in actor_new_website]
    for each_website in actor_new_website:
        if each_website in all_json_data.keys() and all_json_data[each_website]['jp']['title']:
            temp_actor = all_json_data[each_website]['jp']['actor']
            if temp_actor:
                actor_amazon_list.extend(temp_actor.split(','))
                if all_json_data[each_website]['zh_cn']['title']:
                    actor_amazon_list_cn.extend(all_json_data[each_website]['zh_cn']['actor'].split(','))
                if all_json_data[each_website]['zh_tw']['title']:
                    actor_amazon_list_tw.extend(all_json_data[each_website]['zh_tw']['actor'].split(','))
    actor_amazon_list = actor_amazon_list + actor_amazon_list_cn + actor_amazon_list_tw
    actor_amazon = []
    [actor_amazon.append(i.strip()) for i in actor_amazon_list if i.strip() and i.strip() not in actor_amazon]
    if 'ç´ äºº' in actor_amazon:
        actor_amazon.remove('ç´ äºº')
    json_data['actor_amazon'] = actor_amazon

    # å¤„ç† year
    release = json_data['release']
    if release and re.search(r'\d{4}', release):
        json_data['year'] = str(re.search(r'\d{4}', release).group())

    # å¤„ç† numberï¼šç´ äººå½±ç‰‡æ—¶ä½¿ç”¨æœ‰æ•°å­—å‰ç¼€çš„number
    if short_number:
        json_data['number'] = file_number

    json_data['fields_info'] = '\n ğŸŒ [website] %s' % json_data['req_web'].strip('-> ') + json_data['fields_info']
    if "javdb" in all_json_data and "javdbid" in all_json_data["javdb"]['jp']:
        json_data['javdbid'] = all_json_data["javdb"]['jp']['javdbid']
    else:
        json_data['javdbid'] = ''
    return json_data


def _deal_each_field(all_json_data, json_data, website_list, field_name, field_cnname, field_language, config):
    """
    æŒ‰ç…§è®¾ç½®çš„ç½‘ç«™é¡ºåºå¤„ç†å­—æ®µ
    """
    if config.scrape_like == 'speed':
        website_list = [json_data['source']]

    elif 'official' in config.website_set:
        if all_json_data['official']['jp']['title']:
            if field_name not in ['title', 'originaltitle', 'outline', 'originalplot', 'wanted', 'score']:
                website_list.insert(0, all_json_data['official']['jp']['source'])

    if not website_list:
        return

    backup_data = ''
    json_data['log_info'] += '\n\n    ğŸ™‹ğŸ»â€ %s \n    ====================================\n    ğŸŒ æ¥æºä¼˜å…ˆçº§ï¼š%s' % (
        field_cnname, ' -> '.join(website_list))
    for website in website_list:
        title_language = getattr(config, field_language)
        if website not in ['airav_cc', 'iqqtv', 'airav', 'avsex', 'javlibrary', 'mdtv', 'madouqu', 'lulubar']:
            title_language = 'jp'
        elif field_name == 'originaltitle' or field_name == 'originalplot' or field_name == 'trailer' or field_name == 'wanted':
            title_language = 'jp'
        try:
            web_data_json = all_json_data[website][title_language]
        except:
            continue

        if web_data_json['title'] and web_data_json[field_name]:
            if not len(backup_data):
                backup_data = web_data_json[field_name]
                backup_website = website

            if config.scrape_like != 'speed':
                if field_name in ['title', 'outline', 'originaltitle', 'originalplot']:
                    if website in ['airav_cc', 'iqqtv', 'airav', 'avsex', 'javlibrary', 'lulubar']:
                        if langid.classify(web_data_json[field_name])[0] != 'ja':
                            if title_language == 'jp':
                                json_data['log_info'] += f'\n    ğŸ”´ {website} (å¤±è´¥ï¼Œæ£€æµ‹ä¸ºéæ—¥æ–‡ï¼Œè·³è¿‡ï¼)'
                                continue
                        elif title_language != 'jp':
                            json_data['log_info'] += f'\n    ğŸ”´ {website} (å¤±è´¥ï¼Œæ£€æµ‹ä¸ºæ—¥æ–‡ï¼Œè·³è¿‡ï¼)'
                            continue
            if field_name == 'poster':
                json_data['poster_from'] = website
                json_data['image_download'] = web_data_json['image_download']
            elif field_name == 'cover':
                json_data['cover_from'] = website
            elif field_name == 'extrafanart':
                json_data['extrafanart_from'] = website
            elif field_name == 'trailer':
                json_data['trailer_from'] = website
            elif field_name == 'outline':
                json_data['outline_from'] = website
            elif field_name == 'actor':
                json_data['all_actor'] = json_data['all_actor'] if json_data.get('all_actor') else web_data_json[
                    'actor']
                json_data['all_actor_photo'] = json_data['all_actor_photo'] if json_data.get('all_actor_photo') else \
                    web_data_json['actor_photo']
            elif field_name == 'originaltitle':
                if web_data_json['actor']:
                    json_data['amazon_orginaltitle_actor'] = web_data_json['actor'].split(',')[0]
            json_data[field_name] = web_data_json[field_name]
            json_data['fields_info'] += '\n     ' + "%-13s" % field_name + ': %s (%s)' % (website, title_language)
            json_data['log_info'] += f'\n    ğŸŸ¢ {website} (æˆåŠŸ)\n     â†³ {json_data[field_name]}'
            break
        else:
            json_data['log_info'] += f'\n    ğŸ”´ {website} (å¤±è´¥)'
    else:
        if len(backup_data):
            json_data[field_name] = backup_data
            json_data['fields_info'] += '\n     ' + f"{field_name:<13}" + f': {backup_website} ({title_language})'
            json_data['log_info'] += f'\n    ğŸŸ¢ {backup_website} (ä½¿ç”¨å¤‡ç”¨æ•°æ®)\n     â†³ {backup_data}'
        else:
            json_data['fields_info'] += '\n     ' + f"{field_name:<13}" + f': {"-----"} ({"not found"})'


def _call_crawlers(all_json_data, json_data, website_list, field_name, field_cnname, field_language, config,
                    file_number, short_number, mosaic): # 4
    """
    æŒ‰ç…§è®¾ç½®çš„ç½‘ç«™é¡ºåºè·å–å„ä¸ªå­—æ®µä¿¡æ¯
    """
    if 'official' in config.website_set:
        if field_name not in ['title', 'title_zh', 'outline_zh', 'wanted', 'score']:
            website_list.insert(0, 'official')

    backup_jsondata = {}
    for website in website_list:
        if (website in ['avsox', 'mdtv'] and mosaic in ['æœ‰ç ', 'æ— ç ç ´è§£', 'æµå‡º', 'é‡Œç•ª', 'åŠ¨æ¼«']) or (
                website == 'mdtv' and mosaic == 'æ— ç '):
            if field_name != 'title':
                continue
        if field_name in ['title_zh', 'outline_zh']:
            title_language = 'zh_cn'
            field_name = field_name.replace('_zh', '')
        elif field_name in ['originaltitle', 'originalplot', 'trailer', 'wanted']:
            title_language = 'jp'
        elif website not in ['airav_cc', 'iqqtv', 'airav', 'avsex', 'javlibrary', 'mdtv', 'madouqu', 'lulubar']:
            title_language = 'jp'
        else:
            title_language = getattr(config, field_language)

        try:
            web_data_json = all_json_data[website][title_language]
        except:
            web_data = _call_crawler(json_data, website, title_language, file_number, short_number, mosaic,
                                     config.title_language)
            all_json_data.update(web_data)
            web_data_json = all_json_data.get(website).get(title_language)
            json_data['req_web'] = web_data_json['req_web']
            json_data['log_info'] = web_data_json['log_info']

        if field_cnname == 'æ ‡é¢˜':
            json_data.update(web_data_json)
        if web_data_json['title'] and web_data_json[field_name]:
            if not len(backup_jsondata):
                backup_jsondata.update(web_data_json)
                backup_website = website
                backup_jsondata.pop('req_web')
                backup_jsondata.pop('log_info')
            if field_cnname == 'æ ‡é¢˜':
                json_data['outline_from'] = website
                json_data['poster_from'] = website
                json_data['cover_from'] = website
                json_data['extrafanart_from'] = website
                json_data['trailer_from'] = website
            if config.scrape_like != 'speed':
                if website in ['airav_cc', 'iqqtv', 'airav', 'avsex', 'javlibrary', 'lulubar']:
                    if field_name in ['title', 'outline', 'originaltitle', 'originalplot']:
                        if langid.classify(web_data_json[field_name])[0] != 'ja':
                            if title_language == 'jp':
                                json_data[
                                    'log_info'] += f'\n    ğŸ”´ {field_cnname} æ£€æµ‹ä¸ºéæ—¥æ–‡ï¼Œè·³è¿‡ï¼({website})\n     â†³ {web_data_json[field_name]}'
                                continue
                        elif title_language != 'jp':
                            json_data[
                                'log_info'] += f'\n    ğŸ”´ {field_cnname} æ£€æµ‹ä¸ºæ—¥æ–‡ï¼Œè·³è¿‡ï¼({website})\n     â†³ {web_data_json[field_name]}'
                            continue
                elif website == 'official':
                    website = all_json_data['official']['jp']['source']
            json_data['log_info'] += f'\n    ğŸŸ¢ {field_cnname} è·å–æˆåŠŸï¼({website})\n     â†³ {web_data_json[field_name]} '
            break
    else:
        if len(backup_jsondata):
            json_data[
                'log_info'] += f'\n    ğŸŸ¢ {field_cnname} ä½¿ç”¨å¤‡ç”¨æ•°æ®ï¼({backup_website})\n     â†³ {backup_jsondata[field_name]} '
            if field_cnname == 'æ ‡é¢˜':
                json_data.update(backup_jsondata)
        else:
            json_data['log_info'] += f'\n    ğŸ”´ {field_cnname} è·å–å¤±è´¥ï¼'


def _call_specific_crawler(json_data, website):
    file_number = json_data['number']
    short_number = json_data['short_number']
    mosaic = json_data['mosaic']
    json_data['fields_info'] = ''

    title_language = config.title_language
    org_language = title_language
    outline_language = config.outline_language
    actor_language = config.actor_language
    tag_language = config.tag_language
    series_language = config.series_language
    studio_language = config.studio_language
    publisher_language = config.publisher_language
    director_language = config.director_language
    if website not in ['airav_cc', 'iqqtv', 'airav', 'avsex', 'javlibrary', 'mdtv', 'madouqu', 'lulubar']:
        title_language = 'jp'
        outline_language = 'jp'
        actor_language = 'jp'
        tag_language = 'jp'
        series_language = 'jp'
        studio_language = 'jp'
        publisher_language = 'jp'
        director_language = 'jp'
    elif website == 'mdtv':
        title_language = 'zh_cn'
        outline_language = 'zh_cn'
        actor_language = 'zh_cn'
        tag_language = 'zh_cn'
        series_language = 'zh_cn'
        studio_language = 'zh_cn'
        publisher_language = 'zh_cn'
        director_language = 'zh_cn'
    web_data = _call_crawler(json_data, website, title_language, file_number, short_number, mosaic, org_language)
    web_data_json = web_data.get(website).get(title_language)
    json_data.update(web_data_json)
    if not json_data['title']:
        return json_data
    if outline_language != title_language:
        web_data_json = web_data[website][outline_language]
        if web_data_json['outline']:
            json_data['outline'] = web_data_json['outline']
    if actor_language != title_language:
        web_data_json = web_data[website][actor_language]
        if web_data_json['actor']:
            json_data['actor'] = web_data_json['actor']
    if tag_language != title_language:
        web_data_json = web_data[website][tag_language]
        if web_data_json['tag']:
            json_data['tag'] = web_data_json['tag']
    if series_language != title_language:
        web_data_json = web_data[website][series_language]
        if web_data_json['series']:
            json_data['series'] = web_data_json['series']
    if studio_language != title_language:
        web_data_json = web_data[website][studio_language]
        if web_data_json['studio']:
            json_data['studio'] = web_data_json['studio']
    if publisher_language != title_language:
        web_data_json = web_data[website][publisher_language]
        if web_data_json['publisher']:
            json_data['publisher'] = web_data_json['publisher']
    if director_language != title_language:
        web_data_json = web_data[website][director_language]
        if web_data_json['director']:
            json_data['director'] = web_data_json['director']
    if json_data['cover']:
        json_data['cover_list'] = [[website, json_data['cover']]]

    # åŠ å…¥æ¥æºä¿¡æ¯
    json_data['outline_from'] = website
    json_data['poster_from'] = website
    json_data['cover_from'] = website
    json_data['extrafanart_from'] = website
    json_data['trailer_from'] = website
    json_data['fields_info'] = '\n ğŸŒ [website] %s' % json_data['req_web'].strip('-> ')

    if short_number:
        json_data['number'] = file_number

    temp_actor = web_data[website]['jp']['actor'] + ',' + web_data[website]['zh_cn']['actor'] + ',' + \
                 web_data[website]['zh_tw']['actor']
    json_data['actor_amazon'] = []
    [json_data['actor_amazon'].append(i) for i in temp_actor.split(',') if i and i not in json_data['actor_amazon']]
    json_data['all_actor'] = json_data['all_actor'] if json_data.get('all_actor') else web_data_json['actor']
    json_data['all_actor_photo'] = json_data['all_actor_photo'] if json_data.get('all_actor_photo') else web_data_json[
        'actor_photo']

    return json_data


def _crawl(json_data, website_name):  # ä»JSONè¿”å›å…ƒæ•°æ®
    file_number = json_data['number']
    file_path = json_data['file_path']
    short_number = json_data['short_number']
    appoint_number = json_data['appoint_number']
    appoint_url = json_data['appoint_url']
    logs = json_data['logs']
    has_sub = json_data['has_sub']
    c_word = json_data['c_word']
    leak = json_data['leak']
    wuma = json_data['wuma']
    youma = json_data['youma']
    cd_part = json_data['cd_part']
    destroyed = json_data['destroyed']
    mosaic = json_data['mosaic']
    version = json_data['version']
    json_data['title'] = ''
    json_data['req_web'] = ''
    json_data['log_info'] = ''
    json_data['fields_info'] = ''
    json_data['all_actor'] = ''
    json_data['all_actor_photo'] = ''

    # ================================================ç½‘ç«™è§„åˆ™æ·»åŠ å¼€å§‹================================================

    if website_name == 'all':  # ä»å…¨éƒ¨ç½‘ç«™åˆ®å‰Š

        # =======================================================================å…ˆåˆ¤æ–­æ˜¯ä¸æ˜¯å›½äº§ï¼Œé¿å…æµªè´¹æ—¶é—´
        if mosaic == 'å›½äº§' or mosaic == 'åœ‹ç”¢' or (
                re.search(r'([^A-Z]|^)MD[A-Z-]*\d{4,}', file_number) and 'MDVR' not in file_number) or re.search(
            r'MKY-[A-Z]+-\d{3,}', file_number):
            json_data['mosaic'] = 'å›½äº§'
            website_list = config.website_guochan.split(',')
            json_data = _decide_websites(json_data, website_list)

        # =======================================================================kin8
        elif file_number.startswith('KIN8'):
            website_name = 'kin8'
            json_data = _call_specific_crawler(json_data, website_name)

        # =======================================================================åŒäºº
        elif file_number.startswith('DLID'):
            website_name = 'getchu'
            json_data = _call_specific_crawler(json_data, website_name)

        # =======================================================================é‡Œç•ª
        elif 'getchu' in file_path.lower() or 'é‡Œç•ª' in file_path or 'è£ç•ª' in file_path:
            website_name = 'getchu_dmm'
            json_data = _call_specific_crawler(json_data, website_name)

        # =======================================================================Mywife No.1111
        elif 'mywife' in file_path.lower():
            website_name = 'mywife'
            json_data = _call_specific_crawler(json_data, website_name)

        # =======================================================================FC2-111111
        elif 'FC2' in file_number.upper():
            file_number_1 = re.search(r'\d{5,}', file_number)
            if file_number_1:
                file_number_1.group()
                website_list = config.website_fc2.split(',')
                json_data = _decide_websites(json_data, website_list)
            else:
                json_data['error_info'] = 'æœªè¯†åˆ«åˆ°FC2ç•ªå·ï¼š%s' % file_number

        # =======================================================================sexart.15.06.14
        elif re.search(r'[^.]+\.\d{2}\.\d{2}\.\d{2}', file_number) or (
                'æ¬§ç¾' in file_path and 'ä¸œæ¬§ç¾' not in file_path):
            website_list = config.website_oumei.split(',')
            json_data = _decide_websites(json_data, website_list)

        # =======================================================================æ— ç æŠ“å–:111111-111,n1111,HEYZO-1111,SMD-115
        elif mosaic == 'æ— ç ' or mosaic == 'ç„¡ç¢¼':
            website_list = config.website_wuma.split(',')
            json_data = _decide_websites(json_data, website_list)

        # =======================================================================259LUXU-1111
        elif short_number or 'SIRO' in file_number.upper():
            website_list = config.website_suren.split(',')
            json_data = _decide_websites(json_data, website_list)

        # =======================================================================ssni00321
        elif re.match(r'\D{2,}00\d{3,}', file_number) and '-' not in file_number and '_' not in file_number:
            website_list = ['dmm']
            json_data = _decide_websites(json_data, website_list)

        # =======================================================================å‰©ä¸‹çš„ï¼ˆå«åŒ¹é…ä¸äº†ï¼‰çš„æŒ‰æœ‰ç æ¥åˆ®å‰Š
        else:
            website_list = config.website_youma.split(',')
            json_data = _decide_websites(json_data, website_list)
    else:
        json_data = _call_specific_crawler(json_data, website_name)

    # ================================================ç½‘ç«™è¯·æ±‚ç»“æŸ================================================
    # ======================================è¶…æ—¶æˆ–æœªæ‰¾åˆ°è¿”å›
    if json_data['title'] == '':
        return json_data

    number = json_data['number']
    if appoint_number:
        number = appoint_number

    # é©¬èµ›å…‹
    if leak:
        json_data['mosaic'] = 'æ— ç æµå‡º'
    elif destroyed:
        json_data['mosaic'] = 'æ— ç ç ´è§£'
    elif wuma:
        json_data['mosaic'] = 'æ— ç '
    elif youma:
        json_data['mosaic'] = 'æœ‰ç '
    elif mosaic:
        json_data['mosaic'] = mosaic
    if not json_data.get('mosaic'):
        if is_uncensored(number):
            json_data['mosaic'] = 'æ— ç '
        else:
            json_data['mosaic'] = 'æœ‰ç '
    print(number, cd_part, json_data['mosaic'], json_data['req_web'].strip('-> '))

    # è½¦ç‰Œå­—æ¯
    letters = get_number_letters(number)

    # åŸæ ‡é¢˜ï¼Œç”¨äºamazonæœç´¢
    originaltitle = json_data.get('originaltitle') if json_data.get('originaltitle') else ''
    json_data['originaltitle_amazon'] = originaltitle
    for each in json_data['actor_amazon']:  # å»é™¤æ¼”å‘˜åï¼Œé¿å…æœç´¢ä¸åˆ°
        try:
            end_actor = re.compile(r' %s$' % each)
            json_data['originaltitle_amazon'] = re.sub(end_actor, '', json_data['originaltitle_amazon'])
        except:
            pass

    # VR æ—¶ä¸‹è½½å°å°é¢
    if 'VR' in number:
        json_data['image_download'] = True

    # è¿”å›å¤„ç†åçš„json_data
    json_data['number'] = number
    json_data['letters'] = letters
    json_data['has_sub'] = has_sub
    json_data['c_word'] = c_word
    json_data['leak'] = leak
    json_data['wuma'] = wuma
    json_data['youma'] = youma
    json_data['4k'] = ''
    json_data['cd_part'] = cd_part
    json_data['destroyed'] = destroyed
    json_data['actor_href'] = ''
    json_data['version'] = version
    json_data['logs'] = logs
    json_data['file_path'] = file_path
    json_data['appoint_number'] = appoint_number
    json_data['appoint_url'] = appoint_url
    json_data['poster_path'] = ''
    json_data['thumb_path'] = ''
    json_data['fanart_path'] = ''
    json_data['error_info'] = ''

    return json_data


def _get_website_name(json_data, file_mode):
    # è·å–åˆ®å‰Šç½‘ç«™
    website_name = 'all'
    if file_mode == FileMode.Single:  # åˆ®å‰Šå•æ–‡ä»¶ï¼ˆå·¥å…·é¡µé¢ï¼‰
        website_name = Flags.website_name
    elif file_mode == FileMode.Again:  # é‡æ–°åˆ®å‰Š
        website_temp = json_data['website_name']
        if website_temp:
            website_name = website_temp
    elif config.scrape_like == 'single':
        website_name = config.website_single

    return website_name


def crawl(json_data, file_mode):
    # ä»æŒ‡å®šç½‘ç«™è·å–json_data
    website_name = _get_website_name(json_data, file_mode)
    json_data = _crawl(json_data, website_name)
    return _deal_json_data(json_data)


def _deal_json_data(json_data):
    # æ ‡é¢˜ä¸ºç©ºè¿”å›
    title = json_data['title']
    if not title:
        return json_data

    # æ¼”å‘˜
    json_data['actor'] = str(json_data['actor']).strip(" [ ]").replace("'", '').replace(', ', ',').replace('<',
                                                                                                           '(').replace(
        '>', ')').strip(',')  # åˆ—è¡¨è½¬å­—ç¬¦ä¸²ï¼ˆé¿å…ä¸ªåˆ«ç½‘ç«™åˆ®å‰Šè¿”å›çš„æ˜¯åˆ—è¡¨ï¼‰

    # æ ‡ç­¾
    tag = str(json_data['tag']).strip(" [ ]").replace("'", '').replace(', ', ',')  # åˆ—è¡¨è½¬å­—ç¬¦ä¸²ï¼ˆé¿å…ä¸ªåˆ«ç½‘ç«™åˆ®å‰Šè¿”å›çš„æ˜¯åˆ—è¡¨ï¼‰
    tag = re.sub(r',\d+[kKpP]', '', tag)
    tag_rep_word = [',HDé«˜ç”»è´¨', ',HDé«˜ç•«è³ª', ',é«˜ç”»è´¨', ',é«˜ç•«è³ª']
    for each in tag_rep_word:
        tag = tag.replace(each, '')
    json_data['tag'] = tag

    # posterå›¾
    if not json_data.get('poster'):
        json_data['poster'] = ''

    # å‘è¡Œæ—¥æœŸ
    release = json_data['release']
    if release:
        release = release.replace('/', '-').strip('. ')
        if len(release) < 10:
            release_list = re.findall(r'(\d{4})-(\d{1,2})-(\d{1,2})', release)
            if release_list:
                r_year, r_month, r_day = release_list[0]
                r_month = '0' + r_month if len(r_month) == 1 else r_month
                r_day = '0' + r_day if len(r_day) == 1 else r_day
                release = r_year + '-' + r_month + '-' + r_day
    json_data['release'] = release

    # è¯„åˆ†
    if json_data.get('score'):
        json_data['score'] = '%.1f' % float(json_data.get('score'))
    else:
        json_data['score'] = ''

    # originaltitle
    if not json_data.get('originaltitle'):
        json_data['originaltitle'] = ''

    # outline
    if not json_data.get('outline'):
        json_data['outline'] = ''

    # originalplot
    if not json_data.get('originalplot'):
        json_data['originalplot'] = ''

    # series
    if not json_data.get('series'):
        json_data['series'] = ''

    # series
    if not json_data.get('director'):
        json_data['director'] = ''

    # studio
    if not json_data.get('studio'):
        json_data['studio'] = ''

    # publisher
    if not json_data.get('publisher'):
        json_data['publisher'] = json_data['studio']

    # trailer
    if not json_data.get('trailer'):
        json_data['trailer'] = ''

    # wanted
    if not json_data.get('wanted'):
        json_data['wanted'] = ''

    # å­—ç¬¦è½¬ä¹‰ï¼Œé¿å…æ˜¾ç¤ºé—®é¢˜
    key_word = ['title', 'originaltitle', 'number', 'outline', 'originalplot', 'actor', 'tag', 'series', 'director',
                'studio', 'publisher']
    rep_word = {
        '&amp;': '&',
        '&lt;': '<',
        '&gt;': '>',
        '&apos;': "'",
        '&quot;': '"',
        '&lsquo;': 'ã€Œ',
        '&rsquo;': 'ã€',
        '&hellip;': 'â€¦',
        '<br/>': '',
        'ãƒ»': 'Â·',
        'â€œ': 'ã€Œ',
        'â€': 'ã€',
        '...': 'â€¦',
        u'\xa0': '',
        u'\u3000': '',
        u'\u2800': '',
    }
    for each in key_word:
        for key, value in rep_word.items():
            json_data[each] = json_data[each].replace(key, value)

    # å‘½åè§„åˆ™
    naming_media = config.naming_media
    naming_file = config.naming_file
    folder_name = config.folder_name
    json_data['naming_media'] = naming_media
    json_data['naming_file'] = naming_file
    json_data['folder_name'] = folder_name
    return json_data
