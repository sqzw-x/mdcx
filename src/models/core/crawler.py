"""
çˆ¬è™«æ§åˆ¶, è°ƒç”¨ models.crawlers ä¸­å„ä¸ªç½‘ç«™çˆ¬è™«
"""

import asyncio
import re
from typing import Callable

import langid

from ..base.number import get_number_letters, is_uncensored
from ..config.manager import config
from ..config.manual import ManualConfig
from ..crawlers import (
    airav,
    airav_cc,
    avsex,
    avsox,
    cableav,
    cnmdb,
    dahlia,
    dmm,
    faleno,
    fantastica,
    fc2,
    fc2club,
    fc2hub,
    fc2ppvdb,
    freejavbt,
    getchu,
    getchu_dmm,
    giga,
    hdouban,
    hscangku,
    iqqtv_new,
    jav321,
    javbus,
    javday,
    javdb,
    javlibrary_new,
    kin8,
    love6,
    lulubar,
    madouqu,
    mdtv,
    mgstage,
    mmtv,
    mywife,
    official,
    prestige,
    theporndb,
    xcity,
)
from ..entity.enums import FileMode
from .flags import Flags
from .json_data import JsonData, LogBuffer

CRAWLER_FUNCS: dict[str, Callable] = {
    "official": official.main,
    "iqqtv": iqqtv_new.main,
    "avsex": avsex.main,
    "airav_cc": airav_cc.main,
    "airav": airav.main,
    "freejavbt": freejavbt.main,
    "javbus": javbus.main,
    "javdb": javdb.main,
    "jav321": jav321.main,
    "dmm": dmm.main,
    "javlibrary": javlibrary_new.main,
    "xcity": xcity.main,
    "avsox": avsox.main,
    "mgstage": mgstage.main,
    "7mmtv": mmtv.main,
    "fc2": fc2.main,
    "fc2hub": fc2hub.main,
    "fc2club": fc2club.main,
    "fc2ppvdb": fc2ppvdb.main,
    "mdtv": mdtv.main,
    "madouqu": madouqu.main,
    "hscangku": hscangku.main,
    "cableav": cableav.main,
    "getchu": getchu.main,
    "getchu_dmm": getchu_dmm.main,
    "mywife": mywife.main,
    "giga": giga.main,
    "hdouban": hdouban.main,
    "lulubar": lulubar.main,
    "love6": love6.main,
    "cnmdb": cnmdb.main,
    "faleno": faleno.main,
    "fantastica": fantastica.main,
    "theporndb": theporndb.main,
    "dahlia": dahlia.main,
    "prestige": prestige.main,
    "kin8": kin8.main,
    "javday": javday.main,
}


def clean_list(raw: list[str]) -> list[str]:
    """æ¸…ç†åˆ—è¡¨ï¼Œå»é™¤ç©ºå€¼å’Œé‡å¤å€¼, ä¿æŒåŸæœ‰é¡ºåº"""
    cleaned = []
    for item in raw:
        if item.strip() and item not in cleaned:
            cleaned.append(item.strip())
    return cleaned


def _deal_some_list(field: str, website: str, same_list: list[str]) -> list[str]:
    if website not in same_list:
        same_list.append(website)
    if field in ["title", "outline", "thumb", "poster", "trailer", "extrafanart"]:
        same_list.remove(website)
        same_list.insert(0, website)
    elif field in ["tag", "score", "director", "series"]:
        same_list.remove(website)
    return same_list


async def _call_crawler(
    task_input: JsonData,
    website: str,
    language: str,
    org_language: str,
) -> dict[str, dict[str, dict]]:
    """
    è·å–æŸä¸ªç½‘ç«™æ•°æ®
    """
    appoint_number = task_input["appoint_number"]
    appoint_url = task_input["appoint_url"]
    file_path = task_input["file_path"]
    number = task_input["number"]
    mosaic = task_input["mosaic"]
    short_number = task_input["short_number"]

    # 259LUXU-1111ï¼Œ mgstage å’Œ avsex ä¹‹å¤–ä½¿ç”¨ LUXU-1111ï¼ˆç´ äººç•ªå·æ—¶ï¼Œshort_numberæœ‰å€¼ï¼Œä¸å¸¦å‰ç¼€æ•°å­—ï¼›åä¹‹ï¼Œshort_numberä¸ºç©º)
    if short_number and website != "mgstage" and website != "avsex":
        number = short_number

    # è·å–çˆ¬è™«å‡½æ•°
    crawler_func = CRAWLER_FUNCS.get(website, javdb.main)

    # å‡†å¤‡å‚æ•°
    kwargs = {
        "number": number,
        "appoint_url": appoint_url,
        "language": language,
        "file_path": file_path,
        "appoint_number": appoint_number,
        "mosaic": mosaic,
        "short_number": short_number,
        "org_language": org_language,
    }

    return await crawler_func(**kwargs)


async def _call_crawlers(
    task_input: JsonData,
    number_website_list: list[str],
) -> JsonData:
    """
    è·å–ä¸€ç»„ç½‘ç«™çš„æ•°æ®ï¼šæŒ‰ç…§è®¾ç½®çš„ç½‘ç«™ç»„ï¼Œè¯·æ±‚å„å­—æ®µæ•°æ®ï¼Œå¹¶è¿”å›æœ€ç»ˆçš„æ•°æ®
    """
    number = task_input["number"]
    short_number = task_input["short_number"]
    mosaic = task_input["mosaic"]
    scrape_like = config.scrape_like
    none_fields = config.none_fields  # ä¸å•ç‹¬åˆ®å‰Šçš„å­—æ®µ
    use_official = "official" in config.website_set  # ä¼˜å…ˆä½¿ç”¨å®˜æ–¹ç½‘ç«™

    def get_field_websites(field: str) -> list[str]:
        """è·å–æŒ‡å®šå­—æ®µçš„ç½‘ç«™å–å€¼ä¼˜å…ˆçº§åˆ—è¡¨"""
        # æŒ‡å®šå­—æ®µç½‘ç«™åˆ—è¡¨
        field_no_zh = field.replace("_zh", "")  # å»é™¤ _zh åç¼€çš„å­—æ®µå
        field_list = clean_list(getattr(config, f"{field}_website", "").split(","))
        # ä¸æŒ‡å®šç±»å‹ç½‘ç«™åˆ—è¡¨å–äº¤é›†
        field_list = [i for i in field_list if i in number_website_list]
        if use_official:
            field_list.insert(0, "official")
        # æŒ‡å®šå­—æ®µæ’é™¤ç½‘ç«™åˆ—è¡¨
        field_ex_list = clean_list(getattr(config, f"{field_no_zh}_website_exclude", "").split(","))
        # æ‰€æœ‰è®¾å®šçš„æœ¬å­—æ®µæ¥æºå¤±è´¥æ—¶, æ˜¯å¦ç»§ç»­ä½¿ç”¨ç±»å‹ç½‘ç«™è¡¥å…¨
        include_others = field == "title" or field in config.whole_fields
        if include_others and field != "trailer":  # å–å‰©ä½™æœªç›¸äº¤ç½‘ç«™ï¼Œ trailer ä¸å–æœªç›¸äº¤ç½‘ç«™
            field_list.extend([i for i in number_website_list if i not in field_list])
        # æ’é™¤æŒ‡å®šç½‘ç«™
        field_list = [i for i in field_list if i not in field_ex_list]
        # ç‰¹æ®Šå¤„ç†
        # mgstage ç´ äººç•ªå·æ£€æŸ¥
        if short_number:
            not_frist_field_list = ["title", "actor"]  # è¿™äº›å­—æ®µä»¥å¤–ï¼Œç´ äººæŠŠ mgstage æ”¾åœ¨ç¬¬ä¸€ä½
            if field not in not_frist_field_list and "mgstage" in field_list:
                field_list.remove("mgstage")
                field_list.insert(0, "mgstage")
        # faleno.jp ç•ªå·æ£€æŸ¥ dldss177 dhla009
        elif re.findall(r"F[A-Z]{2}SS", number):
            field_list = _deal_some_list(field, "faleno", field_list)
        # dahlia-av.jp ç•ªå·æ£€æŸ¥
        elif number.startswith("DLDSS") or number.startswith("DHLA"):
            field_list = _deal_some_list(field, "dahlia", field_list)
        # fantastica ç•ªå·æ£€æŸ¥ FAVIã€FAAPã€FAPLã€FAKGã€FAHOã€FAVAã€FAKYã€FAMIã€FAITã€FAKAã€FAMOã€FASOã€FAIHã€FASHã€FAKSã€FAAN
        elif (
            re.search(r"FA[A-Z]{2}-?\d+", number.upper())
            or number.upper().startswith("CLASS")
            or number.upper().startswith("FADRV")
            or number.upper().startswith("FAPRO")
            or number.upper().startswith("FAKWM")
            or number.upper().startswith("PDS")
        ):
            field_list = _deal_some_list(field, "fantastica", field_list)
        return field_list

    # è·å–ä½¿ç”¨çš„ç½‘ç«™
    all_fields = [f for f in ManualConfig.CONFIG_DATA_FIELDS if f not in none_fields]  # å»é™¤ä¸ä¸“é—¨åˆ®å‰Šçš„å­—æ®µ
    if scrape_like == "speed":  # å¿«é€Ÿæ¨¡å¼
        all_field_websites = {field: number_website_list for field in all_fields}
    else:  # å…¨éƒ¨æ¨¡å¼
        # å„å­—æ®µç½‘ç«™åˆ—è¡¨
        all_field_websites = {field: get_field_websites(field) for field in all_fields}
        if config.outline_language == "jp" and "outline_zh" in all_field_websites:
            del all_field_websites["outline_zh"]
        if config.title_language == "jp" and "title_zh" in all_field_websites:
            del all_field_websites["title_zh"]
    # å„å­—æ®µè¯­è¨€
    all_field_languages = {field: getattr(config, f"{field}_language", "zh") for field in all_fields}
    # æ‰€æœ‰éœ€è¦è¯·æ±‚çš„ (ç½‘ç«™, è¯­è¨€) å¯¹
    all_websites = (
        (website, all_field_languages[field]) for field, websites in all_field_websites.items() for website in websites
    )

    tasks = []
    for website, language in all_websites:
        tasks.append(_call_crawler(task_input, website, language, config.title_language))
    res: list[dict[str, dict[str, dict]]] = await asyncio.gather(*tasks)

    # åˆå¹¶ç»“æœ
    all_res: dict[tuple[str, str], dict] = {}
    for website_data in res:
        for website, datas in website_data.items():
            for lang, data in datas.items():
                key = (website, lang)
                if key in all_res:
                    raise ValueError(f"Duplicate data for {key} found in crawler results.")
                all_res[key] = data

    # æŒ‰ä¼˜å…ˆçº§åˆå¹¶
    # å¤„ç†é…ç½®é¡¹å’Œè¿”å›å€¼çš„ä¸åŒ¹é…å­—æ®µ
    # 1. originaltitle çš„å–å€¼ä¼˜å…ˆçº§å¯¹åº” title, è¯­è¨€ä¸º jp
    all_field_websites["originaltitle"] = all_field_websites.get("title", number_website_list)
    all_field_languages["originaltitle"] = "jp"
    # 2. originalplot çš„å–å€¼ä¼˜å…ˆçº§å¯¹åº” outline
    all_field_websites["originalplot"] = all_field_websites.get("outline", number_website_list)
    all_field_languages["originalplot"] = "jp"
    # 2. å½“è¯­è¨€é jp æ—¶, æœ€ç»ˆ title çš„å–å€¼ä¼˜å…ˆçº§ä¸º title å’Œ title_zh, ä¸”éœ€æ£€æŸ¥æ‰€æœ‰è¯­è¨€
    if config.title_language != "jp":
        all_field_websites["title"] += all_field_websites.get("title_zh", [])
        all_field_languages["title"] = "all"
    # 3. å½“è¯­è¨€é jp æ—¶, æœ€ç»ˆ outline çš„å–å€¼ä¼˜å…ˆçº§ä¸º outline å’Œ outline_zh, ä¸”éœ€æ£€æŸ¥æ‰€æœ‰è¯­è¨€
    if config.outline_language != "jp":
        all_field_websites["outline"] += all_field_websites.get("outline_zh", [])
        all_field_languages["outline"] = "all"

    reduced: dict = {"number": number, "short_number": short_number, "mosaic": mosaic, "fields_info": ""}
    for field in ManualConfig.CRAWLER_DATA_FIELDS:  # ä¸ CONFIG_DATA_FIELDS ä¸å®Œå…¨ä¸€è‡´
        if field not in all_field_websites:
            # æ²¡æœ‰è®¾å®šæ­¤å­—æ®µçš„ä¼˜å…ˆçº§å’Œè¯­è¨€, åˆ™ä»»æ„å–å€¼
            sources = [(w, lang) for w in number_website_list for lang in ["jp", "zh_cn", "zh_tw"]]
        else:
            sources = [(w, all_field_languages[field]) for w in all_field_websites[field]]

        LogBuffer.info().write(
            f"\n\n    ğŸ™‹ğŸ»â€ {field} \n    ====================================\n"
            f"    ğŸŒ æ¥æºä¼˜å…ˆçº§ï¼š{' -> '.join(i[0] for i in sources)}"
        )
        for website, language in sources:
            if language == "all":
                site_data = (
                    all_res.get((website, "jp"), {})
                    or all_res.get((website, "zh_cn"), {})
                    or all_res.get((website, "zh_tw"), {})
                )
            else:
                site_data = all_res.get((website, language), {})

            if not site_data.get("title", "") or not site_data.get(field, ""):
                LogBuffer.info().write(f"\n    ğŸ”´ {website} (å¤±è´¥)")
                continue

            if config.scrape_like != "speed":
                if field in ["title", "outline", "originaltitle", "originalplot"]:
                    if website in ["airav_cc", "iqqtv", "airav", "avsex", "javlibrary", "lulubar"]:  # why?
                        if langid.classify(site_data[field])[0] != "ja":
                            if language == "jp":
                                LogBuffer.info().write(f"\n    ğŸ”´ {website} (å¤±è´¥ï¼Œæ£€æµ‹ä¸ºéæ—¥æ–‡ï¼Œè·³è¿‡ï¼)")
                                continue
                        elif language != "jp":
                            LogBuffer.info().write(f"\n    ğŸ”´ {website} (å¤±è´¥ï¼Œæ£€æµ‹ä¸ºæ—¥æ–‡ï¼Œè·³è¿‡ï¼)")
                            continue
            # æ·»åŠ æ¥æºä¿¡æ¯
            if field in ["poster", "thumb", "extrafanart", "trailer", "outline"]:
                reduced[field + "_from"] = website

            if field == "poster":
                reduced["image_download"] = site_data["image_download"]
            elif field == "thumb":
                # è®°å½•æ‰€æœ‰ thumb url ä»¥ä¾¿åç»­ä¸‹è½½
                reduced["thumb_list"] = reduced.get("thumb_list", []).append((website, site_data["thumb"]))
            elif field == "actor":
                if isinstance(site_data["actor"], list):
                    # å¤„ç† actor ä¸ºåˆ—è¡¨çš„æƒ…å†µ
                    site_data["actor"] = ",".join(site_data["actor"])
                reduced["all_actor"] = reduced.get("all_actor", site_data["actor"])
                reduced["all_actor_photo"] = reduced.get("all_actor_photo", site_data.get("actor_photo", ""))
                # è®°å½•æ‰€æœ‰ç½‘ç«™çš„ actor ç”¨äº Amazon æœå›¾, å› ä¸ºæœ‰çš„ç½‘ç«™ actor ä¸å¯¹
                reduced["actor_amazon"] = reduced.get("actor_amazon", []).extend(site_data["actor"].split(","))
            elif field == "originaltitle" and site_data.get("actor", ""):
                reduced["amazon_orginaltitle_actor"] = site_data["actor"].split(",")[0]

            reduced[field] = site_data[field]
            reduced["fields_info"] += f"\n     {field:<13}: {website} ({language})"
            LogBuffer.info().write(f"\n    ğŸŸ¢ {website} (æˆåŠŸ)\n     â†³ {reduced[field]}")
            break
        else:  # æ‰€æœ‰æ¥æºéƒ½æ— æ­¤å­—æ®µ
            reduced[field] = None
            reduced["fields_info"] += "\n     {field:<13}: {'-----'} ({'not found'})"

    # å¤„ç† year
    if reduced.get("year", "") and (r := re.search(r"\d{4}", reduced.get("release", ""))):
        reduced["year"] = r.group()

    # å¤„ç† numberï¼šç´ äººå½±ç‰‡æ—¶ä½¿ç”¨æœ‰æ•°å­—å‰ç¼€çš„number
    if short_number:
        reduced["number"] = number

    # å¤„ç† javdbid
    r = all_res.get(("javdb", "jp"), {}) or all_res.get(("javdb", "zh_cn"), {}) or all_res.get(("javdb", "zh_tw"), {})
    if r and "javdbid" in r:
        reduced["javdbid"] = r["javdbid"]

    # todo ç”±äºå¼‚æ­¥, æ­¤å¤„æ—¥å¿—æ··ä¹±. éœ€ç§»é™¤ LogBuffer.req(), æ”¹ä¸ºè¿”å›æ—¥å¿—ä¿¡æ¯
    reduced["fields_info"] = f"\n ğŸŒ [website] {LogBuffer.req().get().strip('-> ')}{reduced['fields_info']}"

    return reduced


async def _call_specific_crawler(task_input: JsonData, website: str) -> JsonData:
    file_number = task_input["number"]
    short_number = task_input["short_number"]

    title_language = config.title_language
    org_language = title_language
    outline_language = config.outline_language
    actor_language = config.actor_language
    tag_language = config.tag_language
    series_language = config.series_language
    studio_language = config.studio_language
    publisher_language = config.publisher_language
    director_language = config.director_language
    if website not in ["airav_cc", "iqqtv", "airav", "avsex", "javlibrary", "mdtv", "madouqu", "lulubar"]:
        title_language = "jp"
        outline_language = "jp"
        actor_language = "jp"
        tag_language = "jp"
        series_language = "jp"
        studio_language = "jp"
        publisher_language = "jp"
        director_language = "jp"
    elif website == "mdtv":
        title_language = "zh_cn"
        outline_language = "zh_cn"
        actor_language = "zh_cn"
        tag_language = "zh_cn"
        series_language = "zh_cn"
        studio_language = "zh_cn"
        publisher_language = "zh_cn"
        director_language = "zh_cn"
    web_data = await _call_crawler(task_input, website, title_language, org_language)
    web_data_json = web_data.get(website, {}).get(title_language, {})
    res = task_input.copy()
    res.update(web_data_json)
    if not res["title"]:
        return res
    if outline_language != title_language:
        web_data_json = web_data[website][outline_language]
        if web_data_json["outline"]:
            res["outline"] = web_data_json["outline"]
    if actor_language != title_language:
        web_data_json = web_data[website][actor_language]
        if web_data_json["actor"]:
            res["actor"] = web_data_json["actor"]
    if tag_language != title_language:
        web_data_json = web_data[website][tag_language]
        if web_data_json["tag"]:
            res["tag"] = web_data_json["tag"]
    if series_language != title_language:
        web_data_json = web_data[website][series_language]
        if web_data_json["series"]:
            res["series"] = web_data_json["series"]
    if studio_language != title_language:
        web_data_json = web_data[website][studio_language]
        if web_data_json["studio"]:
            res["studio"] = web_data_json["studio"]
    if publisher_language != title_language:
        web_data_json = web_data[website][publisher_language]
        if web_data_json["publisher"]:
            res["publisher"] = web_data_json["publisher"]
    if director_language != title_language:
        web_data_json = web_data[website][director_language]
        if web_data_json["director"]:
            res["director"] = web_data_json["director"]
    if res["thumb"]:
        res["thumb_list"] = [(website, res["thumb"])]

    # åŠ å…¥æ¥æºä¿¡æ¯
    res["outline_from"] = website
    res["poster_from"] = website
    res["thumb_from"] = website
    res["extrafanart_from"] = website
    res["trailer_from"] = website
    # todo
    res["fields_info"] = f"\n ğŸŒ [website] {LogBuffer.req().get().strip('-> ')}"

    if short_number:
        res["number"] = file_number

    temp_actor = (
        web_data[website]["jp"]["actor"]
        + ","
        + web_data[website]["zh_cn"]["actor"]
        + ","
        + web_data[website]["zh_tw"]["actor"]
    )
    res["actor_amazon"] = []
    [res["actor_amazon"].append(i) for i in temp_actor.split(",") if i and i not in res["actor_amazon"]]
    res["all_actor"] = res["all_actor"] if res.get("all_actor") else web_data_json["actor"]
    res["all_actor_photo"] = res["all_actor_photo"] if res.get("all_actor_photo") else web_data_json["actor_photo"]

    return res


async def _crawl(task_input: JsonData, website_name: str) -> JsonData:  # ä»JSONè¿”å›å…ƒæ•°æ®
    file_number = task_input["number"]
    file_path = task_input["file_path"]
    short_number = task_input["short_number"]
    appoint_number = task_input["appoint_number"]
    appoint_url = task_input["appoint_url"]
    has_sub = task_input["has_sub"]
    c_word = task_input["c_word"]
    leak = task_input["leak"]
    wuma = task_input["wuma"]
    youma = task_input["youma"]
    cd_part = task_input["cd_part"]
    destroyed = task_input["destroyed"]
    mosaic = task_input["mosaic"]
    version = task_input["version"]
    # task_input["title"] = ""
    # task_input["fields_info"] = ""
    # task_input["all_actor"] = ""
    # task_input["all_actor_photo"] = {}
    # ================================================ç½‘ç«™è§„åˆ™æ·»åŠ å¼€å§‹================================================

    if website_name == "all":  # ä»å…¨éƒ¨ç½‘ç«™åˆ®å‰Š
        # =======================================================================å…ˆåˆ¤æ–­æ˜¯ä¸æ˜¯å›½äº§ï¼Œé¿å…æµªè´¹æ—¶é—´
        if (
            mosaic == "å›½äº§"
            or mosaic == "åœ‹ç”¢"
            or (re.search(r"([^A-Z]|^)MD[A-Z-]*\d{4,}", file_number) and "MDVR" not in file_number)
            or re.search(r"MKY-[A-Z]+-\d{3,}", file_number)
        ):
            task_input["mosaic"] = "å›½äº§"
            website_list = config.website_guochan.split(",")
            res = await _call_crawlers(task_input, website_list)

        # =======================================================================kin8
        elif file_number.startswith("KIN8"):
            website_name = "kin8"
            res = await _call_specific_crawler(task_input, website_name)

        # =======================================================================åŒäºº
        elif file_number.startswith("DLID"):
            website_name = "getchu"
            res = await _call_specific_crawler(task_input, website_name)

        # =======================================================================é‡Œç•ª
        elif "getchu" in file_path.lower() or "é‡Œç•ª" in file_path or "è£ç•ª" in file_path:
            website_name = "getchu_dmm"
            res = await _call_specific_crawler(task_input, website_name)

        # =======================================================================Mywife No.1111
        elif "mywife" in file_path.lower():
            website_name = "mywife"
            res = await _call_specific_crawler(task_input, website_name)

        # =======================================================================FC2-111111
        elif "FC2" in file_number.upper():
            file_number_1 = re.search(r"\d{5,}", file_number)
            if file_number_1:
                file_number_1.group()
                website_list = config.website_fc2.split(",")
                res = await _call_crawlers(task_input, website_list)
            else:
                LogBuffer.error().write(f"æœªè¯†åˆ«åˆ°FC2ç•ªå·ï¼š{file_number}")
                res = task_input.copy()

        # =======================================================================sexart.15.06.14
        elif re.search(r"[^.]+\.\d{2}\.\d{2}\.\d{2}", file_number) or (
            "æ¬§ç¾" in file_path and "ä¸œæ¬§ç¾" not in file_path
        ):
            website_list = config.website_oumei.split(",")
            res = await _call_crawlers(task_input, website_list)

        # =======================================================================æ— ç æŠ“å–:111111-111,n1111,HEYZO-1111,SMD-115
        elif mosaic == "æ— ç " or mosaic == "ç„¡ç¢¼":
            website_list = config.website_wuma.split(",")
            res = await _call_crawlers(task_input, website_list)

        # =======================================================================259LUXU-1111
        elif short_number or "SIRO" in file_number.upper():
            website_list = config.website_suren.split(",")
            res = await _call_crawlers(task_input, website_list)

        # =======================================================================ssni00321
        elif re.match(r"\D{2,}00\d{3,}", file_number) and "-" not in file_number and "_" not in file_number:
            website_list = ["dmm"]
            res = await _call_crawlers(task_input, website_list)

        # =======================================================================å‰©ä¸‹çš„ï¼ˆå«åŒ¹é…ä¸äº†ï¼‰çš„æŒ‰æœ‰ç æ¥åˆ®å‰Š
        else:
            website_list = config.website_youma.split(",")
            res = await _call_crawlers(task_input, website_list)
    else:
        res = await _call_specific_crawler(task_input, website_name)

    # ================================================ç½‘ç«™è¯·æ±‚ç»“æŸ================================================
    # ======================================è¶…æ—¶æˆ–æœªæ‰¾åˆ°è¿”å›
    if res["title"] == "":
        return res

    number = res["number"]
    if appoint_number:
        number = appoint_number

    # é©¬èµ›å…‹
    if leak:
        res["mosaic"] = "æ— ç æµå‡º"
    elif destroyed:
        res["mosaic"] = "æ— ç ç ´è§£"
    elif wuma:
        res["mosaic"] = "æ— ç "
    elif youma:
        res["mosaic"] = "æœ‰ç "
    elif mosaic:
        res["mosaic"] = mosaic
    if not res.get("mosaic"):
        if is_uncensored(number):
            res["mosaic"] = "æ— ç "
        else:
            res["mosaic"] = "æœ‰ç "
    print(number, cd_part, res["mosaic"], LogBuffer.req().get().strip("-> "))

    # è½¦ç‰Œå­—æ¯
    letters = get_number_letters(number)

    # åŸæ ‡é¢˜ï¼Œç”¨äºamazonæœç´¢
    originaltitle = res.get("originaltitle") if res.get("originaltitle") else ""
    res["originaltitle_amazon"] = originaltitle
    for each in res["actor_amazon"]:  # å»é™¤æ¼”å‘˜åï¼Œé¿å…æœç´¢ä¸åˆ°
        try:
            end_actor = re.compile(rf" {each}$")
            res["originaltitle_amazon"] = re.sub(end_actor, "", res["originaltitle_amazon"])
        except Exception:
            pass

    # VR æ—¶ä¸‹è½½å°å°é¢
    if "VR" in number:
        res["image_download"] = True

    # è¿”å›å¤„ç†åçš„json_data
    res["number"] = number
    res["letters"] = letters
    res["has_sub"] = has_sub
    res["c_word"] = c_word
    res["leak"] = leak
    res["wuma"] = wuma
    res["youma"] = youma
    res["cd_part"] = cd_part
    res["destroyed"] = destroyed
    res["version"] = version
    res["file_path"] = file_path
    res["appoint_number"] = appoint_number
    res["appoint_url"] = appoint_url

    return res


def _get_website_name(task_input: JsonData, file_mode: FileMode) -> str:
    # è·å–åˆ®å‰Šç½‘ç«™
    website_name = "all"
    if file_mode == FileMode.Single:  # åˆ®å‰Šå•æ–‡ä»¶ï¼ˆå·¥å…·é¡µé¢ï¼‰
        website_name = Flags.website_name
    elif file_mode == FileMode.Again:  # é‡æ–°åˆ®å‰Š
        website_temp = task_input["website_name"]
        if website_temp:
            website_name = website_temp
    elif config.scrape_like == "single":
        website_name = config.website_single

    return website_name


async def crawl(task_input: JsonData, file_mode: FileMode) -> JsonData:
    # ä»æŒ‡å®šç½‘ç«™è·å–json_data
    website_name = _get_website_name(task_input, file_mode)
    res = await _crawl(task_input, website_name)
    return _deal_res(res)


def _deal_res(res: JsonData) -> JsonData:
    # æ ‡é¢˜ä¸ºç©ºè¿”å›
    title = res["title"]
    if not title:
        return res

    # æ¼”å‘˜
    res["actor"] = (
        str(res["actor"])
        .strip(" [ ]")
        .replace("'", "")
        .replace(", ", ",")
        .replace("<", "(")
        .replace(">", ")")
        .strip(",")
    )  # åˆ—è¡¨è½¬å­—ç¬¦ä¸²ï¼ˆé¿å…ä¸ªåˆ«ç½‘ç«™åˆ®å‰Šè¿”å›çš„æ˜¯åˆ—è¡¨ï¼‰

    # æ ‡ç­¾
    tag = (
        str(res["tag"]).strip(" [ ]").replace("'", "").replace(", ", ",")
    )  # åˆ—è¡¨è½¬å­—ç¬¦ä¸²ï¼ˆé¿å…ä¸ªåˆ«ç½‘ç«™åˆ®å‰Šè¿”å›çš„æ˜¯åˆ—è¡¨ï¼‰
    tag = re.sub(r",\d+[kKpP],", ",", tag)
    tag_rep_word = [",HDé«˜ç”»è´¨", ",HDé«˜ç•«è³ª", ",é«˜ç”»è´¨", ",é«˜ç•«è³ª"]
    for each in tag_rep_word:
        if tag.endswith(each):
            tag = tag.replace(each, "")
        tag = tag.replace(each + ",", ",")
    res["tag"] = tag

    # posterå›¾
    if not res.get("poster"):
        res["poster"] = ""

    # å‘è¡Œæ—¥æœŸ
    release = res["release"]
    if release:
        release = release.replace("/", "-").strip(". ")
        if len(release) < 10:
            release_list = re.findall(r"(\d{4})-(\d{1,2})-(\d{1,2})", release)
            if release_list:
                r_year, r_month, r_day = release_list[0]
                r_month = "0" + r_month if len(r_month) == 1 else r_month
                r_day = "0" + r_day if len(r_day) == 1 else r_day
                release = r_year + "-" + r_month + "-" + r_day
    res["release"] = release

    # è¯„åˆ†
    if res.get("score", ""):
        res["score"] = "%.1f" % float(res.get("score", 0))

    # publisher
    if not res.get("publisher", ""):
        res["publisher"] = res["studio"]

    # å­—ç¬¦è½¬ä¹‰ï¼Œé¿å…æ˜¾ç¤ºé—®é¢˜
    key_word = [
        "title",
        "originaltitle",
        "number",
        "outline",
        "originalplot",
        "actor",
        "tag",
        "series",
        "director",
        "studio",
        "publisher",
    ]
    rep_word = {
        "&amp;": "&",
        "&lt;": "<",
        "&gt;": ">",
        "&apos;": "'",
        "&quot;": '"',
        "&lsquo;": "ã€Œ",
        "&rsquo;": "ã€",
        "&hellip;": "â€¦",
        "<br/>": "",
        "ãƒ»": "Â·",
        "â€œ": "ã€Œ",
        "â€": "ã€",
        "...": "â€¦",
        "\xa0": "",
        "\u3000": "",
        "\u2800": "",
    }
    for each in key_word:
        for key, value in rep_word.items():
            res[each] = res[each].replace(key, value)

    # å‘½åè§„åˆ™
    naming_media = config.naming_media
    naming_file = config.naming_file
    folder_name = config.folder_name
    res["naming_media"] = naming_media
    res["naming_file"] = naming_file
    res["folder_name"] = folder_name
    return res
