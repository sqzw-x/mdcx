"""
åˆ®å‰Šè¿‡ç¨‹çš„ç½‘ç»œæ“ä½œ
"""
import os
import re
import shutil
import time
import traceback
import urllib

from lxml import etree

from models.base.file import copy_file, delete_file, move_file, split_path
from models.base.image import check_pic, cut_thumb_to_poster
from models.base.pool import Pool
from models.base.utils import get_used_time
from models.base.web import check_url, get_amazon_data, get_big_pic_by_google, get_html, get_imgsize, multi_download
from models.config.config import config
from models.core.flags import Flags
from models.core.utils import convert_half
from models.signals import signal


def get_actorname(number):
    # è·å–çœŸå®æ¼”å‘˜åå­—
    url = f'https://av-wiki.net/?s={number}'
    result, res = get_html(url)
    if not result:
        return False, f"Error: {res}"
    html_detail = etree.fromstring(res, etree.HTMLParser(encoding='utf-8'))
    actor_box = html_detail.xpath('//ul[@class="post-meta clearfix"]')
    for each in actor_box:
        actor_name = each.xpath('li[@class="actress-name"]/a/text()')
        actor_number = each.xpath('li[@class="actress-name"]/following-sibling::li[last()]/text()')
        if actor_number:
            if actor_number[0].upper().endswith(number.upper()) or number.upper().endswith(actor_number[0].upper()):
                return True, ','.join(actor_name)
    return False, 'No Result!'


def get_yesjav_title(json_data, movie_number):
    yesjav_url = 'http://www.yesjav.info/search.asp?q=%s&' % movie_number
    movie_title = ''
    result, response = get_html(yesjav_url)
    if result and response:
        parser = etree.HTMLParser(encoding="utf-8")
        html = etree.HTML(response, parser)
        movie_title = html.xpath('//dl[@id="zi"]/p/font/a/b[contains(text(), $number)]/../../a[contains(text(), "ä¸­æ–‡å­—å¹•")]/text()', number=movie_number)
        if movie_title:
            movie_title = movie_title[0]
            for each in config.char_list:
                movie_title = movie_title.replace(each, '')
            movie_title = movie_title.strip()
    return movie_title


def google_translate(title, outline):
    e1 = None
    e2 = None
    if title:
        title, e1 = _google_translate(title)
    if outline:
        outline, e2 = _google_translate(outline)
    return title, outline, e1 or e2


def _google_translate(msg: str) -> (str, str):
    try:
        msg_unquote = urllib.parse.unquote(msg)
        url = f'https://translate.google.com/translate_a/single?client=gtx&sl=auto&tl=zh-CN&dt=t&q={msg_unquote}'
        result, response = get_html(url, json_data=True)
        if not result:
            return msg, f'è¯·æ±‚å¤±è´¥ï¼å¯èƒ½æ˜¯è¢«å°äº†ï¼Œå¯å°è¯•æ›´æ¢ä»£ç†ï¼é”™è¯¯ï¼š{response}'
        return "".join([sen[0] for sen in response[0]]), ""
    except Exception as e:
        return msg, str(e)


def download_file_with_filepath(json_data, url, file_path, folder_new_path):
    if not url:
        return False

    if not os.path.exists(folder_new_path):
        os.makedirs(folder_new_path)
    try:
        if multi_download(url, file_path):
            return True
    except:
        pass
    json_data['logs'] += f"\n ğŸ¥º Download failed! {url}"
    return False


def _mutil_extrafanart_download_thread(task):
    json_data, extrafanart_url, extrafanart_file_path, extrafanart_folder_path, extrafanart_name = task
    if download_file_with_filepath(json_data, extrafanart_url, extrafanart_file_path, extrafanart_folder_path):
        if check_pic(extrafanart_file_path):
            return True
    else:
        json_data['logs'] += "\n ğŸ’¡ %s download failed! ( %s )" % (extrafanart_name, extrafanart_url)
        return False


def get_big_pic_by_amazon(json_data, originaltitle_amazon, actor_amazon):
    if not originaltitle_amazon or not actor_amazon:
        return ''
    hd_pic_url = ''
    originaltitle_amazon = re.sub(r'ã€.*ã€‘', '', originaltitle_amazon)
    originaltitle_amazon_list = [originaltitle_amazon]
    for originaltitle_amazon in originaltitle_amazon_list:
        # éœ€è¦ä¸¤æ¬¡urlencodeï¼Œnb_sb_nossè¡¨ç¤ºæ— æ¨èæ¥æº
        url_search = 'https://www.amazon.co.jp/black-curtain/save-eligibility/black-curtain?returnUrl=/s?k=' + urllib.parse.quote_plus(urllib.parse.quote_plus(
            originaltitle_amazon.replace('&', ' ') + ' [DVD]')) + '&ref=nb_sb_noss'
        result, html_search = get_amazon_data(url_search)

        # æ²¡æœ‰ç»“æœï¼Œå°è¯•æ‹†è¯ï¼Œé‡æ–°æœç´¢
        if 'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒæ­£ã—ãå…¥åŠ›ã•ã‚Œã¦ã„ã¦ã‚‚ä¸€è‡´ã™ã‚‹å•†å“ãŒãªã„å ´åˆã¯ã€åˆ¥ã®è¨€è‘‰ã‚’ãŠè©¦ã—ãã ã•ã„ã€‚' in html_search and len(originaltitle_amazon_list) < 2:
            for each_name in originaltitle_amazon.split(' '):
                if each_name not in originaltitle_amazon_list:
                    if len(each_name) > 8 or (not each_name.encode('utf-8').isalnum() and len(each_name) > 4) and each_name not in actor_amazon:
                        originaltitle_amazon_list.append(each_name)
            continue

        # æœ‰ç»“æœæ—¶ï¼Œæ£€æŸ¥ç»“æœ
        if result and html_search:
            html = etree.fromstring(html_search, etree.HTMLParser())
            originaltitle_amazon_half = convert_half(originaltitle_amazon)
            originaltitle_amazon_half_no_actor = originaltitle_amazon_half

            # æ ‡é¢˜ç¼©çŸ­åŒ¹é…ï¼ˆå¦‚æ— ç»“æœï¼Œåˆ™ä½¿ç”¨ç¼©å°æ ‡é¢˜å†æ¬¡æœç´¢ï¼‰
            if 'æ¤œç´¢ã«ä¸€è‡´ã™ã‚‹å•†å“ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚' in html_search and len(originaltitle_amazon_list) < 2:
                short_originaltitle_amazon = html.xpath('//div[@class="a-section a-spacing-base a-spacing-top-base"]/span[@class="a-size-base a-color-base"]/text()')
                if short_originaltitle_amazon:
                    short_originaltitle_amazon = short_originaltitle_amazon[0].upper().replace(' DVD', '')
                    if short_originaltitle_amazon in originaltitle_amazon.upper():
                        originaltitle_amazon_list.append(short_originaltitle_amazon)
                        short_originaltitle_amazon = convert_half(short_originaltitle_amazon)
                        if short_originaltitle_amazon in originaltitle_amazon_half:
                            originaltitle_amazon_half = short_originaltitle_amazon
                for each_name in originaltitle_amazon.split(' '):
                    if each_name not in originaltitle_amazon_list:
                        if len(each_name) > 8 or (not each_name.encode('utf-8').isalnum() and len(each_name) > 4) and each_name not in actor_amazon:
                            originaltitle_amazon_list.append(each_name)

            # æ ‡é¢˜ä¸å¸¦æ¼”å‘˜ååŒ¹é…
            for each_actor in actor_amazon:
                originaltitle_amazon_half_no_actor = originaltitle_amazon_half_no_actor.replace(each_actor.upper(), '')

            # æ£€æŸ¥æœç´¢ç»“æœ
            actor_result_list = set()
            title_result_list = []
            # s-card-container s-overflow-hidden aok-relative puis-wide-grid-style puis-wide-grid-style-t2 puis-expand-height puis-include-content-margin puis s-latency-cf-section s-card-border
            pic_card = html.xpath('//div[@class="a-section a-spacing-base"]')
            for each in pic_card:  # tek-077
                pic_ver_list = each.xpath('div//a[@class="a-size-base a-link-normal s-underline-text s-underline-link-text s-link-style a-text-bold"]/text()')
                pic_title_list = each.xpath('div//span[@class="a-size-base-plus a-color-base a-text-normal"]/text()')
                pic_url_list = each.xpath('div//div[@class="a-section aok-relative s-image-square-aspect"]/img/@src')
                detail_url_list = each.xpath('div//a[@class="a-link-normal s-no-outline"]/@href')
                if len(pic_ver_list) and len(pic_url_list) and (len(pic_title_list) and len(detail_url_list)):
                    pic_ver = pic_ver_list[0]  # å›¾ç‰‡ç‰ˆæœ¬
                    pic_title = pic_title_list[0]  # å›¾ç‰‡æ ‡é¢˜
                    pic_url = pic_url_list[0]  # å›¾ç‰‡é“¾æ¥
                    detail_url = detail_url_list[0]  # è¯¦æƒ…é¡µé“¾æ¥ï¼ˆæœ‰æ—¶å¸¦æœ‰æ¼”å‘˜åï¼‰
                    if pic_ver in ['DVD', 'Software Download'] and '.jpg' in pic_url:  # æ— å›¾æ—¶æ˜¯.gif
                        pic_title_half = convert_half(re.sub(r'ã€.*ã€‘', '', pic_title))
                        pic_title_half_no_actor = pic_title_half
                        for each_actor in actor_amazon:
                            pic_title_half_no_actor = pic_title_half_no_actor.replace(each_actor, '')

                        # åˆ¤æ–­æ ‡é¢˜æ˜¯å¦å‘½ä¸­
                        if originaltitle_amazon_half[:15] in pic_title_half or originaltitle_amazon_half_no_actor[:15] in pic_title_half_no_actor:
                            detail_url = urllib.parse.unquote_plus(detail_url)
                            temp_title = re.findall(r'(.+)keywords=', detail_url)
                            temp_detail_url = temp_title[0] + pic_title_half if temp_title else detail_url + pic_title_half
                            url = re.sub(r'\._[_]?AC_[^\.]+\.', '.', pic_url)

                            # åˆ¤æ–­æ¼”å‘˜æ˜¯å¦åœ¨æ ‡é¢˜é‡Œï¼Œé¿å…åŒåæ ‡é¢˜è¯¯åŒ¹é… MOPP-023
                            for each_actor in actor_amazon:
                                if each_actor in temp_detail_url:
                                    actor_result_list.add(url)
                                    if 'å†™çœŸä»˜ã' not in pic_title:  # NACR-206
                                        w, h = get_imgsize(url)
                                        if w > 600 or not w:
                                            hd_pic_url = url
                                            return hd_pic_url
                                        else:
                                            json_data['poster'] = pic_url  # ç”¨äº Google æœå›¾
                                            json_data['poster_from'] = 'Amazon'
                                    break
                            else:
                                title_result_list.append([url, 'https://www.amazon.co.jp' + detail_url])

            # å‘½ä¸­æ¼”å‘˜æœ‰å¤šä¸ªç»“æœæ—¶è¿”å›æœ€å¤§çš„ï¼ˆä¸ç­‰äº1759/1758ï¼‰
            if len(actor_result_list):
                pic_w = 0
                for each in actor_result_list:
                    new_pic_w = get_imgsize(each)[0]
                    if new_pic_w > pic_w:
                        if new_pic_w >= 1770 or (1750 > new_pic_w > 600):  # ä¸è¦å°å›¾ FCDSS-001ï¼ŒæˆªçŸ­çš„å›¾ï¼ˆ1758/1759ï¼‰
                            pic_w = new_pic_w
                            hd_pic_url = each
                        else:
                            json_data['poster'] = each  # ç”¨äº Google æœå›¾
                            json_data['poster_from'] = 'Amazon'

                if hd_pic_url:
                    return hd_pic_url

            # å½“æœç´¢ç»“æœå‘½ä¸­äº†æ ‡é¢˜ï¼Œæ²¡æœ‰å‘½ä¸­æ¼”å‘˜æ—¶ï¼Œå°è¯•å»è¯¦æƒ…é¡µè·å–æ¼”å‘˜ä¿¡æ¯
            elif len(title_result_list) <= 20 and 's-pagination-item s-pagination-next s-pagination-button s-pagination-separator' not in html_search:
                for each in title_result_list[:4]:
                    try:
                        url_new = 'https://www.amazon.co.jp' + re.findall(r'(/dp/[^/]+)', each[1])[0]
                    except:
                        url_new = each[1]
                    result, html_detail = get_amazon_data(url_new)
                    if result and html_detail:
                        html = etree.fromstring(html_detail, etree.HTMLParser())
                        detail_actor = str(html.xpath('//span[@class="author notFaded"]/a/text()')).replace(' ', '')
                        detail_info_1 = str(html.xpath('//ul[@class="a-unordered-list a-vertical a-spacing-mini"]//text()')).replace(' ', '')
                        detail_info_2 = str(html.xpath('//div[@id="detailBulletsWrapper_feature_div"]//text()')).replace(' ', '')
                        detail_info_3 = str(html.xpath('//div[@id="productDescription"]//text()')).replace(' ', '')
                        all_info = detail_actor + detail_info_1 + detail_info_2 + detail_info_3
                        for each_actor in actor_amazon:
                            if each_actor in all_info:
                                w, h = get_imgsize(each[0])
                                if w > 720 or not w:
                                    return each[0]
                                else:
                                    json_data['poster'] = each[0]  # ç”¨äº Google æœå›¾
                                    json_data['poster_from'] = 'Amazon'

            # æœ‰å¾ˆå¤šç»“æœæ—¶ï¼ˆæœ‰ä¸‹ä¸€é¡µæŒ‰é’®ï¼‰ï¼ŒåŠ æ¼”å‘˜åå­—é‡æ–°æœç´¢
            if 's-pagination-item s-pagination-next s-pagination-button s-pagination-separator' in html_search or len(title_result_list) > 5:
                amazon_orginaltitle_actor = json_data.get('amazon_orginaltitle_actor')
                if amazon_orginaltitle_actor and amazon_orginaltitle_actor not in originaltitle_amazon:
                    originaltitle_amazon_list.append(f'{originaltitle_amazon} {amazon_orginaltitle_actor}')

    return hd_pic_url


def trailer_download(json_data, folder_new_path, folder_old_path, naming_rule):
    start_time = time.time()
    download_files = config.download_files
    keep_files = config.keep_files
    trailer_name = config.trailer_name
    trailer_url = json_data['trailer']
    trailer_old_folder_path = os.path.join(folder_old_path, 'trailers')
    trailer_new_folder_path = os.path.join(folder_new_path, 'trailers')

    # é¢„å‘Šç‰‡åå­—ä¸å«è§†é¢‘æ–‡ä»¶åï¼ˆåªè®©ä¸€ä¸ªè§†é¢‘å»ä¸‹è½½å³å¯ï¼‰
    if trailer_name == 1:
        trailer_folder_path = os.path.join(folder_new_path, 'trailers')
        trailer_file_name = 'trailer.mp4'
        trailer_file_path = os.path.join(trailer_folder_path, trailer_file_name)

        # é¢„å‘Šç‰‡æ–‡ä»¶å¤¹å·²åœ¨å·²å¤„ç†åˆ—è¡¨æ—¶ï¼Œè¿”å›ï¼ˆè¿™æ—¶åªéœ€è¦ä¸‹è½½ä¸€ä¸ªï¼Œå…¶ä»–åˆ†é›†ä¸éœ€è¦ä¸‹è½½ï¼‰
        if trailer_folder_path in Flags.trailer_deal_set:
            return
        Flags.trailer_deal_set.add(trailer_folder_path)

        # ä¸ä¸‹è½½ä¸ä¿ç•™æ—¶åˆ é™¤è¿”å›
        if 'trailer' not in download_files and 'trailer' not in keep_files:
            # åˆ é™¤ç›®æ ‡æ–‡ä»¶å¤¹å³å¯ï¼Œå…¶ä»–æ–‡ä»¶å¤¹å’Œæ–‡ä»¶å·²ç»åˆ é™¤äº†
            if os.path.exists(trailer_folder_path):
                shutil.rmtree(trailer_folder_path, ignore_errors=True)
            return

    else:
        # é¢„å‘Šç‰‡å¸¦æ–‡ä»¶åï¼ˆæ¯ä¸ªè§†é¢‘éƒ½æœ‰æœºä¼šä¸‹è½½ï¼Œå¦‚æœå·²æœ‰ä¸‹è½½å¥½çš„ï¼Œåˆ™ä½¿ç”¨å·²ä¸‹è½½çš„ï¼‰
        trailer_file_name = naming_rule + '-trailer.mp4'
        trailer_folder_path = folder_new_path
        trailer_file_path = os.path.join(trailer_folder_path, trailer_file_name)

        # ä¸ä¸‹è½½ä¸ä¿ç•™æ—¶åˆ é™¤è¿”å›
        if 'trailer' not in download_files and 'trailer' not in keep_files:
            # åˆ é™¤ç›®æ ‡æ–‡ä»¶ï¼Œåˆ é™¤é¢„å‘Šç‰‡æ—§æ–‡ä»¶å¤¹ã€æ–°æ–‡ä»¶å¤¹ï¼ˆdeal old fileæ—¶æ²¡åˆ é™¤ï¼‰
            if os.path.exists(trailer_file_path):
                delete_file(trailer_file_path)
            if os.path.exists(trailer_old_folder_path):
                shutil.rmtree(trailer_old_folder_path, ignore_errors=True)
            if trailer_new_folder_path != trailer_old_folder_path and os.path.exists(trailer_new_folder_path):
                shutil.rmtree(trailer_new_folder_path, ignore_errors=True)
            return

    # é€‰æ‹©ä¿ç•™æ–‡ä»¶ï¼Œå½“å­˜åœ¨æ–‡ä»¶æ—¶ï¼Œä¸ä¸‹è½½ã€‚ï¼ˆdone trailer path æœªè®¾ç½®æ—¶ï¼ŒæŠŠå½“å‰æ–‡ä»¶è®¾ç½®ä¸º done trailer pathï¼Œä»¥ä¾¿å…¶ä»–åˆ†é›†å¤åˆ¶ï¼‰
    if 'trailer' in keep_files and os.path.exists(trailer_file_path):
        if not Flags.file_done_dic.get(json_data['number']).get('trailer'):
            Flags.file_done_dic[json_data['number']].update({'trailer': trailer_file_path})
            # å¸¦æ–‡ä»¶åæ—¶ï¼Œåˆ é™¤æ‰æ–°ã€æ—§æ–‡ä»¶å¤¹ï¼Œç”¨ä¸åˆ°äº†ã€‚ï¼ˆå…¶ä»–åˆ†é›†å¦‚æœæ²¡æœ‰ï¼Œå¯ä»¥å¤åˆ¶ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„é¢„å‘Šç‰‡ã€‚æ­¤æ—¶ä¸åˆ ï¼Œæ²¡æœºä¼šåˆ é™¤äº†ï¼‰
            if trailer_name == 0:
                if os.path.exists(trailer_old_folder_path):
                    shutil.rmtree(trailer_old_folder_path, ignore_errors=True)
                if trailer_new_folder_path != trailer_old_folder_path and os.path.exists(trailer_new_folder_path):
                    shutil.rmtree(trailer_new_folder_path, ignore_errors=True)
        json_data['logs'] += "\n ğŸ€ Trailer done! (old)(%ss) " % get_used_time(start_time)
        return True

    # å¸¦æ–‡ä»¶åæ—¶ï¼Œé€‰æ‹©ä¸‹è½½ä¸ä¿ç•™ï¼Œæˆ–è€…é€‰æ‹©ä¿ç•™ä½†æ²¡æœ‰é¢„å‘Šç‰‡ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–åˆ†é›†å·²ä¸‹è½½æˆ–æœ¬åœ°é¢„å‘Šç‰‡
    # é€‰æ‹©ä¸‹è½½ä¸ä¿ç•™ï¼Œå½“æ²¡æœ‰ä¸‹è½½æˆåŠŸæ—¶ï¼Œä¸ä¼šåˆ é™¤ä¸ä¿ç•™çš„æ–‡ä»¶
    done_trailer_path = Flags.file_done_dic.get(json_data['number']).get('trailer')
    if trailer_name == 0 and done_trailer_path and os.path.exists(done_trailer_path):
        if os.path.exists(trailer_file_path):
            delete_file(trailer_file_path)
        copy_file(done_trailer_path, trailer_file_path)
        json_data['logs'] += '\n ğŸ€ Trailer done! (copy trailer)(%ss)' % get_used_time(start_time)
        return

    # ä¸ä¸‹è½½æ—¶è¿”å›ï¼ˆé€‰æ‹©ä¸ä¸‹è½½ä¿ç•™ï¼Œä½†æœ¬åœ°å¹¶ä¸å­˜åœ¨ï¼Œæ­¤æ—¶è¿”å›ï¼‰
    if 'trailer,' not in download_files:
        return

    # ä¸‹è½½é¢„å‘Šç‰‡,æ£€æµ‹é“¾æ¥æœ‰æ•ˆæ€§
    content_length = check_url(trailer_url, length=True)
    if content_length:
        # åˆ›å»ºæ–‡ä»¶å¤¹
        if trailer_name == 1 and not os.path.exists(trailer_folder_path):
            os.makedirs(trailer_folder_path)

        # å¼€å§‹ä¸‹è½½
        download_files = config.download_files
        signal.show_traceback_log(f"ğŸ” {json_data['number']} download trailer... {trailer_url}")
        trailer_file_path_temp = trailer_file_path
        if os.path.exists(trailer_file_path):
            trailer_file_path_temp = trailer_file_path + '.[DOWNLOAD].mp4'
        if download_file_with_filepath(json_data, trailer_url, trailer_file_path_temp, trailer_folder_path):
            file_size = os.path.getsize(trailer_file_path_temp)
            if file_size >= content_length or 'ignore_size' in download_files:
                json_data['logs'] += "\n ğŸ€ Trailer done! (%s %s/%s)(%ss) " % (json_data['trailer_from'], file_size, content_length, get_used_time(start_time))
                signal.show_traceback_log(f"âœ… {json_data['number']} trailer done!")
                if trailer_file_path_temp != trailer_file_path:
                    move_file(trailer_file_path_temp, trailer_file_path)
                    delete_file(trailer_file_path_temp)
                done_trailer_path = Flags.file_done_dic.get(json_data['number']).get('trailer')
                if not done_trailer_path:
                    Flags.file_done_dic[json_data['number']].update({'trailer': trailer_file_path})
                    if trailer_name == 0:  # å¸¦æ–‡ä»¶åï¼Œå·²ä¸‹è½½æˆåŠŸï¼Œåˆ é™¤æ‰é‚£äº›ä¸ç”¨çš„æ–‡ä»¶å¤¹å³å¯
                        if os.path.exists(trailer_old_folder_path):
                            shutil.rmtree(trailer_old_folder_path, ignore_errors=True)
                        if trailer_new_folder_path != trailer_old_folder_path and os.path.exists(trailer_new_folder_path):
                            shutil.rmtree(trailer_new_folder_path, ignore_errors=True)
                return True
            else:
                json_data['logs'] += "\n ğŸŸ  Trailer size is incorrect! delete it! (%s %s/%s) " % (json_data['trailer_from'], file_size, content_length)
        # åˆ é™¤ä¸‹è½½å¤±è´¥çš„æ–‡ä»¶
        delete_file(trailer_file_path_temp)
        json_data['logs'] += "\n ğŸŸ  Trailer download failed! (%s) " % trailer_url

    if os.path.exists(trailer_file_path):  # ä½¿ç”¨æ—§æ–‡ä»¶
        done_trailer_path = Flags.file_done_dic.get(json_data['number']).get('trailer')
        if not done_trailer_path:
            Flags.file_done_dic[json_data['number']].update({'trailer': trailer_file_path})
            if trailer_name == 0:  # å¸¦æ–‡ä»¶åï¼Œå·²ä¸‹è½½æˆåŠŸï¼Œåˆ é™¤æ‰é‚£äº›ä¸ç”¨çš„æ–‡ä»¶å¤¹å³å¯
                if os.path.exists(trailer_old_folder_path):
                    shutil.rmtree(trailer_old_folder_path, ignore_errors=True)
                if trailer_new_folder_path != trailer_old_folder_path and os.path.exists(trailer_new_folder_path):
                    shutil.rmtree(trailer_new_folder_path, ignore_errors=True)
        json_data['logs'] += "\n ğŸŸ  Trailer download failed! å°†ç»§ç»­ä½¿ç”¨ä¹‹å‰çš„æœ¬åœ°æ–‡ä»¶ï¼"
        json_data['logs'] += "\n ğŸ€ Trailer done! (old)(%ss)" % get_used_time(start_time)
        return True


def _get_big_thumb(json_data):
    """
    è·å–èƒŒæ™¯å¤§å›¾ï¼š
    1ï¼Œå®˜ç½‘å›¾ç‰‡
    2ï¼ŒAmazon å›¾ç‰‡
    3ï¼ŒGoogle æœå›¾
    """
    start_time = time.time()
    if 'thumb' not in config.download_hd_pics:
        return json_data
    number = json_data['number']
    letters = json_data['letters']
    number_lower_line = number.lower()
    number_lower_no_line = number_lower_line.replace('-', '')
    thumb_width = 0

    # faleno.jp ç•ªå·æ£€æŸ¥ï¼Œéƒ½æ˜¯å¤§å›¾ï¼Œè¿”å›å³å¯
    if json_data['cover_from'] in ['faleno', 'dahlia']:
        if json_data['cover']:
            json_data['logs'] += "\n ğŸ–¼ HD Thumb found! (%s)(%ss)" % (json_data['cover_from'], get_used_time(start_time))
        json_data['poster_big'] = True
        return json_data

    # prestige å›¾ç‰‡æœ‰çš„æ˜¯å¤§å›¾ï¼Œéœ€è¦æ£€æµ‹å›¾ç‰‡åˆ†è¾¨ç‡
    elif json_data['cover_from'] in ['prestige', 'mgstage']:
        if json_data['cover']:
            thumb_width, h = get_imgsize(json_data['cover'])

    # ç‰‡å•†å®˜ç½‘æŸ¥è¯¢
    elif 'official' in config.download_hd_pics:
        # faleno.jp ç•ªå·æ£€æŸ¥
        if re.findall(r'F[A-Z]{2}SS', number):
            req_url = 'https://faleno.jp/top/works/%s/' % number_lower_no_line
            result, response = get_html(req_url)
            if result:
                temp_url = re.findall(r'src="((https://cdn.faleno.net/top/wp-content/uploads/[^_]+_)([^?]+))\?output-quality=', response)
                if temp_url:
                    json_data['cover'] = temp_url[0][0]
                    json_data['poster'] = temp_url[0][1] + '2125.jpg'
                    json_data['cover_from'] = 'faleno'
                    json_data['poster_from'] = 'faleno'
                    json_data['poster_big'] = True
                    trailer_temp = re.findall(r'class="btn09"><a class="pop_sample" href="([^"]+)', response)
                    if trailer_temp:
                        json_data['trailer'] = trailer_temp[0]
                        json_data['trailer_from'] = 'faleno'
                    json_data['logs'] += "\n ğŸ–¼ HD Thumb found! (faleno)(%ss)" % get_used_time(start_time)
                    return json_data

        # km-produce.com ç•ªå·æ£€æŸ¥
        number_letter = letters.lower()
        kmp_key = ['vrkm', 'mdtm', 'mkmp', 'savr', 'bibivr', 'scvr', 'slvr', 'averv', 'kbvr', 'cbikmv']
        prestige_key = ['abp', 'abw', 'aka', 'prdvr', 'pvrbst', 'sdvr', 'docvr']
        if number_letter in kmp_key:
            req_url = f'https://km-produce.com/img/title1/{number_lower_line}.jpg'
            real_url = check_url(req_url)
            if real_url:
                json_data['cover'] = real_url
                json_data['cover_from'] = 'km-produce'
                json_data['logs'] += "\n ğŸ–¼ HD Thumb found! (km-produce)(%ss)" % (get_used_time(start_time))
                return json_data

        # www.prestige-av.com ç•ªå·æ£€æŸ¥
        elif number_letter in prestige_key:
            number_num = re.findall(r'\d+', number)[0]
            if number_letter == 'abw' and int(number_num) > 280:
                pass
            else:
                req_url = f'https://www.prestige-av.com/api/media/goods/prestige/{number_letter}/{number_num}/pb_{number_lower_line}.jpg'
                if number_letter == 'docvr':
                    req_url = f'https://www.prestige-av.com/api/media/goods/doc/{number_letter}/{number_num}/pb_{number_lower_line}.jpg'
                if get_imgsize(req_url)[0] >= 800:
                    json_data['cover'] = req_url
                    json_data['poster'] = req_url.replace('/pb_', '/pf_')
                    json_data['cover_from'] = 'prestige'
                    json_data['poster_from'] = 'prestige'
                    json_data['poster_big'] = True
                    json_data['logs'] += "\n ğŸ–¼ HD Thumb found! (prestige)(%ss)" % (get_used_time(start_time))
                    return json_data

    # ä½¿ç”¨googleä»¥å›¾æœå›¾
    pic_url = json_data.get('cover')
    if 'google' in config.download_hd_pics:
        if pic_url and json_data['cover_from'] != 'theporndb':
            thumb_url, cover_size = get_big_pic_by_google(pic_url)
            if thumb_url and cover_size[0] > thumb_width:
                json_data['cover_size'] = cover_size
                pic_domain = re.findall(r'://([^/]+)', thumb_url)[0]
                json_data['cover_from'] = f'Google({pic_domain})'
                json_data['cover'] = thumb_url
                json_data['logs'] += "\n ğŸ–¼ HD Thumb found! (%s)(%ss)" % (json_data['cover_from'], get_used_time(start_time))

    return json_data


def _get_big_poster(json_data):
    start_time = time.time()

    # æœªå‹¾é€‰ä¸‹è½½é«˜æ¸…å›¾posteræ—¶ï¼Œè¿”å›
    if 'poster' not in config.download_hd_pics:
        return json_data

    # å¦‚æœæœ‰å¤§å›¾æ—¶ï¼Œç›´æ¥ä¸‹è½½
    if json_data.get('poster_big') and get_imgsize(json_data['poster'])[1] > 600:
        json_data['image_download'] = True
        json_data['logs'] += f"\n ğŸ–¼ HD Poster found! ({json_data['poster_from']})({get_used_time(start_time)}s)"
        return json_data

    # åˆå§‹åŒ–æ•°æ®
    number = json_data.get('number')
    poster_url = json_data.get('poster')
    hd_pic_url = ''
    poster_width = 0

    # é€šè¿‡åŸæ ‡é¢˜å» amazon æŸ¥è¯¢
    if 'amazon' in config.download_hd_pics and json_data['mosaic'] in ['æœ‰ç ', 'æœ‰ç¢¼', 'æµå‡º', 'æ— ç ç ´è§£', 'ç„¡ç¢¼ç ´è§£', 'é‡Œç•ª', 'è£ç•ª', 'åŠ¨æ¼«', 'å‹•æ¼«']:
        hd_pic_url = get_big_pic_by_amazon(json_data, json_data['originaltitle_amazon'], json_data['actor_amazon'])
        if hd_pic_url:
            json_data['poster'] = hd_pic_url
            json_data['poster_from'] = 'Amazon'
        if json_data['poster_from'] == 'Amazon':
            json_data['image_download'] = True

    # é€šè¿‡ç•ªå·å» å®˜ç½‘ æŸ¥è¯¢è·å–ç¨å¾®å¤§ä¸€äº›çš„å°é¢å›¾ï¼Œä»¥ä¾¿å» Google æœç´¢
    if not hd_pic_url and 'official' in config.download_hd_pics and 'official' not in config.website_set and json_data['poster_from'] != 'Amazon':
        letters = json_data['letters'].upper()
        official_url = config.official_websites.get(letters)
        if official_url:
            url_search = official_url + '/search/list?keyword=' + number.replace('-', '')
            result, html_search = get_html(url_search)
            if result:
                poster_url_list = re.findall(r'img class="c-main-bg lazyload" data-src="([^"]+)"', html_search)
                if poster_url_list:
                    # ä½¿ç”¨å®˜ç½‘å›¾ä½œä¸ºå°é¢å» google æœç´¢
                    poster_url = poster_url_list[0]
                    json_data['poster'] = poster_url
                    json_data['poster_from'] = official_url.split('.')[-2].replace('https://', '')
                    # vrä½œå“æˆ–è€…å®˜ç½‘å›¾ç‰‡é«˜åº¦å¤§äº500æ—¶ï¼Œä¸‹è½½å°é¢å›¾å¼€
                    if 'VR' in number.upper() or get_imgsize(poster_url)[1] > 500:
                        json_data['image_download'] = True

    # ä½¿ç”¨googleä»¥å›¾æœå›¾ï¼Œæ”¾åœ¨æœ€åæ˜¯å› ä¸ºæœ‰æ—¶æœ‰é”™è¯¯ï¼Œæ¯”å¦‚ kawd-943
    poster_url = json_data.get('poster')
    if not hd_pic_url and poster_url and 'google' in config.download_hd_pics and json_data['poster_from'] != 'theporndb':
        hd_pic_url, poster_size = get_big_pic_by_google(poster_url, poster=True)
        if hd_pic_url:
            if 'prestige' in json_data['poster'] or json_data['poster_from'] == 'Amazon':
                poster_width = get_imgsize(poster_url)[0]
            if poster_size[0] > poster_width:
                json_data['poster'] = hd_pic_url
                json_data['poster_size'] = poster_size
                pic_domain = re.findall(r'://([^/]+)', hd_pic_url)[0]
                json_data['poster_from'] = f'Google({pic_domain})'

    # å¦‚æœæ‰¾åˆ°äº†é«˜æ¸…é“¾æ¥ï¼Œåˆ™æ›¿æ¢
    if hd_pic_url:
        json_data['image_download'] = True
        json_data['logs'] += "\n ğŸ–¼ HD Poster found! (%s)(%ss)" % (json_data['poster_from'], get_used_time(start_time))

    return json_data


def thumb_download(json_data, folder_new_path, thumb_final_path):
    start_time = time.time()
    poster_path = json_data['poster_path']
    thumb_path = json_data['thumb_path']
    fanart_path = json_data['fanart_path']

    # æœ¬åœ°å­˜åœ¨ thumb.jpgï¼Œä¸”å‹¾é€‰ä¿ç•™æ—§æ–‡ä»¶æ—¶ï¼Œä¸ä¸‹è½½
    if thumb_path and 'thumb' in config.keep_files:
        json_data['logs'] += "\n ğŸ€ Thumb done! (old)(%ss) " % get_used_time(start_time)
        return True

    # å¦‚æœthumbä¸ä¸‹è½½ï¼Œçœ‹fanartã€posterè¦ä¸è¦ä¸‹è½½ï¼Œéƒ½ä¸ä¸‹è½½åˆ™è¿”å›
    if 'thumb' not in config.download_files:
        if 'poster' in config.download_files and ('poster' not in config.keep_files or not poster_path):
            pass
        elif 'fanart' in config.download_files and ('fanart' not in config.keep_files or not fanart_path):
            pass
        else:
            return True

    # å°è¯•å¤åˆ¶å…¶ä»–åˆ†é›†ã€‚çœ‹åˆ†é›†æœ‰æ²¡æœ‰ä¸‹è½½ï¼Œå¦‚æœä¸‹è½½å®Œæˆåˆ™å¯ä»¥å¤åˆ¶ï¼Œå¦åˆ™å°±è‡ªè¡Œä¸‹è½½
    if json_data['cd_part']:
        done_thumb_path = Flags.file_done_dic.get(json_data['number']).get('thumb')
        if done_thumb_path and os.path.exists(done_thumb_path) and split_path(done_thumb_path)[0] == split_path(thumb_final_path)[0]:
            copy_file(done_thumb_path, thumb_final_path)
            json_data['logs'] += "\n ğŸ€ Thumb done! (copy cd-thumb)(%ss) " % get_used_time(start_time)
            json_data['cover_from'] = 'copy cd-thumb'
            json_data['thumb_path'] = thumb_final_path
            return True

    # è·å–é«˜æ¸…èƒŒæ™¯å›¾
    json_data = _get_big_thumb(json_data)

    # ä¸‹è½½å›¾ç‰‡
    cover_url = json_data.get('cover')
    cover_from = json_data.get('cover_from')
    if cover_url:
        cover_list = json_data['cover_list']
        while [cover_from, cover_url] in cover_list:
            cover_list.remove([cover_from, cover_url])
        cover_list.insert(0, [cover_from, cover_url])

        thumb_final_path_temp = thumb_final_path
        if os.path.exists(thumb_final_path):
            thumb_final_path_temp = thumb_final_path + '.[DOWNLOAD].jpg'
        for each in cover_list:
            if not each[1]:
                continue
            cover_from, cover_url = each
            cover_url = check_url(cover_url)
            if not cover_url:
                json_data['logs'] += "\n ğŸŸ  æ£€æµ‹åˆ° Thumb å›¾ç‰‡å¤±æ•ˆ! è·³è¿‡ï¼(%s)(%ss) " % (cover_from, get_used_time(start_time)) + each[1]
                continue
            json_data['cover_from'] = cover_from
            if download_file_with_filepath(json_data, cover_url, thumb_final_path_temp, folder_new_path):
                cover_size = check_pic(thumb_final_path_temp)
                if cover_size:
                    if not cover_from.startswith('Google') or cover_size == json_data['cover_size'] or (
                            cover_size[0] >= 800 and abs(cover_size[0] / cover_size[1] - json_data['cover_size'][0] / json_data['cover_size'][1]) <= 0.1):
                        # å›¾ç‰‡ä¸‹è½½æ­£å¸¸ï¼Œæ›¿æ¢æ—§çš„ thumb.jpg
                        if thumb_final_path_temp != thumb_final_path:
                            move_file(thumb_final_path_temp, thumb_final_path)
                            delete_file(thumb_final_path_temp)
                        if json_data['cd_part']:
                            dic = {'thumb': thumb_final_path}
                            Flags.file_done_dic[json_data['number']].update(dic)
                        json_data['thumb_marked'] = False  # è¡¨ç¤ºè¿˜æ²¡æœ‰èµ°åŠ æ°´å°æµç¨‹
                        json_data['logs'] += "\n ğŸ€ Thumb done! (%s)(%ss) " % (json_data['cover_from'], get_used_time(start_time))
                        json_data['thumb_path'] = thumb_final_path
                        return True
                    else:
                        delete_file(thumb_final_path_temp)
                        json_data['logs'] += "\n ğŸŸ  æ£€æµ‹åˆ° Thumb åˆ†è¾¨ç‡ä¸å¯¹%s! å·²åˆ é™¤ (%s)(%ss)" % (str(cover_size), cover_from, get_used_time(start_time))
                        continue
                json_data['logs'] += f"\n ğŸŸ  Thumb download failed! {cover_from}: {cover_url} "
    else:
        json_data['logs'] += "\n ğŸŸ  Thumb url is empty! "

    # ä¸‹è½½å¤±è´¥ï¼Œæœ¬åœ°æœ‰å›¾
    if thumb_path:
        json_data['logs'] += "\n ğŸŸ  Thumb download failed! å°†ç»§ç»­ä½¿ç”¨ä¹‹å‰çš„å›¾ç‰‡ï¼"
        json_data['logs'] += "\n ğŸ€ Thumb done! (old)(%ss) " % get_used_time(start_time)
        return True
    else:
        if 'ignore_pic_fail' in config.download_files:
            json_data['logs'] += "\n ğŸŸ  Thumb download failed! (ä½ å·²å‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€) "
            json_data['logs'] += "\n ğŸ€ Thumb done! (none)(%ss)" % get_used_time(start_time)
            return True
        else:
            json_data['logs'] += "\n ğŸ”´ Thumb download failed! ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€ "
            json_data['error_info'] = 'Thumb download failed! ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€'
            return False


def poster_download(json_data, folder_new_path, poster_final_path):
    start_time = time.time()
    download_files = config.download_files
    keep_files = config.keep_files
    poster_path = json_data['poster_path']
    thumb_path = json_data['thumb_path']
    fanart_path = json_data['fanart_path']
    image_cut = ''

    # ä¸ä¸‹è½½posterã€ä¸ä¿ç•™posteræ—¶ï¼Œè¿”å›
    if 'poster' not in download_files and 'poster' not in keep_files:
        if poster_path:
            delete_file(poster_path)
        return True

    # æœ¬åœ°æœ‰posteræ—¶ï¼Œä¸”å‹¾é€‰ä¿ç•™æ—§æ–‡ä»¶æ—¶ï¼Œä¸ä¸‹è½½
    if poster_path and 'poster' in keep_files:
        json_data['logs'] += "\n ğŸ€ Poster done! (old)(%ss)" % get_used_time(start_time)
        return True

    # ä¸ä¸‹è½½æ—¶è¿”å›
    if 'poster' not in download_files:
        return True

    # å°è¯•å¤åˆ¶å…¶ä»–åˆ†é›†ã€‚çœ‹åˆ†é›†æœ‰æ²¡æœ‰ä¸‹è½½ï¼Œå¦‚æœä¸‹è½½å®Œæˆåˆ™å¯ä»¥å¤åˆ¶ï¼Œå¦åˆ™å°±è‡ªè¡Œä¸‹è½½
    if json_data['cd_part']:
        done_poster_path = Flags.file_done_dic.get(json_data['number']).get('poster')
        if done_poster_path and os.path.exists(done_poster_path) and split_path(done_poster_path)[0] == split_path(poster_final_path)[0]:
            copy_file(done_poster_path, poster_final_path)
            json_data['poster_from'] = 'copy cd-poster'
            json_data['poster_path'] = poster_final_path
            json_data['logs'] += "\n ğŸ€ Poster done! (copy cd-poster)(%ss)" % get_used_time(start_time)
            return True

    # å‹¾é€‰å¤åˆ¶ thumbæ—¶ï¼šå›½äº§ï¼Œå¤åˆ¶thumbï¼›æ— ç ï¼Œå‹¾é€‰ä¸è£å‰ªæ—¶ï¼Œä¹Ÿå¤åˆ¶thumb
    if thumb_path:
        mosaic = json_data['mosaic']
        number = json_data['number']
        copy_flag = False
        if number.startswith('FC2'):
            image_cut = 'center'
            if 'ignore_fc2' in download_files:
                copy_flag = True
        elif mosaic == 'å›½äº§' or mosaic == 'åœ‹ç”¢':
            image_cut = 'right'
            if 'ignore_guochan' in download_files:
                copy_flag = True
        elif mosaic == 'æ— ç ' or mosaic == 'ç„¡ç¢¼' or mosaic == 'ç„¡ä¿®æ­£':
            image_cut = 'center'
            if 'ignore_wuma' in download_files:
                copy_flag = True
        elif mosaic == 'æœ‰ç ' or mosaic == 'æœ‰ç¢¼':
            if 'ignore_youma' in download_files:
                copy_flag = True
        if copy_flag:
            copy_file(thumb_path, poster_final_path)
            json_data['poster_marked'] = json_data['thumb_marked']
            json_data['poster_from'] = 'copy thumb'
            json_data['poster_path'] = poster_final_path
            json_data['logs'] += "\n ğŸ€ Poster done! (copy thumb)(%ss)" % get_used_time(start_time)
            return True

    # è·å–é«˜æ¸… poster
    json_data = _get_big_poster(json_data)

    # ä¸‹è½½å›¾ç‰‡
    poster_url = json_data.get('poster')
    poster_from = json_data.get('poster_from')
    poster_final_path_temp = poster_final_path
    if os.path.exists(poster_final_path):
        poster_final_path_temp = poster_final_path + '.[DOWNLOAD].jpg'
    if json_data['image_download']:
        start_time = time.time()
        if download_file_with_filepath(json_data, poster_url, poster_final_path_temp, folder_new_path):
            poster_size = check_pic(poster_final_path_temp)
            if poster_size:
                if not poster_from.startswith('Google') or poster_size == json_data['poster_size'] or 'media-amazon.com' in poster_url:
                    if poster_final_path_temp != poster_final_path:
                        move_file(poster_final_path_temp, poster_final_path)
                        delete_file(poster_final_path_temp)
                    if json_data['cd_part']:
                        dic = {'poster': poster_final_path}
                        Flags.file_done_dic[json_data['number']].update(dic)
                    json_data['poster_marked'] = False  # ä¸‹è½½çš„å›¾ï¼Œè¿˜æ²¡åŠ æ°´å°
                    json_data['poster_path'] = poster_final_path
                    json_data['logs'] += "\n ğŸ€ Poster done! (%s)(%ss)" % (poster_from, get_used_time(start_time))
                    return True
                else:
                    delete_file(poster_final_path_temp)
                    json_data['logs'] += "\n ğŸŸ  æ£€æµ‹åˆ° Poster åˆ†è¾¨ç‡ä¸å¯¹%s! å·²åˆ é™¤ (%s)" % (str(poster_size), poster_from)

    # åˆ¤æ–­ä¹‹å‰æœ‰æ²¡æœ‰ poster å’Œ thumb
    if not poster_path and not thumb_path:
        json_data['poster_path'] = ''
        if 'ignore_pic_fail' in download_files:
            json_data['logs'] += "\n ğŸŸ  Poster download failed! (ä½ å·²å‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€) "
            json_data['logs'] += "\n ğŸ€ Poster done! (none)(%ss)" % get_used_time(start_time)
            return True
        else:
            json_data['logs'] += "\n ğŸ”´ Poster download failed! ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€ "
            json_data['error_info'] = 'Poster download failed! ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€'
            return False

    # ä½¿ç”¨thumbè£å‰ª
    poster_final_path_temp = poster_final_path + '.[CUT].jpg'
    if fanart_path:
        thumb_path = fanart_path
    if cut_thumb_to_poster(json_data, thumb_path, poster_final_path_temp, image_cut):
        # è£å‰ªæˆåŠŸï¼Œæ›¿æ¢æ—§å›¾
        move_file(poster_final_path_temp, poster_final_path)
        if json_data['cd_part']:
            dic = {'poster': poster_final_path}
            Flags.file_done_dic[json_data['number']].update(dic)
        json_data['poster_path'] = poster_final_path
        json_data['poster_marked'] = False
        return True

    # è£å‰ªå¤±è´¥ï¼Œæœ¬åœ°æœ‰å›¾
    if poster_path:
        json_data['logs'] += "\n ğŸŸ  Poster cut failed! å°†ç»§ç»­ä½¿ç”¨ä¹‹å‰çš„å›¾ç‰‡ï¼"
        json_data['logs'] += "\n ğŸ€ Poster done! (old)(%ss) " % get_used_time(start_time)
        return True
    else:
        if 'ignore_pic_fail' in download_files:
            json_data['logs'] += "\n ğŸŸ  Poster cut failed! (ä½ å·²å‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€) "
            json_data['logs'] += "\n ğŸ€ Poster done! (none)(%ss)" % get_used_time(start_time)
            return True
        else:
            json_data['logs'] += "\n ğŸ”´ Poster cut failed! ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€ "
            json_data['error_info'] = 'Poster failedï¼ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€'
            return False


def fanart_download(json_data, fanart_final_path):
    """
    å¤åˆ¶thumbä¸ºfanart
    """
    start_time = time.time()
    thumb_path = json_data['thumb_path']
    fanart_path = json_data['fanart_path']
    download_files = config.download_files
    keep_files = config.keep_files

    # ä¸ä¿ç•™ä¸ä¸‹è½½æ—¶åˆ é™¤è¿”å›
    if ',fanart' not in keep_files and ',fanart' not in download_files:
        if fanart_path and os.path.exists(fanart_path):
            delete_file(fanart_path)
        return True

    # ä¿ç•™ï¼Œå¹¶ä¸”æœ¬åœ°å­˜åœ¨ fanart.jpgï¼Œä¸ä¸‹è½½è¿”å›
    if ',fanart' in keep_files and fanart_path:
        json_data['logs'] += "\n ğŸ€ Fanart done! (old)(%ss)" % get_used_time(start_time)
        return True

    # ä¸ä¸‹è½½æ—¶ï¼Œè¿”å›
    if ',fanart' not in download_files:
        return True

    # å°è¯•å¤åˆ¶å…¶ä»–åˆ†é›†ã€‚çœ‹åˆ†é›†æœ‰æ²¡æœ‰ä¸‹è½½ï¼Œå¦‚æœä¸‹è½½å®Œæˆåˆ™å¯ä»¥å¤åˆ¶ï¼Œå¦åˆ™å°±è‡ªè¡Œä¸‹è½½
    if json_data['cd_part']:
        done_fanart_path = Flags.file_done_dic.get(json_data['number']).get('fanart')
        if done_fanart_path and os.path.exists(done_fanart_path) and split_path(done_fanart_path)[0] == split_path(fanart_final_path)[0]:
            if fanart_path:
                delete_file(fanart_path)
            copy_file(done_fanart_path, fanart_final_path)
            json_data['fanart_from'] = 'copy cd-fanart'
            json_data['fanart_path'] = fanart_final_path
            json_data['logs'] += "\n ğŸ€ Fanart done! (copy cd-fanart)(%ss)" % get_used_time(start_time)
            return True

    # å¤åˆ¶thumb
    if thumb_path:
        if fanart_path:
            delete_file(fanart_path)
        copy_file(thumb_path, fanart_final_path)
        json_data['fanart_from'] = 'copy thumb'
        json_data['fanart_path'] = fanart_final_path
        json_data['fanart_marked'] = json_data['thumb_marked']
        json_data['logs'] += "\n ğŸ€ Fanart done! (copy thumb)(%ss)" % get_used_time(start_time)
        if json_data['cd_part']:
            dic = {'fanart': fanart_final_path}
            Flags.file_done_dic[json_data['number']].update(dic)
        return True
    else:
        # æœ¬åœ°æœ‰ fanart æ—¶ï¼Œä¸ä¸‹è½½
        if fanart_path:
            json_data['logs'] += "\n ğŸŸ  Fanart copy failed! æœªæ‰¾åˆ° thumb å›¾ç‰‡ï¼Œå°†ç»§ç»­ä½¿ç”¨ä¹‹å‰çš„å›¾ç‰‡ï¼"
            json_data['logs'] += "\n ğŸ€ Fanart done! (old)(%ss)" % get_used_time(start_time)
            return True

        else:
            if 'ignore_pic_fail' in download_files:
                json_data['logs'] += "\n ğŸŸ  Fanart failed! (ä½ å·²å‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€) "
                json_data['logs'] += "\n ğŸ€ Fanart done! (none)(%ss)" % get_used_time(start_time)
                return True
            else:
                json_data['logs'] += "\n ğŸ”´ Fanart failed! ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€ "
                json_data['error_info'] = 'Fanart ä¸‹è½½å¤±è´¥ï¼ä½ å¯ä»¥åˆ°ã€Œè®¾ç½®ã€-ã€Œä¸‹è½½ã€ï¼Œå‹¾é€‰ã€Œå›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ï¼Œä¸è§†ä¸ºå¤±è´¥ï¼ã€'
                return False


def extrafanart_download(json_data, folder_new_path):
    start_time = time.time()
    download_files = config.download_files
    keep_files = config.keep_files
    extrafanart_list = json_data.get('extrafanart')
    extrafanart_folder_path = os.path.join(folder_new_path, 'extrafanart')

    # ä¸ä¸‹è½½ä¸ä¿ç•™æ—¶åˆ é™¤è¿”å›
    if 'extrafanart' not in download_files and 'extrafanart' not in keep_files:
        if os.path.exists(extrafanart_folder_path):
            shutil.rmtree(extrafanart_folder_path, ignore_errors=True)
        return

    # æœ¬åœ°å­˜åœ¨ extrafanart_folderï¼Œä¸”å‹¾é€‰ä¿ç•™æ—§æ–‡ä»¶æ—¶ï¼Œä¸ä¸‹è½½
    if 'extrafanart' in keep_files and os.path.exists(extrafanart_folder_path):
        json_data['logs'] += "\n ğŸ€ Extrafanart done! (old)(%ss) " % get_used_time(start_time)
        return True

    # å¦‚æœ extrafanart ä¸ä¸‹è½½
    if 'extrafanart' not in download_files:
        return True

    # æ£€æµ‹é“¾æ¥æœ‰æ•ˆæ€§
    if extrafanart_list and check_url(extrafanart_list[0]):
        extrafanart_folder_path_temp = extrafanart_folder_path
        if os.path.exists(extrafanart_folder_path_temp):
            extrafanart_folder_path_temp = extrafanart_folder_path + '[DOWNLOAD]'
            if not os.path.exists(extrafanart_folder_path_temp):
                os.makedirs(extrafanart_folder_path_temp)
        else:
            os.makedirs(extrafanart_folder_path_temp)

        extrafanart_count = 0
        extrafanart_count_succ = 0
        task_list = []
        for extrafanart_url in extrafanart_list:
            extrafanart_count += 1
            extrafanart_name = 'fanart' + str(extrafanart_count) + '.jpg'
            extrafanart_file_path = os.path.join(extrafanart_folder_path_temp, extrafanart_name)
            task_list.append([json_data, extrafanart_url, extrafanart_file_path, extrafanart_folder_path_temp, extrafanart_name])
        extrafanart_pool = Pool(20)  # å‰§ç…§ä¸‹è½½çº¿ç¨‹æ± 
        result = extrafanart_pool.map(_mutil_extrafanart_download_thread, task_list)
        for res in result:
            if res:
                extrafanart_count_succ += 1
        if extrafanart_count_succ == extrafanart_count:
            if extrafanart_folder_path_temp != extrafanart_folder_path:
                shutil.rmtree(extrafanart_folder_path)
                os.rename(extrafanart_folder_path_temp, extrafanart_folder_path)
            json_data['logs'] += "\n ğŸ€ ExtraFanart done! (%s %s/%s)(%ss)" % (
                json_data['extrafanart_from'], extrafanart_count_succ, extrafanart_count, get_used_time(start_time))
            return True
        else:
            json_data['logs'] += "\n ğŸŸ   ExtraFanart download failed! (%s %s/%s)(%ss)" % (
                json_data['extrafanart_from'], extrafanart_count_succ, extrafanart_count, get_used_time(start_time))
            if extrafanart_folder_path_temp != extrafanart_folder_path:
                shutil.rmtree(extrafanart_folder_path_temp)
            else:
                json_data['logs'] += "\n ğŸ€ ExtraFanart done! (incomplete)(%ss)" % get_used_time(start_time)
                return False
        json_data['logs'] += "\n ğŸŸ  ExtraFanart download failed! å°†ç»§ç»­ä½¿ç”¨ä¹‹å‰çš„æœ¬åœ°æ–‡ä»¶ï¼"
    if os.path.exists(extrafanart_folder_path):  # ä½¿ç”¨æ—§æ–‡ä»¶
        json_data['logs'] += "\n ğŸ€ ExtraFanart done! (old)(%ss)" % get_used_time(start_time)
        return True


def show_netstatus():
    signal.show_net_info(time.strftime('%Y-%m-%d %H:%M:%S').center(80, '='))
    proxy_type = ''
    retry_count = 0
    proxy = ''
    timeout = 0
    try:
        proxy_type, proxy, timeout, retry_count = config.type, config.proxy, config.timeout, config.retry
    except:
        signal.show_traceback_log(traceback.format_exc())
        signal.show_net_info(traceback.format_exc())
    if proxy == '' or proxy_type == '' or proxy_type == 'no':
        signal.show_net_info(' å½“å‰ç½‘ç»œçŠ¶æ€ï¼šâŒ æœªå¯ç”¨ä»£ç†\n   ç±»å‹ï¼š ' + str(proxy_type) + '    åœ°å€ï¼š' + str(proxy) + '    è¶…æ—¶æ—¶é—´ï¼š' + str(timeout) + '    é‡è¯•æ¬¡æ•°ï¼š' + str(
            retry_count))
    else:
        signal.show_net_info(' å½“å‰ç½‘ç»œçŠ¶æ€ï¼šâœ… å·²å¯ç”¨ä»£ç†\n   ç±»å‹ï¼š ' + proxy_type + '    åœ°å€ï¼š' + proxy + '    è¶…æ—¶æ—¶é—´ï¼š' + str(timeout) + '    é‡è¯•æ¬¡æ•°ï¼š' + str(
            retry_count))
    signal.show_net_info('=' * 80)


def check_proxyChange():
    new_proxy = (config.type, config.proxy, config.timeout, config.retry)
    if Flags.current_proxy:
        if new_proxy != Flags.current_proxy:
            signal.show_net_info('\nğŸŒˆ ä»£ç†è®¾ç½®å·²æ”¹å˜ï¼š')
            show_netstatus()
    Flags.current_proxy = new_proxy
