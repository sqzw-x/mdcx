#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import json
import re
import time
import urllib.parse

import urllib3
from lxml import etree

from models.base.web import curl_html
from models.config import config
from models.signals import signal

urllib3.disable_warnings()  # yapf: disable


# import traceback


def get_web_number(html):
    result = html.xpath('//*[contains(text(), "ç•ªè™Ÿ") or contains(text(), "ç•ªå·")]//span/text()')
    return result[0].strip() if result else ''


def get_number(html, number):
    result = html.xpath('//*[contains(text(), "ç•ªè™Ÿ") or contains(text(), "ç•ªå·")]//span/text()')
    num = result[0].strip() if result else ''
    return number if number else num


def get_title(html):
    result = html.xpath('//div[@class="video-title my-3"]/h1/text()')
    result = str(result[0]).strip() if result else ''
    # å»æ‰æ— æ„ä¹‰çš„ç®€ä»‹(é©¬èµ›å…‹ç ´åç‰ˆ)ï¼Œ'å…‹ç ´'ä¸¤å­—ç®€ç¹åŒå½¢
    if not result or 'å…‹ç ´' in result:
        return ''
    return result


def get_actor(html):
    try:
        actor_list = html.xpath('//*[contains(text(), "å¥³å„ª") or contains(text(), "å¥³ä¼˜")]//a/text()')
        result = ','.join(actor_list)
    except:
        result = ''
    return result


def get_actor_photo(actor):
    actor = actor.split(',')
    data = {}
    for i in actor:
        actor_photo = {i: ''}
        data.update(actor_photo)
    return data


def get_studio(html):
    result = html.xpath('//*[contains(text(), "å» å•†") or contains(text(), "å‚å•†")]//a/text()')
    return result[0] if result else ''


def get_release(html):
    result = html.xpath('//i[@class="fa fa-clock me-2"]/../text()')
    if result:
        s = re.search(r'\d{4}-\d{2}-\d{2}', result[0]).group()
        return s if s else ''
    return ''


def get_year(release):
    try:
        result = str(re.search(r'\d{4}', release).group())
        return result
    except:
        return release


def get_tag(html):
    result = html.xpath('//*[contains(text(), "æ¨™ç±¤") or contains(text(), "æ ‡ç±¤")]//a/text()')
    return ','.join(result) if result else ''


def get_cover(html):
    result = html.xpath('//script[@type="application/ld+json"]/text()')[0]
    if result:
        data_dict = json.loads(result)
        result = data_dict.get("thumbnailUrl", "")[0]
    return result if result else ''


def get_outline(html):
    result = html.xpath('//div[@class="video-info"]/p/text()')
    result = str(result[0]).strip() if result else ''
    # å»æ‰æ— æ„ä¹‰çš„ç®€ä»‹(é©¬èµ›å…‹ç ´åç‰ˆ)ï¼Œ'å…‹ç ´'ä¸¤å­—ç®€ç¹åŒå½¢
    if not result or 'å…‹ç ´' in result:
        return ''
    else:
        # å»é™¤ç®€ä»‹ä¸­çš„æ— æ„ä¹‰ä¿¡æ¯ï¼Œä¸­é—´å’Œé¦–å°¾çš„ç©ºç™½å­—ç¬¦ã€*æ ¹æ®åˆ†å‘ç­‰
        result = re.sub(r'[\n\t]', '', result).split('*æ ¹æ®åˆ†å‘', 1)[0].strip()
    return result


def get_series(html):
    result = html.xpath('//*[contains(text(), "ç³»åˆ—")]//a/text()')
    result = result[0] if result else ''
    return result


def retry_request(real_url, log_info, web_info):
    result, html_content = curl_html(real_url)
    if not result:
        debug_info = f'ç½‘ç»œè¯·æ±‚é”™è¯¯: {html_content} '
        log_info += web_info + debug_info
        raise Exception(debug_info)
    html_info = etree.fromstring(html_content, etree.HTMLParser())
    title = get_title(html_info)  # è·å–æ ‡é¢˜
    if not title:
        debug_info = 'æ•°æ®è·å–å¤±è´¥: æœªè·å–åˆ°titleï¼'
        log_info += web_info + debug_info
        raise Exception(debug_info)
    web_number = get_web_number(html_info)  # è·å–ç•ªå·ï¼Œç”¨æ¥æ›¿æ¢æ ‡é¢˜é‡Œçš„ç•ªå·
    web_number1 = f'[{web_number}]'
    title = title.replace(web_number1, '').strip()
    outline = get_outline(html_info)
    actor = get_actor(html_info)  # è·å–actor
    cover_url = get_cover(html_info)  # è·å–cover
    tag = get_tag(html_info)
    studio = get_studio(html_info)
    return html_info, title, outline, actor, cover_url, tag, studio, log_info


def get_real_url(html, number):
    item_list = html.xpath('//div[@class="col oneVideo"]')
    for each in item_list:
        # href="/video?hid=99-21-39624"
        detail_url = each.xpath('.//a/@href')[0]
        title = each.xpath('.//h5/text()')[0]
        # æ³¨æ„å»é™¤é©¬èµ›å…‹ç ´åç‰ˆè¿™ç§å‡ ä¹æ²¡æœ‰æœ‰æ•ˆå­—æ®µçš„æ¡ç›®
        if number.upper() in title and all(keyword not in title for keyword in ['å…‹ç ´', 'æ— ç ç ´è§£', 'ç„¡ç¢¼ç ´è§£']):
            return detail_url
    return ''


def main(number, appoint_url='', log_info='', req_web='', language='zh_cn'):
    start_time = time.time()
    website_name = 'airav_cc'
    req_web += f'-> {website_name}[{language}]'
    number = number.upper()
    if re.match(r'N\d{4}', number):  # n1403
        number = number.lower()
    real_url = appoint_url
    image_cut = 'right'
    image_download = False
    mosaic = 'æœ‰ç '
    airav_url = getattr(config, 'airav_cc_website', 'https://airav.io')
    if language == 'zh_cn':
        airav_url += '/cn'
    web_info = '\n       '
    log_info += f' \n    ğŸŒ airav[{language.replace("zh_", "")}]'

    # real_url = 'https://airav5.fun/jp/playon.aspx?hid=44733'

    try:  # æ•è·ä¸»åŠ¨æŠ›å‡ºçš„å¼‚å¸¸
        if not real_url:

            # é€šè¿‡æœç´¢è·å–real_url https://airav.io/search_result?kw=ssis-200
            url_search = airav_url + f'/search_result?kw={number}'
            debug_info = f'æœç´¢åœ°å€: {url_search} '
            log_info += web_info + debug_info

            # ========================================================================æœç´¢ç•ªå·
            result, html_search = curl_html(url_search)
            if not result:
                debug_info = f'ç½‘ç»œè¯·æ±‚é”™è¯¯: {html_search} '
                log_info += web_info + debug_info
                raise Exception(debug_info)
            html = etree.fromstring(html_search, etree.HTMLParser())
            real_url = html.xpath('//div[@class="col oneVideo"]//a[@href]/@href')
            # if real_url:
            #     real_url = airav_url + '/' + real_url[0]
            # else:
            # æ²¡æœ‰æœç´¢ç»“æœ
            if not real_url:
                debug_info = 'æœç´¢ç»“æœ: æœªåŒ¹é…åˆ°ç•ªå·ï¼'
                log_info += web_info + debug_info
                raise Exception(debug_info)

        if real_url:
            # åªæœ‰ä¸€ä¸ªæœç´¢ç»“æœæ—¶ç›´æ¥å–å€¼ å¤šä¸ªåˆ™è¿›å…¥åˆ¤æ–­
            real_url = real_url[0] if len(real_url) == 1 else get_real_url(html, number)
            # æœç´¢ç»“æœé¡µé¢æœ‰æ¡ç›®ï¼Œä½†æ— æ³•åŒ¹é…åˆ°ç•ªå·
            if not real_url:
                debug_info = 'æœç´¢ç»“æœ: æœªåŒ¹é…åˆ°ç•ªå·ï¼'
                log_info += web_info + debug_info
                raise Exception(debug_info)
            else:
                real_url = urllib.parse.urljoin(airav_url, real_url) if real_url.startswith("/") else real_url

            debug_info = f'ç•ªå·åœ°å€: {real_url} '
            log_info += web_info + debug_info
            for i in range(3):
                html_info, title, outline, actor, cover_url, tag, studio, log_info = (retry_request(real_url, log_info, web_info))

                if cover_url.startswith("/"):  # coverurl å¯èƒ½æ˜¯ç›¸å¯¹è·¯å¾„
                    cover_url = urllib.parse.urljoin(airav_url, cover_url)

                temp_str = title + outline + actor + tag + studio
                if 'ï¿½' not in temp_str:
                    break
                else:
                    debug_info = f'{number} è¯·æ±‚ airav_cc è¿”å›å†…å®¹å­˜åœ¨ä¹±ç  ï¿½ï¼Œå°è¯•ç¬¬ {(i + 1)}/3 æ¬¡è¯·æ±‚'
                    signal.add_log(debug_info)
                    log_info += web_info + debug_info
            else:
                debug_info = f'{number} å·²è¯·æ±‚ä¸‰æ¬¡ï¼Œè¿”å›å†…å®¹ä»å­˜åœ¨ä¹±ç  ï¿½ ï¼è§†ä¸ºå¤±è´¥ï¼'
                signal.add_log(debug_info)
                log_info += web_info + debug_info
                raise Exception(debug_info)
            actor_photo = get_actor_photo(actor)
            number = get_number(html_info, number)
            release = get_release(html_info)
            year = get_year(release)
            runtime = ''
            score = ''
            series = get_series(html_info)
            director = ''
            publisher = ''
            extrafanart = ''
            if 'æ— ç ' in tag or 'ç„¡ä¿®æ­£' in tag or 'ç„¡ç ' in tag or 'uncensored' in tag.lower():
                mosaic = 'æ— ç '
            title_rep = ['ç¬¬ä¸€é›†', 'ç¬¬äºŒé›†', ' - ä¸Š', ' - ä¸‹', ' ä¸Šé›†', ' ä¸‹é›†', ' -ä¸Š', ' -ä¸‹']
            for each in title_rep:
                title = title.replace(each, '').strip()
            try:
                dic = {
                    'number': number,
                    'title': title,
                    'originaltitle': title,
                    'actor': actor,
                    'outline': outline,
                    'originalplot': outline,
                    'tag': tag,
                    'release': release,
                    'year': year,
                    'runtime': runtime,
                    'score': score,
                    'series': series,
                    'director': director,
                    'studio': studio,
                    'publisher': publisher,
                    'source': 'airav_cc',
                    'actor_photo': actor_photo,
                    'cover': cover_url,
                    'poster': cover_url.replace('big_pic', 'small_pic'),
                    'extrafanart': extrafanart,
                    'trailer': '',
                    'image_download': image_download,
                    'image_cut': image_cut,
                    'log_info': log_info,
                    'error_info': '',
                    'req_web': req_web + f'({round((time.time() - start_time), )}s) ',
                    'mosaic': mosaic,
                    'website': real_url,
                    'wanted': '',
                }
                debug_info = 'æ•°æ®è·å–æˆåŠŸï¼'
                log_info += web_info + debug_info
                dic['log_info'] = log_info
            except Exception as e:
                debug_info = f'æ•°æ®ç”Ÿæˆå‡ºé”™: {str(e)}'
                log_info += web_info + debug_info
                raise Exception(debug_info)
    except Exception as e:
        # print(traceback.format_exc())
        debug_info = str(e)
        dic = {
            'title': '',
            'cover': '',
            'website': '',
            'log_info': log_info,
            'error_info': debug_info,
            'req_web': req_web + f'({round((time.time() - start_time), )}s) ',
        }
    dic = {website_name: {'zh_cn': dic, 'zh_tw': dic, 'jp': dic}}
    js = json.dumps(dic, ensure_ascii=False, sort_keys=False, indent=4, separators=(',', ': '), )  # .encode('UTF-8')
    return js


if __name__ == '__main__':
    # yapf: disable
    # print(main('', 'https://airav.io/playon.aspx?hid=99-21-46640'))
    # print(main('PRED-300'))    # é©¬èµ›å…‹ç ´åç‰ˆ
    # print(main('snis-036', language='jp'))
    # print(main('snis-036'))
    # print(main('MIAE-346'))
    # print(main('STARS-1919'))    # posterå›¾ç‰‡
    # print(main('abw-157'))
    # print(main('abs-141'))
    # print(main('HYSD-00083'))
    # print(main('IESP-660'))
    # print(main('n1403'))
    # print(main('GANA-1910'))
    # print(main('heyzo-1031'))
    # print(main('x-art.19.11.03'))
    # print(main('032020-001'))
    # print(main('S2M-055'))
    # print(main('LUXU-1217'))
    # print(main('1101132', ''))
    # print(main('OFJE-318'))
    # print(main('110119-001'))
    # print(main('abs-001'))
    # print(main('SSIS-090', ''))
    # print(main('SSIS-090', ''))
    # print(main('SNIS-016', ''))
    # print(main('HYSD-00083', ''))
    # print(main('IESP-660', ''))
    # print(main('n1403', ''))
    # print(main('GANA-1910', ''))
    # print(main('heyzo-1031', ''))
    # print(main('x-art.19.11.03'))
    # print(main('032020-001', ''))
    # print(main('S2M-055', ''))
    # print(main('LUXU-1217', ''))
    # print(main('x-art.19.11.03', ''))
    # print(main('ssis-200', ''))     # å¤šä¸ªæœç´¢ç»“æœ
    # print(main('JUY-331', ''))      # å­˜åœ¨ç³»åˆ—å­—æ®µ
    # print(main('SONE-248', ''))      # ç®€ä»‹å­˜åœ¨æ— æ•ˆä¿¡æ¯  "*æ ¹æ®åˆ†å‘æ–¹å¼,å†…å®¹å¯èƒ½ä¼šæœ‰æ‰€ä¸åŒ"
    print('CAWD-688', '')  # æ— ç ç ´è§£
