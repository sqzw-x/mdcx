#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import re
import time
import urllib

import unicodedata
import urllib3
from lxml import etree

from models.base.web import get_html
from models.crawlers import getchu_dl

urllib3.disable_warnings()  # yapf: disable


# import traceback


def get_web_number(html, number):
    result = html.xpath('//td[contains(text(), "å“ç•ªï¼š")]/following-sibling::td/text()')
    return result[0].strip().upper() if result else number


def get_title(html):
    result = html.xpath('//h1[@id="soft-title"]/text()')
    return result[0].strip() if result else ''


def get_studio(html):
    result = html.xpath('//a[@class="glance"]/text()')
    return result[0] if result else ''


def get_release(html):
    result = html.xpath("//td[contains(text(),'ç™ºå£²æ—¥ï¼š')]/following-sibling::td/a/text()")
    return result[0].replace('/', '-') if result and re.search(r'\d+', result[0]) else ''


def get_year(release):
    try:
        result = str(re.search(r'\d{4}', release).group())
        return result
    except:
        return release


def get_director(html):
    result = html.xpath("//td[contains(text(),'ç›£ç£ï¼š')]/following-sibling::td/text()")
    if not result:
        result = html.xpath("//a[contains(@href,'person=')]/text()")
    if not result:
        result = html.xpath("//td[contains(text(),'ã‚­ãƒ£ãƒ©ãƒ‡ã‚¶ã‚¤ãƒ³ï¼š')]/following-sibling::td/text()")
    return result[0] if result else ''


def get_runtime(html):
    result = html.xpath("//td[contains(text(),'æ™‚é–“ï¼š')]/following-sibling::td/text()")
    if result:
        result = re.findall(r'\d*', result[0])
    return result[0] if result else ''


def get_tag(html):
    result = html.xpath("//td[contains(text(), 'ã‚µãƒ–ã‚¸ãƒ£ãƒ³ãƒ«ï¼š') or contains(text(), 'ã‚«ãƒ†ã‚´ãƒªï¼š')]/following-sibling::td/a/text()")
    return ','.join(result).replace(',[ä¸€è¦§]', '') if result else ''


def get_cover(html):
    result = html.xpath('//meta[@property="og:image"]/@content')
    if result:
        return 'http://www.getchu.com' + result[0] if 'http' not in result[0] else result[0]
    return ''


def get_outline(html):
    all_info = html.xpath('//div[@class="tablebody"]')
    result = ''
    for each in all_info:
        info = each.xpath('normalize-space(string())')
        result += '\n' + info
    return result.strip()


def get_mosaic(html, mosaic):
    result = html.xpath('//li[@class="genretab current"]/text()')
    if result:
        r = result[0]
        if r == 'ã‚¢ãƒ€ãƒ«ãƒˆã‚¢ãƒ‹ãƒ¡':
            mosaic = 'é‡Œç•ª'
        elif r == 'æ›¸ç±ãƒ»é›‘èªŒ':
            mosaic = 'ä¹¦ç±'
        elif r == 'ã‚¢ãƒ‹ãƒ¡':
            mosaic = 'åŠ¨æ¼«'

    return mosaic


def get_extrafanart(html):
    result_list = html.xpath("//div[contains(text(),'ã‚µãƒ³ãƒ—ãƒ«ç”»åƒ')]/following-sibling::div[1]/a/@href")
    result = []
    for each in result_list:
        each = each.replace('./', 'http://www.getchu.com/')
        result.append(each)
    return result


def main(number, appoint_url='', log_info='', req_web='', language='jp'):
    if 'DLID' in number.upper() or 'ITEM' in number.upper() or 'GETCHU' in number.upper() or 'dl.getchu' in appoint_url:
        return getchu_dl.main(number, appoint_url, log_info, req_web, 'jp')
    start_time = time.time()
    website_name = 'getchu'
    getchu_url = 'http://www.getchu.com'
    req_web += '-> %s' % website_name
    real_url = appoint_url.replace('&gc=gc', '') + '&gc=gc' if appoint_url else ''
    cover_url = ''
    image_cut = ''
    image_download = True
    url_search = ''
    web_info = '\n       '
    log_info += ' \n    ğŸŒ getchu'
    debug_info = ''

    # real_url = 'http://www.getchu.com/soft.phtml?id=1141110&gc=gc'
    # real_url = 'http://www.getchu.com/soft.phtml?id=1178713&gc=gc'
    # real_url = 'http://www.getchu.com/soft.phtml?id=1007200&gc=gc'

    try:  # æ•è·ä¸»åŠ¨æŠ›å‡ºçš„å¼‚å¸¸
        if not real_url:
            number = number.replace('10bit', '').replace('è£•æœª', 'ç¥æœª').replace('â€œ', 'â€').replace('Â·', 'ãƒ»')

            keyword = unicodedata.normalize('NFC', number)  # Mac ä¼šæ‹†æˆä¸¤ä¸ªå­—ç¬¦ï¼Œå³ NFDï¼Œè€Œç½‘é¡µè¯·æ±‚ä½¿ç”¨çš„æ˜¯ NFC
            try:
                keyword = keyword.encode('cp932').decode('shift_jis')  # è½¬æ¢ä¸ºå¸¸è§æ—¥æ–‡ï¼Œæ¯”å¦‚ï½ è½¬æ¢æˆ ã€œ
            except:
                pass
            keyword2 = urllib.parse.quote_plus(keyword,
                                               encoding="EUC-JP")  # quote() ä¸ç¼–ç æ–œçº¿ï¼Œç©ºæ ¼â€˜ â€™ç¼–ç ä¸ºâ€˜%20â€™ï¼›quote_plus() ä¼šç¼–ç æ–œçº¿ä¸ºâ€˜%2Fâ€™; ç©ºæ ¼â€˜ â€™ç¼–ç ä¸ºâ€˜+â€™
            url_search = f'http://www.getchu.com/php/search.phtml?genre=all&search_keyword={keyword2}&gc=gc'
            # http://www.getchu.com/php/search.phtml?genre=anime_dvd&search_keyword=_WORD_&check_key_dtl=1&submit=&genre=anime_dvd&gc=gc
            debug_info = f'æœç´¢åœ°å€: {url_search} '
            log_info += web_info + debug_info

            # ========================================================================æœç´¢ç•ªå·
            result, html_search = get_html(url_search, encoding='euc-jp', timeout=40)
            if not result:
                debug_info = f'ç½‘ç»œè¯·æ±‚é”™è¯¯: {html_search} '
                log_info += web_info + debug_info
                raise Exception(debug_info)
            html = etree.fromstring(html_search, etree.HTMLParser())
            url_list = html.xpath("//a[@class='blueb']/@href")
            title_list = html.xpath("//a[@class='blueb']/text()")

            if url_list:
                real_url = getchu_url + url_list[0].replace('../', '/') + '&gc=gc'
                keyword_temp = re.sub(r'[ \[\]\ï¼»\ï¼½]+', '', keyword)
                for i in range(len(url_list)):
                    title_temp = re.sub(r'[ \[\]\ï¼»\ï¼½]+', '', title_list[i])
                    if keyword_temp in title_temp:  # æœ‰å¤šä¸ªåˆ†é›†æ—¶ï¼Œç”¨æ ‡é¢˜ç¬¦åˆçš„
                        real_url = getchu_url + url_list[i].replace('../', '/') + '&gc=gc'
                        break
            else:
                debug_info = 'æœç´¢ç»“æœ: æœªåŒ¹é…åˆ°ç•ªå·ï¼'
                log_info += web_info + debug_info
                return getchu_dl.main(number, appoint_url, log_info, req_web, 'jp')

        if real_url:
            debug_info = f'ç•ªå·åœ°å€: {real_url} '
            log_info += web_info + debug_info

            result, html_content = get_html(real_url, encoding='euc-jp', timeout=40)
            if not result:
                debug_info = f'ç½‘ç»œè¯·æ±‚é”™è¯¯: {html_content} '
                log_info += web_info + debug_info
                raise Exception(debug_info)
            html_info = etree.fromstring(html_content, etree.HTMLParser())
            title = get_title(html_info)
            if not title:
                debug_info = 'æ•°æ®è·å–å¤±è´¥: æœªè·å–åˆ°titleï¼'
                log_info += web_info + debug_info
                raise Exception(debug_info)
            outline = get_outline(html_info)
            actor = ''
            actor_photo = {'': ''}
            cover_url = get_cover(html_info)
            number = get_web_number(html_info, number)
            tag = get_tag(html_info)
            studio = get_studio(html_info)
            release = get_release(html_info)
            year = get_year(release)
            runtime = get_runtime(html_info)
            score = ''
            series = ''
            director = get_director(html_info)
            publisher = ''
            extrafanart = get_extrafanart(html_info)
            mosaic = 'åŠ¨æ¼«'
            if '18ç¦' in html_content:
                mosaic = 'é‡Œç•ª'
            mosaic = get_mosaic(html_info, mosaic)
            try:
                dic = {
                    'number': number,
                    'title': title,
                    'originaltitle': title,
                    'actor': actor,
                    'outline': outline,
                    'originalplot': outline,
                    'tag': tag,
                    'release': release,
                    'year': year,
                    'runtime': runtime,
                    'score': score,
                    'series': series,
                    'director': director,
                    'studio': studio,
                    'publisher': publisher,
                    'source': 'getchu',
                    'actor_photo': actor_photo,
                    'cover': cover_url,
                    'poster': cover_url,
                    'extrafanart': extrafanart,
                    'trailer': '',
                    'image_download': image_download,
                    'image_cut': image_cut,
                    'log_info': log_info,
                    'error_info': '',
                    'req_web': req_web + '(%ss) ' % (round((time.time() - start_time), )),
                    'mosaic': mosaic,
                    'website': real_url,
                    'wanted': '',
                }
                debug_info = 'æ•°æ®è·å–æˆåŠŸï¼'
                log_info += web_info + debug_info
                dic['log_info'] = log_info
            except Exception as e:
                debug_info = 'æ•°æ®ç”Ÿæˆå‡ºé”™: %s' % str(e)
                log_info += web_info + debug_info
                raise Exception(debug_info)
    except Exception as e:

        # print(traceback.format_exc())
        debug_info = str(e)
        dic = {
            'title': '',
            'cover': '',
            'website': '',
            'log_info': log_info,
            'error_info': debug_info,
            'req_web': req_web + '(%ss) ' % (round((time.time() - start_time), )),
        }
    dic = {website_name: {'zh_cn': dic, 'zh_tw': dic, 'jp': dic}}
    js = json.dumps(
        dic,
        ensure_ascii=False,
        sort_keys=False,
        indent=4,
        separators=(',', ': '),
    )
    return js


if __name__ == '__main__':
    # yapf: disable
    # print(main('ã‚³ãƒ³ãƒ“ãƒ‹â—‹â—‹Z ç¬¬ä¸‰è©± ã‚ãªãŸã€ãƒ¤ãƒ³ã‚¯ãƒ¬ãƒãƒã§ã™ã‚ˆã­ã€‚æ—¦é‚£ã«ä¸‡å¼•ããŒãƒãƒ¬ã¦ã„ã„ã‚“ã§ã™ã‹ï¼Ÿ'))
    # print(main('dokidokiã‚Šã¨ã‚‹å¤§å®¶ã•ã‚“ ãŠå®¶è³ƒ6çªãç›® å¦–ã—ã„è¸Šã‚Šã§æ‚ªéœŠç¥“ã„ï¼ã€å©¦è­¦ã€ã•ã‚“ã®ãã‚ã©ã„ã‚ªã‚·ã‚ªã‚­'))
    # print(main('[PoRO]ã‚¨ãƒ­ã‚³ãƒ³ãƒ’ã‚™ãƒ‹åº—é•· æ³£ãã¸ã‚™ãè“®ã£è‘‰ãƒ»æ ï½ãŠä»•ç½®ãã—ã‚™ã‡ã‚‰ã—ãƒãƒŠãƒé€¸æ©Ÿï½'))
    print(main('äººå¦»ã€èœœã¨è‚‰ ç¬¬äºŒå·»ï¼»æœˆé‡å®šè¦ï¼½'))
    # print(main('ACHDL-1159'))
    # print(main('å¥½ãã«ã—ã‚„ãŒã‚Œ GOTcomics'))    # æ›¸ç±ï¼Œæ²¡æœ‰ç•ªå·
    # print(main('ã‚ã¾ã‚ã¾ãƒ­â—ãƒ¼ã‚¿å¥³è£…ç”·å­ãƒ¬ã‚º ã‚­ã‚¹ãƒ»ãƒ•ã‚§ãƒ©ãƒ»69ã‹ã‚‰ã®3Pä»‹å…¥'))
    # print(main('DLID4033023'))
    # print(main('', appoint_url='https://dl.getchu.com/i/item4033023'))
    # print(main('ACMDP-1005')) # æœ‰æ—¶é—´ã€å¯¼æ¼”ï¼Œä¸Šä¸‹é›†ACMDP-1005B
    # print(main('ISTU-5391'))
    # print(main('INH-392'))
    # print(main('ISTU-5391', appoint_url='http://www.getchu.com/soft.phtml?id=1180483'))
    # print(main('SPYÃ—FAMILY Vol.1 Blu-ray Discï¼œåˆå›ç”Ÿç”£é™å®šç‰ˆï¼'))    # dmm æ²¡æœ‰
