#!/usr/bin/env python3
import json
import re
import time
from datetime import datetime

import requests
import urllib3
from lxml import etree

from models.base.web import curl_html
from models.config.config import config

urllib3.disable_warnings()  # yapf: disable


def get_actor_photo(actor):
    """
    è·å–æ¼”å‘˜ç…§ç‰‡ä¿¡æ¯çš„å­—å…¸
    :param actor: æ¼”å‘˜åå­—ç¬¦ä¸²ï¼Œä»¥é€—å·åˆ†éš”
    :return: åŒ…å«æ¼”å‘˜åå’Œç…§ç‰‡URLçš„å­—å…¸
    """
    actor = actor.split(",")
    data = {}
    for i in actor:
        actor_photo = {i: ""}
        data.update(actor_photo)
    return data


def get_detail_info(html, number, file_path):
    """
    ä»è¯¦æƒ…é¡µHTMLä¸­æå–å½±ç‰‡ä¿¡æ¯
    :param html: HTMLè§£æåçš„å…ƒç´ æ ‘
    :param number: å½±ç‰‡ç•ªå·
    :param file_path: æ–‡ä»¶è·¯å¾„
    :return: æå–çš„å„ç§å…ƒæ•°æ®
    """
    # è§£ææ ‡é¢˜
    title_h1 = html.xpath('//meta[@property="og:title"]/@content')
    title = title_h1[0] if title_h1 else number

    # è§£æå‘è¡Œæ—¥æœŸ
    release_date = ""
    release_nodes = html.xpath(
        '//div[contains(@class, "field--name-field-video-date")]//time/@datetime'
    )
    if release_nodes:
        try:
            date_text = release_nodes[0]
            date_obj = datetime.strptime(date_text.split("T")[0], "%Y-%m-%d")
            release = date_obj.strftime("%Y-%m-%d")
            year = str(date_obj.year)
        except:
            release = ""
            year = ""
    else:
        release = ""
        year = ""

    # è§£ææ¼”å‘˜ä¿¡æ¯
    actors = html.xpath(
        '//div[contains(@class, "field--name-field-video-actor")]//div[@class="field--item"]/a/text()'
    )
    actor = ",".join([a.strip().lstrip("#") for a in actors]) if actors else ""

    # è§£ææ ‡ç­¾
    tags = html.xpath(
        '//div[contains(@class, "field--name-field-video-tags")]//div[@class="field--item"]/a/text()'
    )
    tag = ",".join([t.strip().lstrip("#") for t in tags]) if tags else ""

    # è§£æå°é¢å›¾ç‰‡
    cover_url = html.xpath('//meta[@property="og:image"]/@content')
    cover_url = cover_url[0] if cover_url else ""

    # è§£æé¢‘é“/ç±»åˆ«
    studio = html.xpath(
        '//div[contains(@class, "field--name-field-video-channel")]//div[@class="field--item"]/a/text()'
    )
    studio = studio[0].strip() if studio else ""

    # è§£æå½±ç‰‡ç±»å‹ï¼ˆæ— ç ã€æœ‰ç ç­‰ï¼‰
    video_type = html.xpath(
        '//div[contains(@class, "field--name-field-video-type")]//div[@class="field--item"]/a/text()'
    )
    video_type = video_type[0].strip() if video_type else ""

    # è§£ææè¿°
    description = html.xpath('//meta[@name="description"]/@content')
    outline = description[0] if description else ""

    # è§£æè§†é¢‘é“¾æ¥
    video_url = ""
    video_scripts = html.xpath('//script[contains(text(), "m3u8")]/text()')
    for script in video_scripts:
        m3u8_match = re.search(r'source\s*=\s*[\'"]([^"\']+\.m3u8)[\'"]', script)
        if m3u8_match:
            video_url = m3u8_match.group(1)
            break

    return (
        number,
        title,
        actor,
        cover_url,
        studio,
        release,
        year,
        tag,
        outline,
        video_url,
        video_type,
    )


def get_real_url(html, number_list):
    """
    ä»æœç´¢ç»“æœé¡µé¢ä¸­è·å–æœ€åŒ¹é…çš„è¯¦æƒ…é¡µURL
    :param html: HTMLè§£æåçš„å…ƒç´ æ ‘
    :param number_list: è¦åŒ¹é…çš„ç•ªå·åˆ—è¡¨
    :return: åŒ¹é…ç»“æœçŠ¶æ€ã€åŒ¹é…çš„ç•ªå·ã€æ ‡é¢˜å’ŒURL
    """
    item_list = html.xpath('//div[contains(@class, "imgcover")]')

    for item in item_list:
        detail_url = item.xpath(".//a/@href")
        if not detail_url:
            continue

        detail_url = detail_url[0]
        title = item.xpath("../h3/text()")
        if not title:
            continue

        title = title[0].strip()

        # æ¯”è¾ƒæ ‡é¢˜ä¸ç•ªå·æ˜¯å¦åŒ¹é…
        for n in number_list:
            temp_n = re.sub(r"[\W_]", "", n).upper()
            temp_title = re.sub(r"[\W_]", "", title).upper()
            if temp_n in temp_title:
                return True, n, title, detail_url

    return False, "", "", ""


def search(keyword, page=1):
    """
    æœç´¢åŠŸèƒ½å®ç°
    :param keyword: æœç´¢å…³é”®è¯
    :param page: é¡µç 
    :return: æœç´¢ç»“æœåˆ—è¡¨
    """
    av911_url = "https://av911.tv"
    search_url = f"{av911_url}/serach?fulltext={keyword}&page={page-1}"
    result, response = curl_html(search_url)

    if not result:
        return []

    html = etree.fromstring(response, etree.HTMLParser())
    results = []

    item_list = html.xpath('//div[contains(@class, "imgcover")]')

    for item in item_list:
        result = {}

        link = item.xpath(".//a/@href")
        if link:
            result["url"] = av911_url + link[0]

        title = item.xpath("../h3/text()")
        if title:
            result["title"] = title[0].strip()

        cover = item.xpath(".//img/@src")
        if cover:
            result["cover"] = av911_url + cover[0]

        if result:
            results.append(result)

    return results


def download_image(url, save_path):
    """
    ä¸‹è½½å›¾ç‰‡
    :param url: å›¾ç‰‡URL
    :param save_path: ä¿å­˜è·¯å¾„
    :return: æ˜¯å¦ä¸‹è½½æˆåŠŸ
    """
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        }
        response = requests.get(url, headers=headers, timeout=10, stream=True)
        if response.status_code == 200:
            with open(save_path, "wb") as f:
                for chunk in response.iter_content(chunk_size=1024):
                    if chunk:
                        f.write(chunk)
            return True
        return False
    except Exception as e:
        print(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {e}")
        return False


def main(
    number,
    appoint_url="",
    log_info="",
    req_web="",
    language="zh_cn",
    file_path="",
    appoint_number="",
):
    """
    ä¸»å‡½æ•°ï¼Œè·å–å½±ç‰‡ä¿¡æ¯
    :param number: ç•ªå·
    :param appoint_url: æŒ‡å®šçš„URL
    :param log_info: æ—¥å¿—ä¿¡æ¯
    :param req_web: è¯·æ±‚ç½‘ç«™ä¿¡æ¯
    :param language: è¯­è¨€
    :param file_path: æ–‡ä»¶è·¯å¾„
    :param appoint_number: æŒ‡å®šçš„ç•ªå·
    :return: JSONæ ¼å¼çš„å½±ç‰‡ä¿¡æ¯
    """
    start_time = time.time()
    website_name = "av911"
    req_web += "-> %s" % website_name
    title = ""
    cover_url = ""
    web_info = "\n       "
    log_info += " \n    ğŸŒ av911"
    debug_info = ""
    real_url = appoint_url
    av911_url = "https://av911.tv"

    try:
        if not real_url:
            # å¤„ç†ç•ªå·
            number_list = [number]
            if appoint_number:
                number_list.append(appoint_number)

            # å°è¯•æœç´¢ç•ªå·
            for each in number_list:
                real_url = f"{av911_url}/serach?fulltext={each}"
                debug_info = f"è¯·æ±‚åœ°å€: {real_url} "
                log_info += web_info + debug_info
                result, response = curl_html(real_url)

                if not result:
                    debug_info = "ç½‘ç»œè¯·æ±‚é”™è¯¯: %s" % response
                    log_info += web_info + debug_info
                    raise Exception(debug_info)

                search_page = etree.fromstring(response, etree.HTMLParser())
                result, number, title, real_url = get_real_url(search_page, number_list)
                if result:
                    real_url = (
                        av911_url + real_url
                        if not real_url.startswith("http")
                        else real_url
                    )
                    break
            else:
                debug_info = "æ²¡æœ‰åŒ¹é…çš„æœç´¢ç»“æœ"
                log_info += web_info + debug_info
                raise Exception(debug_info)

        debug_info = f"ç•ªå·åœ°å€: {real_url} "
        log_info += web_info + debug_info
        result, response = curl_html(real_url)

        if not result:
            debug_info = "æ²¡æœ‰æ‰¾åˆ°æ•°æ® %s " % response
            log_info += web_info + debug_info
            raise Exception(debug_info)

        detail_page = etree.fromstring(response, etree.HTMLParser())
        (
            number,
            title,
            actor,
            cover_url,
            studio,
            release,
            year,
            tag,
            outline,
            video_url,
            video_type,
        ) = get_detail_info(detail_page, number, file_path)
        actor_photo = get_actor_photo(actor)

        try:
            dic = {
                "number": number,
                "title": title,
                "originaltitle": title,
                "actor": actor,
                "outline": outline,
                "originalplot": outline,
                "tag": tag,
                "release": release,
                "year": year,
                "runtime": "",
                "score": "",
                "series": "",
                "country": (
                    "CN" if video_type == "ç„¡ç¢¼" or video_type == "æ— ç " else "JP"
                ),
                "director": "",
                "studio": studio,
                "publisher": studio,
                "source": "av911",
                "website": real_url,
                "actor_photo": actor_photo,
                "cover": cover_url,
                "poster": cover_url,
                "extrafanart": "",
                "trailer": video_url,
                "image_download": False,
                "image_cut": "no",
                "log_info": log_info,
                "error_info": "",
                "req_web": req_web
                + "(%ss) "
                % (
                    round(
                        (time.time() - start_time),
                    )
                ),
                "mosaic": (
                    "æ— ç " if video_type == "ç„¡ç¢¼" or video_type == "æ— ç " else "æœ‰ç "
                ),
                "wanted": "",
            }
            debug_info = "æ•°æ®è·å–æˆåŠŸï¼"
            log_info += web_info + debug_info
            dic["log_info"] = log_info
        except Exception as e:
            debug_info = "æ•°æ®ç”Ÿæˆå‡ºé”™: %s" % str(e)
            log_info += web_info + debug_info
            raise Exception(debug_info)

    except Exception as e:
        # print(traceback.format_exc())
        debug_info = str(e)
        dic = {
            "title": "",
            "cover": "",
            "website": "",
            "log_info": log_info,
            "error_info": debug_info,
            "req_web": req_web
            + "(%ss) "
            % (
                round(
                    (time.time() - start_time),
                )
            ),
        }
    dic = {website_name: {"zh_cn": dic, "zh_tw": dic, "jp": dic}}
    js = json.dumps(
        dic,
        ensure_ascii=False,
        sort_keys=False,
        indent=4,
        separators=(",", ": "),
    )
    return js


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print(main("å·¨ä¹³", file_path=""))
    # æµ‹è¯•æœç´¢åŠŸèƒ½
    # results = search("å·¨ä¹³", 1)
    # print(f"æœç´¢ç»“æœæ•°é‡: {len(results)}")
    # if results:
    #     print(f"ç¬¬ä¸€ä¸ªç»“æœ: {results[0]}")
